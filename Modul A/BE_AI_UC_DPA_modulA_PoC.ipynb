{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook for use case digital posting assistant - stage1 - Proof of Concept\n",
    "### Module A vectorize accounting assignment guide\n",
    "#### Ojectives\n",
    "- In this module we will develop the load and the vectorization of the text-file for the accounting assignment guide\n",
    "- The vectorized accounting assignemnt guide will finaly stored in a SAP HANA vector database\n",
    "\n",
    "#### Processing steps from concept\n",
    "A0 - preparation\n",
    "\n",
    "A2 - load and splitt: load the pdf-file containing the accounting assignment guide data from a folder and splitt the data into text_chunks\n",
    "\n",
    "A3 - vectorize and embedd: vectorize the splitted data with embedding function. Use an embedding function to convert the text chunks into vector representations\n",
    "\n",
    "A4 - store: create/clear a sap hana database - table and store the vector in this table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A0 - Setup and configuration Modul A\n",
    "\n",
    "The following setup-steps where processed:\n",
    "\n",
    "* A0.0 Start SAP instances\n",
    "* A0.1 install py-packages\n",
    "* A0.2 load env-variables from config.json-file\n",
    "* A0.3 Setup and test connection to HANA DB\n",
    "* A0.4 Setup LLM-Connection to SAP AI-HUB\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A0.0 Start SAP Instances\n",
    "\n",
    "* Start BTP Cockpit\n",
    "* Start SAP Build Dev Space\n",
    "* Start HANA DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A0.1 install py-packages\n",
    "# RESET KERNEL AFTER INSTALLATION\n",
    "\n",
    "%pip install --upgrade pip\n",
    "\n",
    "%pip install --quiet hdbcli --break-system-packages\n",
    "%pip install --quiet generative-ai-hub-sdk[all] --break-system-packages\n",
    "%pip install --quiet folium --break-system-packages\n",
    "%pip install --quiet ipywidgets --break-system-packages\n",
    "%pip install --quiet pypdf\n",
    "%pip install --quiet -U ipykernel\n",
    "%pip install --quiet hana-ml\n",
    "%pip install --quiet sqlalchemy-hana\n",
    "%pip install --quiet nltk\n",
    "%pip install --quiet langchain langchain_experimental langchain_openai\n",
    "print(\"py-packages installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A0.2 load env-variables from config.json-file\n",
    "# This script loads environment variables from a JSON configuration file\n",
    "# and sets them in the current environment. It raises an error if the file does not exist\n",
    "# or if the JSON file is malformed.\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "def load_env_variables(config_file):\n",
    "    \"\"\"\n",
    "    Load environment variables from a JSON configuration file.\n",
    "\n",
    "    Args:\n",
    "        config_file (str): Path to the JSON configuration file.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the environment variables.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(config_file):\n",
    "        raise FileNotFoundError(f\"The configuration file {config_file} does not exist.\")\n",
    "    \n",
    "    try:\n",
    "        with open(config_file, 'r') as file:\n",
    "            env_variables = json.load(file)\n",
    "    except json.JSONDecodeError as e:\n",
    "        raise ValueError(f\"Error decoding JSON from the configuration file {config_file}: {e}\")\n",
    "    \n",
    "    for key, value in env_variables.items():\n",
    "        # Convert non-string values to strings before setting them in os.environ\n",
    "        if isinstance(value, dict):\n",
    "            value = json.dumps(value)  # Convert dictionaries to JSON strings\n",
    "        os.environ[key] = str(value)\n",
    "    \n",
    "    return env_variables\n",
    "\n",
    "# Example usage\n",
    "config_file = \"/home/user/.aicore/config.json\"\n",
    "try:\n",
    "    env_variables = load_env_variables(config_file)\n",
    "    print(f\"Loaded environment variables: {env_variables}\")\n",
    "except (FileNotFoundError, ValueError) as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A0.2 Test connection with env-Variables to SAP AI core\n",
    "\n",
    "from gen_ai_hub.proxy.native.openai import embeddings\n",
    "import os\n",
    "\n",
    "# Ensure the correct model name is used\n",
    "model_embedding_name = os.getenv(\"AICORE_DEPLOYMENT_MODEL_EMBEDDING\", \"text-embedding-ada-002\")  # Default to a valid model\n",
    "try:\n",
    "    response = embeddings.create(\n",
    "        input=\"SAP Generative AI Hub is awesome!\",\n",
    "        # deployment_id=deployment_id, # Uncomment if using a  model deployment ID\n",
    "        # model_id=model_id,  # Uncomment if using a  model ID\n",
    "        model_name=model_embedding_name,  # Uncomment if using a  model name\n",
    "        # model_version=\"latest\",  # Uncomment if using a specific version\n",
    "        # model_type=\"text\",  # Uncomment if using a specific model type\n",
    "        #model_version=\"latest\",  # Uncomment if using the latest version\n",
    "        #model_type=\"text-embedding\",  # Uncomment if using a specific model type\n",
    "        #model_name=model_embedding\n",
    "    )\n",
    "    print(response.data)\n",
    "except ValueError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Ensure the model name matches an existing deployment in SAP AI Hub.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A0.3 Setup and test connection to HANA DB\n",
    "\n",
    "import os\n",
    "# from hana_ml import ConnectionContext\n",
    "from hdbcli import dbapi\n",
    "\n",
    "# Fetch environment variables\n",
    "hdb_host_address = os.getenv(\"hdb_host_address\")\n",
    "hdb_user = os.getenv(\"hdb_user\")\n",
    "hdb_password = os.getenv(\"hdb_password\")\n",
    "hdb_port = os.getenv(\"hdb_port\")\n",
    "\n",
    "# Debugging: Print non-sensitive environment variables\n",
    "print(f\"hdb_host_address: {hdb_host_address}\")\n",
    "print(f\"hdb_user: {hdb_user}\")\n",
    "print(f\"hdb_port: {hdb_port}\")\n",
    "\n",
    "# Ensure variables are defined\n",
    "if not all([hdb_host_address, hdb_user, hdb_password, hdb_port]):\n",
    "    raise ValueError(\"One or more HANA DB connection parameters are missing.\")\n",
    "\n",
    "# Convert port to integer\n",
    "hdb_port = int(hdb_port)\n",
    "\n",
    "# Create a connection to the HANA database\n",
    "# hana_connection = ConnectionContext(\n",
    "#     address=hdb_host_address,\n",
    "#     port=hdb_port,\n",
    "#     user=hdb_user,\n",
    "#     password=hdb_password,\n",
    "#     encrypt=True\n",
    "# )\n",
    "\n",
    "# Test the connection\n",
    "# print(\"HANA DB Version:\", hana_connection.hana_version())\n",
    "# print(\"Current Schema:\", hana_connection.get_current_schema())\n",
    "\n",
    "hana_connection = dbapi.connect(\n",
    "    address=hdb_host_address,\n",
    "    port=hdb_port,\n",
    "    user=hdb_user,\n",
    "    password=hdb_password,\n",
    "    #encrypt=True\n",
    "    autocommit=True,\n",
    "    sslValidateCertificate=False,\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A0.4 Setup LLM-Connection to SAP AI-HUB\n",
    "\n",
    "import os\n",
    "import dotenv\n",
    "from gen_ai_hub.proxy.langchain.openai import ChatOpenAI\n",
    "from gen_ai_hub.proxy.langchain.openai import OpenAI\n",
    "\n",
    "# Lade aicore_model_name aus der Umgebungskonfiguration\n",
    "aicore_model_name = str(os.getenv(\"AICORE_DEPLOYMENT_MODEL\"))\n",
    "\n",
    "# √úberpr√ºfe, ob die Variable definiert ist\n",
    "if not aicore_model_name:\n",
    "    raise ValueError(f\"\"\"Parameter LLM-Model-Name {aicore_model_name} fehlt in der Umgebungskonfiguration.\"\"\")\n",
    "\n",
    "llm = ChatOpenAI(proxy_model_name=aicore_model_name)\n",
    "#llm = OpenAI(proxy_model_name=aicore_model_name)\n",
    "\n",
    "if not llm:\n",
    "    raise ValueError(f\"\"\"Parameter LLM-Model-Name {aicore_model_name} fehlt in der Umgebungskonfiguration.\"\"\")\n",
    "else:\n",
    "    print(f\"\"\"Parameter LLM-Model-Name: {aicore_model_name} wurde erfolgreich geladen.\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A0.5 Setup embedding-model from AI Hub\n",
    "\n",
    "from gen_ai_hub.proxy.langchain.init_models import init_embedding_model\n",
    "\n",
    "ai_core_embedding_model_name = str(os.getenv('AICORE_DEPLOYMENT_MODEL_EMBEDDING'))\n",
    " \n",
    "try:\n",
    "    embeddings = init_embedding_model(ai_core_embedding_model_name)\n",
    "    print(\"Embedding model initialized successfully.\")\n",
    "except Exception as e:\n",
    "    print(\"Embedding model not initialized.\")\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A0.6 Setup vectorestore in SAP HANA Database\n",
    "# Hint: check table creation with sap-hana-database explorer: select * from ACCOUNTING_ASSIGN_SUPPORT_TABLE_DBADMIN\n",
    "\n",
    "from langchain_community.vectorstores.hanavector import HanaDB\n",
    "\n",
    "vector_table_name = str(os.getenv('hdb_table_name'))\n",
    "\n",
    "hana_database = HanaDB(\n",
    "    embedding = embeddings, \n",
    "    connection = hana_connection, \n",
    "    table_name = vector_table_name\n",
    ")\n",
    "\n",
    "try:\n",
    "    print(f\"\"\"\n",
    "    Successfully created SAP HANA VectorStore interface: {hana_database.connection}\n",
    "    and SAP HANA table: {vector_table_name}.\n",
    "    \"\"\")\n",
    "except Exception as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### processing functions Modul A\n",
    "\n",
    "- function A2: load the pdf-file with accounting assignment guide data and splitt the data into text_chunk\n",
    "\n",
    "- function A3: vectorize the splitted data with embedding function \n",
    "\n",
    "- function A4.1: create a LangChain VectorStore interface for the HANA database and specify the table\n",
    "\n",
    "- function A4.2: delete existing documents from the table and load embeddings to SAP HANA-Tabele\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function A2: load the pdf-file containing the accounting assignment guide data from a folder and splitt the data into text_chunks\n",
    "# function A2.1 load data\n",
    "\n",
    "import os\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "def load_pdf(file_path):\n",
    "    # Check if the file exists\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n",
    "    \n",
    "    # Load the PDF file\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    documents = loader.load()\n",
    "    return (documents)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Test the function with a sample PDF file path\n",
    "    file_path = \"data/sample_accounting_guide.pdf\"\n",
    "    try:\n",
    "        documents = load_pdf(file_path)\n",
    "        print(f\"Length of text created: {len(documents)}\")\n",
    "        print(f\"First page from Text: {documents[1]}\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function A2: load the pdf-file containing the accounting assignment guide data from a folder and splitt the data into text_chunks\n",
    "# function A2.2.3 split document in chunks - version 3: Semantic Chunking with LChain \n",
    "# (see LChain docs - https://python.langchain.com/docs/how_to/semantic-chunker/)\n",
    "# problem: SemanticChunker needs String-Structure, load document has LChain document-object-structure\n",
    "\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from gen_ai_hub.proxy.langchain.init_models import init_embedding_model\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Input: documents (from function A2 - load data)\n",
    "# Output: text_chunks\n",
    "\n",
    "# parameters\n",
    "chunk_size_param_min = 500\n",
    "chunk_size_param_max = 1000\n",
    "chunk_size_param = 1000\n",
    "chunk_overlap_param = 200   \n",
    "chunk_overlap_param_min = 200\n",
    "chunk_overlap_param_max = 400\n",
    "\n",
    "\n",
    "# load env-key for embedidding model SAP AI Core\n",
    "ai_core_embedding_model_name = str(os.getenv('AICORE_DEPLOYMENT_MODEL_EMBEDDING'))\n",
    "\n",
    "# init embedding-instance\n",
    "try:\n",
    "    embeddings = init_embedding_model(ai_core_embedding_model_name)\n",
    "    print(\"Embedding model initialized successfully.\")\n",
    "except Exception as e:\n",
    "    print(\"Embedding model not initialized.\")\n",
    "    print(e)\n",
    "\n",
    "\n",
    "# Init text_splitter-Instance with type \n",
    "\n",
    "# text_splitter = SemanticChunker(embeddings=embeddings)\n",
    "text_splitter = SemanticChunker(embeddings=embeddings, breakpoint_threshold_type=\"gradient\")\n",
    "# text_splitter = SemanticChunker(embeddings=embeddings, breakpoint_threshold_type=\"mean\", breakpoint_threshold=0.5)\n",
    "# text_splitter = SemanticChunker(embeddings=embeddings, breakpoint_threshold_type=\"median\")\n",
    "# text_splitter = SemanticChunker(embeddings=embeddings, breakpoint_threshold_type=\"interquartile\")\n",
    "# text_splitter = SemanticChunker(embeddings=embeddings, breakpoint_threshold_type=\"percentile\", min_chunk_size=100, max_chunk_size=1000)\n",
    "# text_splitter = SemanticChunker(embeddings=embeddings, breakpoint_threshold_type=\"standard_deviation\")\n",
    "# text_splitter = SemanticChunker(embeddings=embeddings, breakpoint_threshold_type=\"percentile\")\n",
    "\n",
    "\n",
    "\n",
    "# split docs for every page in documents with SematicChunker-Splitter and rebuild document-object Lchain\n",
    "# We split text in the usual way, e.g., by invoking .create_documents to create LangChain Document objects\n",
    "# docs = text_splitter.create_documents([state_of_the_union] <- List not document-object)\n",
    "\n",
    "text_chunks = []\n",
    "for doc in documents:\n",
    "    text_split = text_splitter.split_text(doc.page_content)\n",
    "    # rebuild documents-objekt in LChain-Document-Structure\n",
    "    for text in text_split:\n",
    "        text_chunks.append(Document(page_content=text, metadata=doc.metadata))\n",
    "\n",
    "\n",
    "print(f\"\\nüìÑ Insgesamt {len(text_chunks)} Chunks generiert.\")\n",
    "for i, chunk in enumerate(text_chunks[5:10]):\n",
    "    print(f\"\\n--- Chunk {i+1} ---\\n{chunk}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function A4 - store: create/clear a sap hana database - table and store the vector in this table\n",
    "# function A4.2.1 - delete existing documents from the table and load embeddings to SAP HANA-Table\n",
    "\n",
    "# Delete already existing documents from the SAP HANA table\n",
    "hana_database.delete(filter={})\n",
    "\n",
    "# add the loaded document text_chunks\n",
    "hana_database.add_documents(text_chunks)\n",
    "\n",
    "print(f\"Successfully added {len(text_chunks)} document chunks to the database.\")\n",
    "print(\"table-name: \",hana_database.table_name)\n",
    "print(\"Successfully connected to the HANA Cloud database.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function A4 - store: create/clear a sap hana database - table and store the vector in this table\n",
    "# check function A4.2 - query to the table to verify embeddings\n",
    "# SQL: SELECT TOP 1000\n",
    "#      \"VEC_TEXT\",\n",
    "#      \"VEC_META\",\n",
    "#      \"VEC_VECTOR\"\n",
    "#      FROM \"DBADMIN\".\"ACCOUNTING_ASSIGN_SUPPORT_TABLE_DBADMIN\" \n",
    "#      WHERE VEC_TEXT LIKE '%R√ºckstellung%'\n",
    "\n",
    "cursor = hana_connection.cursor()\n",
    "sql = f'SELECT VEC_TEXT, TO_NVARCHAR(VEC_VECTOR) FROM \"{hana_database.table_name}\" WHERE VEC_TEXT LIKE \\'%R√ºckstellung%\\''\n",
    "# sql = f'SELECT TOP 1000 VEC_TEXT, TO_NVARCHAR(VEC_VECTOR) FROM \"{hana_database.table_name}\"'  \n",
    "\n",
    "cursor.execute(sql)\n",
    "vectors = cursor.fetchall()\n",
    "\n",
    "print(vectors[5:10])\n",
    "\n",
    "# for vector in vectors:\n",
    "#     print(vector)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
