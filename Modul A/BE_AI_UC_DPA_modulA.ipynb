{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook for use case digital posting assistant - stage1\n",
    "### Module A vectorize accounting assignment guide\n",
    "#### Ojectives\n",
    "- In this module we will develop the load and the vectorization of the text-file for the accounting assignment guide\n",
    "- The vectorized accounting assignemnt guide will finaly stored in a SAP HANA vector database\n",
    "\n",
    "#### Processing steps from concept\n",
    "A0 - preparation\n",
    "\n",
    "A2 - load and splitt: load the pdf-file containing the accounting assignment guide data from a folder and splitt the data into text_chunks\n",
    "\n",
    "A3 - vectorize and embedd: vectorize the splitted data with embedding function. Use an embedding function to convert the text chunks into vector representations\n",
    "\n",
    "A4 - store: create/clear a sap hana database - table and store the vector in this table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A0 - Setup and configuration Modul A\n",
    "\n",
    "The following setup-steps where processed:\n",
    "\n",
    "* A0.0 Start SAP instances\n",
    "* A0.1 install py-packages\n",
    "* A0.2 load env-variables from config.json-file\n",
    "* A0.3 Setup and test connection to HANA DB\n",
    "* A0.4 Setup LLM-Connection to SAP AI-HUB\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A0.0 Start SAP Instances\n",
    "\n",
    "* Start BTP Cockpit\n",
    "* Start SAP Build Dev Space\n",
    "* Start HANA DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /home/user/.asdf-inst/installs/python/3.13.1/lib/python3.13/site-packages (25.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "py-packages installed!\n"
     ]
    }
   ],
   "source": [
    "# A0.1 install py-packages\n",
    "# RESET KERNEL AFTER INSTALLATION\n",
    "\n",
    "%pip install --upgrade pip\n",
    "\n",
    "%pip install --quiet hdbcli --break-system-packages\n",
    "%pip install --quiet generative-ai-hub-sdk[all] --break-system-packages\n",
    "%pip install --quiet folium --break-system-packages\n",
    "%pip install --quiet ipywidgets --break-system-packages\n",
    "%pip install --quiet pypdf\n",
    "%pip install --quiet -U ipykernel\n",
    "%pip install --quiet hana-ml\n",
    "%pip install --quiet hdbcli\n",
    "%pip install --quiet sqlalchemy-hana\n",
    "%pip install --quiet nltk\n",
    "%pip install --quiet langchain langchain_experimental langchain_openai\n",
    "print(\"py-packages installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded environment variables: {'AICORE_AUTH_URL': 'https://sap-ai.authentication.eu10.hana.ondemand.com', 'AICORE_CLIENT_ID': 'sb-6ebfacf0-d811-447e-9262-b18974ccd67e!b364403|aicore!b540', 'AICORE_CLIENT_SECRET': '17e8de41-d715-4cfe-b086-9c59302640df$_9OJ4fWV28VeLV5uSMyxA_DfSXHna3dEE9-0KocfNeE=', 'AICORE_RESOURCE_GROUP': 'default', 'AICORE_BASE_URL': 'https://api.ai.prod.eu-central-1.aws.ml.hana.ondemand.com/v2', 'AICORE_DEPLOYMENT_ID': 'dc4204c43f309ca1', 'AICORE_DEPLOYMENT_URL': 'https://api.ai.prod.eu-central-1.aws.ml.hana.ondemand.com/v2/inference/deployments/dc4204c43f309ca1', 'AICORE_DEPLOYMENT_MODEL': 'gpt-4', 'AICORE_DEPLOYMENT_MODEL_VERSION': 'latest', 'AICORE_DEPLOYMENT_MODEL_EMBEDDING': 'text-embedding-ada-002', 'hdb_host_address': 'bfff8255-c34a-41cb-a822-bf9b5f56fb16.hna0.prod-eu20.hanacloud.ondemand.com', 'hdb_user': 'DBADMIN', 'hdb_password': 'DB@hmx04', 'hdb_port': '443', 'hdb_table_name': 'ACCOUNTING_ASSIGN_SUPPORT_TABLE_DBADMIN'}\n"
     ]
    }
   ],
   "source": [
    "# A0.2 load env-variables from config.json-file\n",
    "# This script loads environment variables from a JSON configuration file\n",
    "# and sets them in the current environment. It raises an error if the file does not exist\n",
    "# or if the JSON file is malformed.\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "def load_env_variables(config_file):\n",
    "    \"\"\"\n",
    "    Load environment variables from a JSON configuration file.\n",
    "\n",
    "    Args:\n",
    "        config_file (str): Path to the JSON configuration file.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the environment variables.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(config_file):\n",
    "        raise FileNotFoundError(f\"The configuration file {config_file} does not exist.\")\n",
    "    \n",
    "    try:\n",
    "        with open(config_file, 'r') as file:\n",
    "            env_variables = json.load(file)\n",
    "    except json.JSONDecodeError as e:\n",
    "        raise ValueError(f\"Error decoding JSON from the configuration file {config_file}: {e}\")\n",
    "    \n",
    "    for key, value in env_variables.items():\n",
    "        # Convert non-string values to strings before setting them in os.environ\n",
    "        if isinstance(value, dict):\n",
    "            value = json.dumps(value)  # Convert dictionaries to JSON strings\n",
    "        os.environ[key] = str(value)\n",
    "    \n",
    "    return env_variables\n",
    "\n",
    "# Example usage\n",
    "config_file = \"/home/user/.aicore/config.json\"\n",
    "try:\n",
    "    env_variables = load_env_variables(config_file)\n",
    "    print(f\"Loaded environment variables: {env_variables}\")\n",
    "except (FileNotFoundError, ValueError) as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A0.2 Test connection with env-Variables to SAP AI core\n",
    "\n",
    "from gen_ai_hub.proxy.native.openai import embeddings\n",
    "\n",
    "response = embeddings.create(\n",
    "    input=\"SAP Generative AI Hub is awesome!\",\n",
    "    model_name=\"text-embedding-ada-002\"\n",
    "    \n",
    ")\n",
    "print(response.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A0.3 Setup and test connection to HANA DB\n",
    "\n",
    "import os\n",
    "# from hana_ml import ConnectionContext\n",
    "from hdbcli import dbapi\n",
    "\n",
    "# Fetch environment variables\n",
    "hdb_host_address = os.getenv(\"hdb_host_address\")\n",
    "hdb_user = os.getenv(\"hdb_user\")\n",
    "hdb_password = os.getenv(\"hdb_password\")\n",
    "hdb_port = os.getenv(\"hdb_port\")\n",
    "\n",
    "# Debugging: Print non-sensitive environment variables\n",
    "print(f\"hdb_host_address: {hdb_host_address}\")\n",
    "print(f\"hdb_user: {hdb_user}\")\n",
    "print(f\"hdb_port: {hdb_port}\")\n",
    "\n",
    "# Ensure variables are defined\n",
    "if not all([hdb_host_address, hdb_user, hdb_password, hdb_port]):\n",
    "    raise ValueError(\"One or more HANA DB connection parameters are missing.\")\n",
    "\n",
    "# Convert port to integer\n",
    "hdb_port = int(hdb_port)\n",
    "\n",
    "# Create a connection to the HANA database\n",
    "# hana_connection = ConnectionContext(\n",
    "#     address=hdb_host_address,\n",
    "#     port=hdb_port,\n",
    "#     user=hdb_user,\n",
    "#     password=hdb_password,\n",
    "#     encrypt=True\n",
    "# )\n",
    "\n",
    "# Test the connection\n",
    "# print(\"HANA DB Version:\", hana_connection.hana_version())\n",
    "# print(\"Current Schema:\", hana_connection.get_current_schema())\n",
    "\n",
    "hana_connection = dbapi.connect(\n",
    "    address=hdb_host_address,\n",
    "    port=hdb_port,\n",
    "    user=hdb_user,\n",
    "    password=hdb_password,\n",
    "    #encrypt=True\n",
    "    autocommit=True,\n",
    "    sslValidateCertificate=False,\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A0.4 Setup LLM-Connection to SAP AI-HUB\n",
    "\n",
    "import os\n",
    "import dotenv\n",
    "from gen_ai_hub.proxy.langchain.openai import ChatOpenAI\n",
    "from gen_ai_hub.proxy.langchain.openai import OpenAI\n",
    "\n",
    "# Lade aicore_model_name aus der Umgebungskonfiguration\n",
    "aicore_model_name = str(os.getenv(\"AICORE_DEPLOYMENT_MODEL\"))\n",
    "\n",
    "# Überprüfe, ob die Variable definiert ist\n",
    "if not aicore_model_name:\n",
    "    raise ValueError(f\"\"\"Parameter LLM-Model-Name {aicore_model_name} fehlt in der Umgebungskonfiguration.\"\"\")\n",
    "\n",
    "llm = ChatOpenAI(proxy_model_name=aicore_model_name)\n",
    "#llm = OpenAI(proxy_model_name=aicore_model_name)\n",
    "\n",
    "if not llm:\n",
    "    raise ValueError(f\"\"\"Parameter LLM-Model-Name {aicore_model_name} fehlt in der Umgebungskonfiguration.\"\"\")\n",
    "else:\n",
    "    print(f\"\"\"Parameter LLM-Model-Name: {aicore_model_name} wurde erfolgreich geladen.\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### processing functions Modul A\n",
    "\n",
    "- function A2: load the pdf-file with accounting assignment guide data and splitt the data into text_chunk\n",
    "\n",
    "- function A3: vectorize the splitted data with embedding function \n",
    "\n",
    "- function A4.1: create a LangChain VectorStore interface for the HANA database and specify the table\n",
    "\n",
    "- function A4.2: delete existing documents from the table and load embeddings to SAP HANA-Tabele\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text created: 70\n",
      "First page from Text: page_content='Kontierungshandbuch \n",
      " \n",
      " \n",
      "2 von 70 \n",
      "2.2.2.6.2 Aufzinsung langfristiger Finanzforderungen 26 \n",
      "2.2.2.7 Transaktionen in Fremdwährungen - Forderungen 27 \n",
      "2.2.2.7.1 Forderungsabwertung durch Währungsverluste 28 \n",
      "2.2.2.7.2 Ausweis unrealisierter Währungsgewinne bei Forderungen aus L u. L 29 \n",
      "2.2.2.7.3 Rücknahme der unrealisierten Währungsgewinne aus L u. L. 30 \n",
      "2.2.3 Wertpapiere des Umlaufvermögens 30 \n",
      "2.2.3.1 Zuordnung 30 \n",
      "2.2.3.2 Bewertung von Wertpapieren des Umlaufvermögens 31 \n",
      "2.2.3.3 Umgliederung von Wertpapieren 33 \n",
      "2.2.3.3.1 Umgliederung von „trading“ (T) nach „available for sale“ (A) oder „held to \n",
      "maturity“ (H) nach Ausweis eines unrealisierten Gewinns 33 \n",
      "2.2.3.3.2 Umgliederung von „available for sale“ (A) oder „held to maturity“ (H) nach \n",
      "„trading“ (T) bei Ausweis eines unrealisierten Gewinns 34 \n",
      "2.2.3.3.3 Umgliederung von „held to maturity“ (H) nach „available for sale“ (A) bei \n",
      "erfolgsneutralem Ausweis eines unrealisierten Gewinns 35 \n",
      "2.2.3.3.4 Umgliederung von „available for sale“ (A) nach „held to maturity“ (H nach \n",
      "erfolgsneutraler Aufwertung 36 \n",
      "2.2.3.3.5 Erfolgswirksame Vereinnahmung der Neubewertungsrücklage über die \n",
      "Restlaufzeit des Wertpapiers 37 \n",
      "2.2.3.4 Wertminderungen am Beispiel marktfähiger Wertpapiere 37 \n",
      "2.3 Aktive latente Steuern 39 \n",
      "2.3.1 Ansatz und Methodik latenter Steuern 39 \n",
      "2.3.2 Buchung der aktivischen Steuerdifferenz 40 \n",
      "2.3.3 Auflösung der aktivischen Steuerdifferenz 41 \n",
      "3 Bilanz: Passiva 42 \n",
      "3.1 Rückstellungen 42 \n",
      "3.1.1 Zuordnung 42 \n",
      "3.1.2 Bewertung 43 \n",
      "3.1.3 Aufwandsrückstellungen nach HGB 44 \n",
      "3.1.3.1 Auflösung der HGB-Rückstellung bei Eintritt des Rückstellungsgrundes 45 \n",
      "3.1.3.2 Auflösung der HGB-Rückst. bei Nicht-Eintritt des Rückstellungsgrundes 46 \n",
      "3.1.4 Langfristige Rückstellungen: Aufzinsung / Abzinsung 46 \n",
      "3.1.4.1 Aufzinsung langfristiger Rückstellungen 48 \n",
      "3.1.4.2 Abzinsung langfristiger Rückstellungen 49 \n",
      "3.1.5 Pensionsrückstellungen 50 \n",
      "3.1.5.1 Bildung von Pensionsrückstellungen 51 \n",
      "3.1.5.2 Auflösung von Pensionsrückstellungen 52 \n",
      "3.2 Verbindlichkeiten 53 \n",
      "3.2.1 Zuordnung von Verbindlichkeiten nach HGB und US GAAP 53 \n",
      "3.2.2 Bewertung von Verbindlichkeiten 55 \n",
      "3.2.3 Abzinsung von Verbindlichkeiten 55 \n",
      "3.2.4 Verbindlichkeiten in Fremdwährungen 57' metadata={'producer': 'Microsoft® Word für Microsoft 365', 'creator': 'Microsoft® Word für Microsoft 365', 'creationdate': '2025-04-08T10:47:12+02:00', 'title': 'Kontierungshandbuch', 'author': 'Ein zufriedengestellter Microsoft Office-Anwender', 'moddate': '2025-04-08T10:47:12+02:00', 'source': 'data/sample_accounting_guide.pdf', 'total_pages': 70, 'page': 1, 'page_label': '2'}\n"
     ]
    }
   ],
   "source": [
    "# function A2: load the pdf-file containing the accounting assignment guide data from a folder and splitt the data into text_chunks\n",
    "# function A2.1 load data\n",
    "\n",
    "import os\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "def load_pdf(file_path):\n",
    "    # Check if the file exists\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n",
    "    \n",
    "    # Load the PDF file\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    documents = loader.load()\n",
    "    return (documents)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Test the function with a sample PDF file path\n",
    "    file_path = \"data/sample_accounting_guide.pdf\"\n",
    "    try:\n",
    "        documents = load_pdf(file_path)\n",
    "        print(f\"Length of text created: {len(documents)}\")\n",
    "        print(f\"First page from Text: {documents[1]}\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function A2: load the pdf-file containing the accounting assignment guide data from a folder and splitt the data into text_chunks\n",
    "# function A2.2.1 split document in chunks - version 1: Character Text-Splitter)\n",
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "chunk_size_param = 100\n",
    "chunk_overlap_param = 20\n",
    "\n",
    "def split_document_charakter(document):\n",
    "    # Split the documents into text_chunks\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=chunk_size_param, chunk_overlap=chunk_overlap_param)\n",
    "    text_chunks = text_splitter.split_documents(documents)\n",
    "    \n",
    "    return text_chunks\n",
    "\n",
    "   \n",
    "# Test the function with a sample PDF file path\n",
    "text_chunks = split_document_charakter(documents)\n",
    "print(f\"Number of text_chunks created: {len(text_chunks)}\")\n",
    "print(f\"First text_chunks: {text_chunks[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Lade Dokument...\n",
      "🧩 Erstelle Chunks...\n",
      "🧠 Verwende Sprache für Tokenizer: german\n",
      "\n",
      "📄 Insgesamt 1397 Chunks generiert.\n",
      "\n",
      "--- Chunk 1 ---\n",
      "page_content='Kontierungshandbuch \n",
      " \n",
      " \n",
      "1 von 70 \n",
      "Inhaltsverzeichnis \n",
      "1 Einleitung 3'\n",
      "\n",
      "--- Chunk 2 ---\n",
      "page_content='1 Einleitung 3 \n",
      "1.1 Inhalt und Aufbau des Handbuchs 3 \n",
      "1.2 Kontennomenklatur 4'\n",
      "\n",
      "--- Chunk 3 ---\n",
      "page_content='1.3 Buchungslogik und Zusatzkontierungen 5 \n",
      "1.4 Bedienungshinweise zum Kontierungshandbuch 6'\n",
      "\n",
      "--- Chunk 4 ---\n",
      "page_content='1.4.1 Hyperlinks 6 \n",
      "1.4.2 Farbschema 6 \n",
      "2 Bilanz: Aktiva 7 \n",
      "2.1 Anlagevermögen 7'\n",
      "\n",
      "--- Chunk 5 ---\n",
      "page_content='2.1.1 Immaterielle Vermögensgegenstände 7 \n",
      "2.1.1.1 Selbsterstellte Software 7 \n",
      "2.1.2 Sachanlagen 9'\n"
     ]
    }
   ],
   "source": [
    "# function A2: load the pdf-file containing the accounting assignment guide data from a folder and splitt the data into text_chunks\n",
    "# function A2.2.2 Class for load pdf-data and split documents in chunks (Sematic Chunker)\n",
    "\n",
    "import os\n",
    "import nltk\n",
    "from typing import List\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "\n",
    "class PDFSemanticChunker:\n",
    "    def __init__(self, pdf_path: str, language: str = \"german\", chunk_size: int = 100, chunk_overlap: int = 20):\n",
    "        self.pdf_path = pdf_path\n",
    "        self.language = language\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "\n",
    "        # Stelle sicher, dass Punkt verfügbar ist\n",
    "        self._ensure_nltk_resources()\n",
    "\n",
    "    def _ensure_nltk_resources(self):\n",
    "        try:\n",
    "            nltk.data.find(\"tokenizers/punkt\")\n",
    "        except LookupError:\n",
    "            print(\"📥 Lade NLTK Punkt-Tokenizer herunter...\")\n",
    "            nltk.download(\"punkt\")\n",
    "\n",
    "    def load_documents(self):\n",
    "        if not os.path.exists(self.pdf_path):\n",
    "            raise FileNotFoundError(f\"❌ Die Datei '{self.pdf_path}' wurde nicht gefunden.\")\n",
    "        loader = PyPDFLoader(self.pdf_path)\n",
    "        return loader.load()\n",
    "\n",
    "    def extract_texts(self, documents) -> List[str]:\n",
    "        return [doc.page_content for doc in documents]\n",
    "\n",
    "    def chunk_semantically(self, texts: List[str]) -> List[str]:\n",
    "        print(f\"🧠 Verwende Sprache für Tokenizer: {self.language}\")\n",
    "        valid_langs = [\"english\", \"german\"]\n",
    "        if self.language not in valid_langs:\n",
    "            print(f\"⚠️ Sprache '{self.language}' nicht unterstützt. Standard = 'german'\")\n",
    "            self.language = \"german\"\n",
    "\n",
    "        all_sentences = []\n",
    "        for text in texts:\n",
    "            all_sentences.extend(sent_tokenize(text, language=self.language))\n",
    "\n",
    "        chunks = []\n",
    "        current_chunk = \"\"\n",
    "        for sentence in all_sentences:\n",
    "            if len(current_chunk) + len(sentence) < self.chunk_size:\n",
    "                current_chunk += \" \" + sentence\n",
    "            else:\n",
    "                chunks.append(current_chunk.strip())\n",
    "                current_chunk = sentence\n",
    "        if current_chunk:\n",
    "            chunks.append(current_chunk.strip())\n",
    "\n",
    "        # Mit LangChain weiter aufteilen\n",
    "        splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=self.chunk_size,\n",
    "            chunk_overlap=self.chunk_overlap,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \".\", \" \"]\n",
    "        )\n",
    "\n",
    "        final_chunks = []\n",
    "        for chunk in chunks:\n",
    "            final_chunks.extend(splitter.split_text(chunk))\n",
    "\n",
    "        # chunks in LangChain-Dokument-Interface übergeben\n",
    "        chunk_documents = [Document(page_content=chunk) for chunk in final_chunks]\n",
    "\n",
    "        return chunk_documents\n",
    "\n",
    "    def process(self) -> List[str]:\n",
    "        print(\"📂 Lade Dokument...\")\n",
    "        docs = self.load_documents()\n",
    "        texts = self.extract_texts(docs)\n",
    "        print(\"🧩 Erstelle Chunks...\")\n",
    "        return self.chunk_semantically(texts)\n",
    "    \n",
    "#function-call Class\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = \"data/sample_accounting_guide.pdf\"\n",
    "\n",
    "    chunker = PDFSemanticChunker(\n",
    "        pdf_path=file_path,\n",
    "        language=\"german\",\n",
    "        chunk_size=100,\n",
    "        chunk_overlap=20\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        text_chunks = chunker.process()\n",
    "        print(f\"\\n📄 Insgesamt {len(text_chunks)} Chunks generiert.\")\n",
    "        for i, chunk in enumerate(text_chunks[:5]):\n",
    "            print(f\"\\n--- Chunk {i+1} ---\\n{chunk}\")\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model initialized successfully.\n",
      "text-splitter-instance set!\n",
      "\n",
      "📄 Insgesamt 139 Chunks generiert.\n",
      "\n",
      "--- Chunk 1 ---\n",
      "page_content='Kontierungshandbuch \n",
      " \n",
      " \n",
      "1 von 70 \n",
      "Inhaltsverzeichnis \n",
      "1 Einleitung 3 \n",
      "1.1 Inhalt und Aufbau des Handbuchs 3 \n",
      "1.2 Kontennomenklatur 4 \n",
      "1.3 Buchungslogik und Zusatzkontierungen 5 \n",
      "1.4 Bedienungshinweise zum Kontierungshandbuch 6 \n",
      "1.4.1 Hyperlinks 6 \n",
      "1.4.2 Farbschema 6 \n",
      "2 Bilanz: Aktiva 7 \n",
      "2.1 Anlagevermögen 7 \n",
      "2.1.1 Immaterielle Vermögensgegenstände 7 \n",
      "2.1.1.1 Selbsterstellte Software 7 \n",
      "2.1.2 Sachanlagen 9 \n",
      "2.1.2.1 Außerplanmäßige Abschreibung von Sachanlagen 9 \n",
      "2.1.2.2 Aktivierung der Verbindlichkeiten aus Capital Lease 10 \n",
      "2.1.2.2.1 Aktivierung der Verbindlichkeiten aus Capital Lease 11 \n",
      "2.1.2.2.2 Aktivierung der Verbindlichkeiten aus Capital Lease 11 \n",
      "2.2 Umlaufvermögen 12 \n",
      "2.2.1 Vorräte 12 \n",
      "2.2.1.1 Ermittlung des beizulegenden Werts (Marktwert) 13 \n",
      "2.2.1.2 Niederstwerttest (Lower of Cost or Market Prinzip) 13 \n",
      "2.2.2 Forderungen und sonstige Vermögensgegenstände 14 \n",
      "2.2.2.1 Forderungen gegen verbundene Unternehmen 15 \n",
      "2.2.2.2 Umbuchung von langfristigen Forderungen auf kurzfristigen Anteil 15 \n",
      "2.2.2.2.1 Umbuchung von langfristigen Darlehensforderungen auf kurzfristigen Anteil 15 \n",
      "2.2.2.2.2 Abbau des kurzfristigen Anteils der langfristigen Forderungen 16 \n",
      "2.2.2.2.3 Ausgleich der offenen Posten der langfristigen Forderungen und des \n",
      "kurzfristigen Anteils bei Rückzahlung des Gesamtdarlehensbetrages 16 \n",
      "2.2.2.2.4 Umbuchung von langfristigen Forderungen aus L u.' metadata={'producer': 'Microsoft® Word für Microsoft 365', 'creator': 'Microsoft® Word für Microsoft 365', 'creationdate': '2025-04-08T10:47:12+02:00', 'title': 'Kontierungshandbuch', 'author': 'Ein zufriedengestellter Microsoft Office-Anwender', 'moddate': '2025-04-08T10:47:12+02:00', 'source': 'data/sample_accounting_guide.pdf', 'total_pages': 70, 'page': 0, 'page_label': '1'}\n",
      "\n",
      "--- Chunk 2 ---\n",
      "page_content='L auf kurzfristigen Anteil \n",
      "   16 \n",
      "2.2.2.2.5 Abbau des kurzfristigen Anteils langfristiger Forderungen 17 \n",
      "2.2.2.3 Forderungsumbuchung von kurzfristigen auf langfristige Forderungen 17 \n",
      "2.2.2.4 Abschreibung von Forderungen 18 \n",
      "2.2.2.4.1 Abschreibung von Forderungen wegen Uneinbringlichkeit – identische \n",
      "Bewertung nach HGB und US GAAP 19 \n",
      "2.2.2.4.2 Abschreibung von Forderungen wegen Uneinbringlichkeit - unterschiedliche \n",
      "Bewertung nach HGB und US GAAP 20 \n",
      "2.2.2.5 Auf-/ Abzinsung von langfristigen Forderungen aus L u. L 21 \n",
      "2.2.2.5.1 Abzinsung langfristiger Forderungen aus L u. L 21 \n",
      "2.2.2.5.2 Aufzinsung langfristiger Forderungen aus L u. L 23 \n",
      "2.2.2.6 Auf-/ Abzinsung von langfristigen Finanzforderungen 25 \n",
      "2.2.2.6.1 Abzinsung langfristiger Finanzforderungen 26' metadata={'producer': 'Microsoft® Word für Microsoft 365', 'creator': 'Microsoft® Word für Microsoft 365', 'creationdate': '2025-04-08T10:47:12+02:00', 'title': 'Kontierungshandbuch', 'author': 'Ein zufriedengestellter Microsoft Office-Anwender', 'moddate': '2025-04-08T10:47:12+02:00', 'source': 'data/sample_accounting_guide.pdf', 'total_pages': 70, 'page': 0, 'page_label': '1'}\n",
      "\n",
      "--- Chunk 3 ---\n",
      "page_content='Kontierungshandbuch \n",
      " \n",
      " \n",
      "2 von 70 \n",
      "2.2.2.6.2 Aufzinsung langfristiger Finanzforderungen 26 \n",
      "2.2.2.7 Transaktionen in Fremdwährungen - Forderungen 27 \n",
      "2.2.2.7.1 Forderungsabwertung durch Währungsverluste 28 \n",
      "2.2.2.7.2 Ausweis unrealisierter Währungsgewinne bei Forderungen aus L u.' metadata={'producer': 'Microsoft® Word für Microsoft 365', 'creator': 'Microsoft® Word für Microsoft 365', 'creationdate': '2025-04-08T10:47:12+02:00', 'title': 'Kontierungshandbuch', 'author': 'Ein zufriedengestellter Microsoft Office-Anwender', 'moddate': '2025-04-08T10:47:12+02:00', 'source': 'data/sample_accounting_guide.pdf', 'total_pages': 70, 'page': 1, 'page_label': '2'}\n",
      "\n",
      "--- Chunk 4 ---\n",
      "page_content='L 29 \n",
      "2.2.2.7.3 Rücknahme der unrealisierten Währungsgewinne aus L u. L. 30 \n",
      "2.2.3 Wertpapiere des Umlaufvermögens 30 \n",
      "2.2.3.1 Zuordnung 30 \n",
      "2.2.3.2 Bewertung von Wertpapieren des Umlaufvermögens 31 \n",
      "2.2.3.3 Umgliederung von Wertpapieren 33 \n",
      "2.2.3.3.1 Umgliederung von „trading“ (T) nach „available for sale“ (A) oder „held to \n",
      "maturity“ (H) nach Ausweis eines unrealisierten Gewinns 33 \n",
      "2.2.3.3.2 Umgliederung von „available for sale“ (A) oder „held to maturity“ (H) nach \n",
      "„trading“ (T) bei Ausweis eines unrealisierten Gewinns 34 \n",
      "2.2.3.3.3 Umgliederung von „held to maturity“ (H) nach „available for sale“ (A) bei \n",
      "erfolgsneutralem Ausweis eines unrealisierten Gewinns 35 \n",
      "2.2.3.3.4 Umgliederung von „available for sale“ (A) nach „held to maturity“ (H nach \n",
      "erfolgsneutraler Aufwertung 36 \n",
      "2.2.3.3.5 Erfolgswirksame Vereinnahmung der Neubewertungsrücklage über die \n",
      "Restlaufzeit des Wertpapiers 37 \n",
      "2.2.3.4 Wertminderungen am Beispiel marktfähiger Wertpapiere 37 \n",
      "2.3 Aktive latente Steuern 39 \n",
      "2.3.1 Ansatz und Methodik latenter Steuern 39 \n",
      "2.3.2 Buchung der aktivischen Steuerdifferenz 40 \n",
      "2.3.3 Auflösung der aktivischen Steuerdifferenz 41 \n",
      "3 Bilanz: Passiva 42 \n",
      "3.1 Rückstellungen 42 \n",
      "3.1.1 Zuordnung 42 \n",
      "3.1.2 Bewertung 43 \n",
      "3.1.3 Aufwandsrückstellungen nach HGB 44 \n",
      "3.1.3.1 Auflösung der HGB-Rückstellung bei Eintritt des Rückstellungsgrundes 45 \n",
      "3.1.3.2 Auflösung der HGB-Rückst. bei Nicht-Eintritt des Rückstellungsgrundes 46 \n",
      "3.1.4 Langfristige Rückstellungen: Aufzinsung / Abzinsung 46 \n",
      "3.1.4.1 Aufzinsung langfristiger Rückstellungen 48 \n",
      "3.1.4.2 Abzinsung langfristiger Rückstellungen 49 \n",
      "3.1.5 Pensionsrückstellungen 50 \n",
      "3.1.5.1 Bildung von Pensionsrückstellungen 51 \n",
      "3.1.5.2 Auflösung von Pensionsrückstellungen 52 \n",
      "3.2 Verbindlichkeiten 53 \n",
      "3.2.1 Zuordnung von Verbindlichkeiten nach HGB und US GAAP 53 \n",
      "3.2.2 Bewertung von Verbindlichkeiten 55 \n",
      "3.2.3 Abzinsung von Verbindlichkeiten 55 \n",
      "3.2.4 Verbindlichkeiten in Fremdwährungen 57' metadata={'producer': 'Microsoft® Word für Microsoft 365', 'creator': 'Microsoft® Word für Microsoft 365', 'creationdate': '2025-04-08T10:47:12+02:00', 'title': 'Kontierungshandbuch', 'author': 'Ein zufriedengestellter Microsoft Office-Anwender', 'moddate': '2025-04-08T10:47:12+02:00', 'source': 'data/sample_accounting_guide.pdf', 'total_pages': 70, 'page': 1, 'page_label': '2'}\n",
      "\n",
      "--- Chunk 5 ---\n",
      "page_content='Kontierungshandbuch \n",
      " \n",
      " \n",
      "3 von 70 \n",
      "3.2.4.1 Identische Verbindlichkeitsaufwertung aus L u. L (Währung) 57 \n",
      "3.2.4.2 Verbindlichkeitsabwertung aus L u. L US GAAP (Währung) 59 \n",
      "3.2.4.3 Ausweis unrealisierter Gewinne bei Verbindlichkeiten 59 \n",
      "3.2.5 Verbindlichkeitsumbuchung von kurzfristig auf langfristig 60 \n",
      "3.2.6 Verbindlichkeitsumbuchung von langfristig auf kurzfristig 61 \n",
      "3.2.6.1 Verbindlichkeitsumbuchung des kurzfristigen Anteils der langfristigen \n",
      "Verbindlichkeiten 61 \n",
      "3.2.6.2 Abbau des kurzfristigen Anteils der langfristigen Verbindlichkeiten 61 \n",
      "3.2.6.3 Verbindlichkeitsumbuchung des kurzfristigen Anteils der langfristigen \n",
      "Verbindlichkeiten 62 \n",
      "3.2.6.4 Abbau des kurzfristigen Anteils der langfristigen Verbindlichkeiten 62 \n",
      "3.2.6.5 Umbuchung des kurzfristigen Anteils langfristiger Verbindlichkeiten aus Capital \n",
      "Lease 63 \n",
      "3.2.6.6 Abbau des kurzfristigen Anteils langfristiger Verbindlichkeiten aus Capital Lease \n",
      "  63 \n",
      "3.3 Passive latente Steuern 64 \n",
      "3.3.1 Buchung der passivischen Steuerdifferenz 65 \n",
      "3.3.2 Auflösung der passivischen Steuerdifferenz 66 \n",
      "4 Die Gliederung der GuV nach dem Umsatzkostenverfahren 66 \n",
      "4.1 Umsatzkostenverfahren nach US GAAP bei #### 68 \n",
      " \n",
      " \n",
      " \n",
      "1 Einleitung \n",
      " \n",
      "1.1 Inhalt und Aufbau des Handbuchs \n",
      " \n",
      "Dieses Kontierungshandbuch soll die Handhabung der parallelen Buchführung  gemäß \n",
      "deutschem Handelsrecht (HGB) und der US GAAP anhand von Sachverhalten verdeutlichen, die \n",
      "für den Einzelabschluß von Gesellschaften des #### Konzerns von Belang sind. Die \n",
      "Buchungsbeispiele wurden primär für Gesellschaften erstellt, die für ihre kaufmännischen \n",
      "Anwendungen SAP R/3 einsetzen, können jedoch hinsichtlich der grundlegenden Buchungslogik \n",
      "auf alle Gesellschaften übertragen werden, die eine kaufmännische Anwendungssoftware \n",
      "verwenden, in der die Integration von externem und internem Rechnungswesen zu gewährleisten \n",
      "ist. Im Besonderen werden nur die Unterschiede in der Gliederung oder Bewertung der \n",
      "verschiedenen Rechnungslegungen thematisiert. Der Aufbau des Handbuchs  lehnt sich an die Gliederung der Bilanz und GuV nach Handelsrecht \n",
      "an, Unterschiede zu US -amerikanischen Vorschriften werden an den jeweiligen Positionen \n",
      "angesprochen. Zur Ermittlung von Bemessungsgrundlagen sind teilweise Bewertungsschemata \n",
      "eingefügt.' metadata={'producer': 'Microsoft® Word für Microsoft 365', 'creator': 'Microsoft® Word für Microsoft 365', 'creationdate': '2025-04-08T10:47:12+02:00', 'title': 'Kontierungshandbuch', 'author': 'Ein zufriedengestellter Microsoft Office-Anwender', 'moddate': '2025-04-08T10:47:12+02:00', 'source': 'data/sample_accounting_guide.pdf', 'total_pages': 70, 'page': 2, 'page_label': '3'}\n"
     ]
    }
   ],
   "source": [
    "# function A2: load the pdf-file containing the accounting assignment guide data from a folder and splitt the data into text_chunks\n",
    "# function A2.2.3 split document in chunks - version 3: Semantic Chunking with LChain \n",
    "# (see LChain docs - https://python.langchain.com/docs/how_to/semantic-chunker/)\n",
    "# problem: SemanticChunker needs String-Structure, load document has LChain document-object-structure\n",
    "\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from gen_ai_hub.proxy.langchain.init_models import init_embedding_model\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Input: documents (from function A2 - load data)\n",
    "# Output: text_chunks\n",
    "\n",
    "# load env-key for embedidding model SAP AI Core\n",
    "ai_core_embedding_model_name = str(os.getenv('AICORE_DEPLOYMENT_MODEL_EMBEDDING'))\n",
    "\n",
    "# init embedding-instance\n",
    "try:\n",
    "    embeddings = init_embedding_model(ai_core_embedding_model_name)\n",
    "    print(\"Embedding model initialized successfully.\")\n",
    "except Exception as e:\n",
    "    print(\"Embedding model not initialized.\")\n",
    "    print(e)\n",
    "\n",
    "\n",
    "# Init text_splitter-Instance with type \"gradient\" \n",
    "\n",
    "text_splitter = SemanticChunker(\n",
    "    embeddings=embeddings,\n",
    "    breakpoint_threshold_type=\"gradient\"\n",
    ")\n",
    "\n",
    "print(\"text-splitter-instance set!\")\n",
    "\n",
    "# Init text_splitter-Instance - other types\n",
    "# \n",
    "# text_splitter = SemanticChunker(embeddings)\n",
    "# text_splitter = SemanticChunker(embeddings=embeddings, breakpoint_threshold_type=\"interquartile\")\n",
    "# text_splitter = SemanticChunker(embeddings=embeddings, breakpoint_threshold_type=\"standard_deviation\")\n",
    "# text_splitter = SemanticChunker(embeddings=embeddings, breakpoint_threshold_type=\"percentile\")\n",
    "\n",
    "# We split text in the usual way, e.g., by invoking .create_documents to create LangChain Document objects\n",
    "# docs = text_splitter.create_documents([state_of_the_union] <- List not document-object)\n",
    "\n",
    "\n",
    "# split docs for every page in documents with SematicChunker-Splitter and rebuild document-object Lchain\n",
    "text_chunks = []\n",
    "for doc in documents:\n",
    "    text_split = text_splitter.split_text(doc.page_content)\n",
    "    # rebuild documents-objekt in LChain-Document-Structure\n",
    "    for text in text_split:\n",
    "        text_chunks.append(Document(page_content=text, metadata=doc.metadata))\n",
    "\n",
    "\n",
    "print(f\"\\n📄 Insgesamt {len(text_chunks)} Chunks generiert.\")\n",
    "for i, chunk in enumerate(text_chunks[:5]):\n",
    "    print(f\"\\n--- Chunk {i+1} ---\\n{chunk}\")\n",
    "\n",
    "### - other method for splitting an rebuild\n",
    "\n",
    "# Extract text from Document-Objekten ()\n",
    "# texts = [doc.page_content for doc in documents]\n",
    "# metadatas = [doc.metadata for doc in documents]\n",
    "\n",
    "# all_split_texts = []\n",
    "# all_split_metadatas = []\n",
    "\n",
    "# # Verarbeiten Sie jeden Text einzeln mit split_text (SemanticChunker works with a list not with LChain-Document-object)\n",
    "# for i, text in enumerate(texts):\n",
    "#     split_texts = text_splitter.split_text(text)\n",
    "#     split_metadatas = [metadatas[i]] * len(split_texts) # Metadaten für jeden Chunk duplizieren\n",
    "#     all_split_texts.extend(split_texts)\n",
    "#     all_split_metadatas.extend(split_metadatas)\n",
    "\n",
    "# # create new document from splitted Texte and Metadaten\n",
    "# split_docs = [Document(page_content=text, metadata=metadata)\n",
    "#               for text, metadata in zip(all_split_texts, all_split_metadatas)]\n",
    "\n",
    "# print(split_docs[0].page_content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function A3 - vectorize the splitted data with embedding function \n",
    "# function A3.1 init embeddings-instance\n",
    "# Use the embedding models from SAP AI-hub for embedding.\n",
    "\n",
    "from gen_ai_hub.proxy.langchain.init_models import init_embedding_model\n",
    "\n",
    "ai_core_embedding_model_name = str(os.getenv('AICORE_DEPLOYMENT_MODEL_EMBEDDING'))\n",
    " \n",
    "try:\n",
    "    embeddings = init_embedding_model(ai_core_embedding_model_name)\n",
    "    print(\"Embedding model initialized successfully.\")\n",
    "except Exception as e:\n",
    "    print(\"Embedding model not initialized.\")\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Successfully created SAP HANA VectorStore interface: <dbapi.Connection Connection object : bfff8255-c34a-41cb-a822-bf9b5f56fb16.hna0.prod-eu20.hanacloud.ondemand.com,443,DBADMIN,DB@hmx04,True>\n",
      "    and SAP HANA table: ACCOUNTING_ASSIGN_SUPPORT_TABLE_DBADMIN.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# function A4 - store: create/clear a sap hana database - table and store the vector in this table\n",
    "# function A4.1 - Create a LangChain VectorStore interface for the HANA database and specify the table \n",
    "# Hint: check table creation with sap-hana-database explorer: select * from ACCOUNTING_ASSIGN_SUPPORT_TABLE_DBADMIN\n",
    "\n",
    "from langchain_community.vectorstores.hanavector import HanaDB\n",
    "\n",
    "vector_table_name = str(os.getenv('hdb_table_name'))\n",
    "\n",
    "hana_database = HanaDB(\n",
    "    embedding = embeddings, \n",
    "    connection = hana_connection, \n",
    "    table_name = vector_table_name\n",
    ")\n",
    "\n",
    "try:\n",
    "    print(f\"\"\"\n",
    "    Successfully created SAP HANA VectorStore interface: {hana_database.connection}\n",
    "    and SAP HANA table: {vector_table_name}.\n",
    "    \"\"\")\n",
    "except Exception as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully added 139 document chunks to the database.\n",
      "table-name:  ACCOUNTING_ASSIGN_SUPPORT_TABLE_DBADMIN\n",
      "Successfully connected to the HANA Cloud database.\n"
     ]
    }
   ],
   "source": [
    "# function A4 - store: create/clear a sap hana database - table and store the vector in this table\n",
    "# function A4.2.1 - delete existing documents from the table and load embeddings to SAP HANA-Table\n",
    "\n",
    "# Delete already existing documents from the SAP HANA table\n",
    "hana_database.delete(filter={})\n",
    "\n",
    "# add the loaded document text_chunks\n",
    "hana_database.add_documents(text_chunks)\n",
    "\n",
    "print(f\"Successfully added {len(text_chunks)} document chunks to the database.\")\n",
    "print(\"table-name: \",hana_database.table_name)\n",
    "print(\"Successfully connected to the HANA Cloud database.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Lade Dokument...\n",
      "🧩 Erstelle Chunks...\n",
      "🧠 Verwende Sprache für Tokenizer: german\n",
      "✅ 1397 Chunks wurden erfolgreich zur HANA-Datenbank hinzugefügt.\n",
      "📁 Tabellenname: ACCOUNTING_ASSIGN_SUPPORT_TABLE_DBADMIN\n"
     ]
    }
   ],
   "source": [
    "# function A2 - load and splitt: load the pdf-file containing the accounting assignment guide data from a folder and splitt the data into text_chunks\n",
    "# function A3 - vectorize and embedd: vectorize the splitted data with embedding function. Use an embedding function to convert the text chunks into vector representations\n",
    "# function A4 - store: create/clear a sap hana database - table and store the vector in this table\n",
    "\n",
    "# function A2/A3/A4 as one\n",
    "\n",
    "#from my_chunking_module import PDFSemanticChunker  # falls es in Datei gespeichert ist\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = \"data/sample_accounting_guide.pdf\"\n",
    "\n",
    "    # init chunker-instance\n",
    "\n",
    "    chunker = PDFSemanticChunker(\n",
    "        pdf_path=file_path,\n",
    "        language=\"german\",\n",
    "        chunk_size=100,\n",
    "        chunk_overlap=20\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # 1. Chunking ausführen\n",
    "        text_chunks = chunker.process()\n",
    "\n",
    "        # 2. Optional - Umwandeln in LangChain Document-Objekte\n",
    "        # from langchain.schema import Document\n",
    "        # documents = [Document(page_content=chunk) for chunk in text_chunks]\n",
    "\n",
    "        # 3. Vorherige Daten in HANA löschen\n",
    "        hana_database.delete(filter={})\n",
    "\n",
    "        # 4. Neue Dokumente hinzufügen\n",
    "        hana_database.add_documents(documents)\n",
    "\n",
    "        print(f\"✅ {len(documents)} Chunks wurden erfolgreich zur HANA-Datenbank hinzugefügt.\")\n",
    "        print(\"📁 Tabellenname:\", hana_database.table_name)\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"❌ Datei nicht gefunden: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check function A4.2 - query to the table to verify embeddings\n",
    "\n",
    "cursor = hana_connection.cursor()\n",
    "sql = f'SELECT VEC_TEXT, TO_NVARCHAR(VEC_VECTOR) FROM \"{hana_database.table_name}\"'\n",
    "\n",
    "cursor.execute(sql)\n",
    "vectors = cursor.fetchall()\n",
    "\n",
    "print(vectors[:1])\n",
    "\n",
    "# for vector in vectors:\n",
    "#     print(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrap-Book Development Modul A\n",
    "\n",
    "* function A2: load the pdf-file and split\n",
    "* function A4.3: Check retrieval for the embeddings in the SAP-Hana-Database\n",
    "* check function A4.3: check retrieval from SAP HANA DB with prompt using chain_type=\"map_reduce\"\n",
    "* check function A4.3: check retrieval from SAP HANA DB with prompt using chain_type=\"stuff\"\n",
    "* check function A4.3: check retrieval from SAP HANA DB with prompt using chain_type=\"map_reduce\" and optimized prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function A2: load the pdf-file containing the accounting assignment guide data from a folder and splitt the data into text_chunks\n",
    "# the funktion uses the os modules to read the file and split the data with langChain-modules\n",
    "\n",
    "\n",
    "import os\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "def load_and_split_pdf(file_path):\n",
    "    # Check if the file exists\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n",
    "    \n",
    "    # Load the PDF file\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    documents = loader.load()\n",
    "    \n",
    "    # Split the documents into text_chunks\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=3000, chunk_overlap=200)\n",
    "    text_chunks = text_splitter.split_documents(documents)\n",
    "    \n",
    "    return text_chunks\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Test the function with a sample PDF file path\n",
    "    file_path = \"data/sample_accounting_guide.pdf\"\n",
    "    try:\n",
    "        text_chunks = load_and_split_pdf(file_path)\n",
    "        print(f\"Number of text_chunks created: {len(text_chunks)}\")\n",
    "        print(f\"First text_chunks: {text_chunks[0]}\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Wir splitten zuerst nach Absätzen und dann rekursiv, falls ein Absatz zu groß ist\n",
    "paragraphs = text.split(\"\\n\\n\")  # Annahme: Absätze sind durch zwei Zeilen getrennt\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300,  # Gesamtgröße eines Chunks\n",
    "    chunk_overlap=50,  # Überlappung in Tokens/Zeichen\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \"],  # Reihenfolge der Split-Priorität\n",
    ")\n",
    "\n",
    "chunks = []\n",
    "for para in paragraphs:\n",
    "    if para.strip():  # Leere Absätze überspringen\n",
    "        sub_chunks = text_splitter.split_text(para)\n",
    "        chunks.extend(sub_chunks)\n",
    "\n",
    "# Ausgabe\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"--- Chunk {i+1} ---\")\n",
    "    print(chunk)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check retrievaleval for the embeddings in the SAP-Hana-Database\n",
    "\n",
    "This code snippet integrates various components from the langchain library to create a retrieval-based question-answering (QA) system. Here's a breakdown of the key parts and their functionality:\n",
    "\n",
    "Retriever Initialization: The db.as_retriever function is used to initialize a retriever object with specific search arguments ('k':20), which likely defines the number of search results to consider.\n",
    "\n",
    "Prompt Template : The PromptTemplate was defined in the previous step that instructs how to use the context to answer a question. It emphasizes not to fabricate answers if the information is unavailable. The template also outlines the structure for the expected JSON output with various product and supplier details.The prompt template is crucial for guiding the model's responses, ensuring that the answers are relevant and accurate based on the retrieved information.\n",
    "\n",
    "Once the retriever and prompt template are set up, the next step involves using the LLM (Language Model) to generate answers based on the retrieved documents. This process typically includes passing the retrieved context to the LLM along with the user's query, allowing it to formulate a coherent and contextually appropriate response.After that, the LLM processes the prompt and generates a response, which can then be formatted and returned to the user, ensuring a seamless interaction with the QA system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function A4.3: Check retrievaleval for the embeddings in the SAP-Hana-Database\n",
    "\n",
    "import os\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "llm_chain = prompt | llm\n",
    "\n",
    "question = \"What NFL team won the Super Bowl in the year Justin Bieber was born?\"\n",
    "\n",
    "print(llm_chain.invoke({'question': question}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check function 4.2 : check retrieval from SAP HANA DB with prompt using chain_type=\"map_reduce\"\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "map_template = \"\"\"\n",
    "Analysiere den folgenden Kontext und extrahiere relevante Informationen zur Kontierung:\n",
    "\n",
    "{context}\n",
    "\n",
    "Frage: {question}\n",
    "\n",
    "Gib die relevanten Informationen in einem kurzen Zwischenergebnis zurück.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "reduce_template = \"\"\"\n",
    "Basierend auf den folgenden Zwischenergebnissen, erstelle eine finale Antwort:\n",
    "\n",
    "{summerization}\n",
    "\n",
    "Frage: {question}\n",
    "\n",
    "Formatiere die Ergebnisse in einer Liste von JSON-Elementen mit den folgenden Schlüsseln:\n",
    "\"Geschäftsfall\"\n",
    "\"Konto Soll\"\n",
    "\"Konto Haben\"\n",
    "\n",
    "Die Ergebnisse dürfen keine json markdown codeblock syntax enthalten.\n",
    "Wenn keine relevanten Informationen gefunden wurden, gib an dass Du keine Antwort kennst.\n",
    "\"\"\"\n",
    "\n",
    "MAP_PROMPT = PromptTemplate(template=map_template, input_variables=[\"context\", \"question\"])\n",
    "REDUCE_PROMPT = PromptTemplate(template=reduce_template, input_variables=[\"summerization\", \"question\"])\n",
    "\n",
    "chain_type_kwargs = {\n",
    "    \"question_prompt\": MAP_PROMPT,\n",
    "    \"reduce_prompt\": REDUCE_PROMPT\n",
    "}\n",
    "\n",
    "question = \"Finde Kontierung für die Umbuchung von langfristigen Forderungen\"\n",
    "\n",
    "retriever = hana_database.as_retriever(search_kwargs={'k':20})\n",
    "\n",
    "question_answer_retriever = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    chain_type='map_reduce',\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "answer = question_answer_retriever.run(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check function 4.2 : check retrieval from SAP HANA DB with prompt using chain_type=\"stuff\"\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "prompt_template = \"\"\"Verwende den folgenden Kontext, um die Frage am Ende zu beantworten. Wenn du die Antwort nicht kennst,\n",
    "    sage einfach, dass du es nicht weißt. Versuche nicht, eine Antwort zu erfinden. Formatiere die Ergebnisse als Liste von JSON-Elementen mit den folgenden Schlüsseln:\n",
    "\n",
    "    \"Geschäftsfall\",\n",
    "    \"Konto Soll\",\n",
    "    \"Konto Haben\"\n",
    "\n",
    "    Füge keine JSON-Markdown-Codeblock-Syntax in die Ergebnisse ein.\n",
    "\n",
    "    {context}\n",
    "\n",
    "    Frage: {question}\n",
    "\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(template=prompt_template, \n",
    "                       input_variables=[\"context\", \"question\"]\n",
    "                      )\n",
    "    \n",
    "chain_type_kwargs = {\"prompt\": PROMPT}\n",
    "\n",
    "question = \"Finde Kontierung für die Buchung von Rückstellungen\"\n",
    "\n",
    "count_retrieved_documents = 10\n",
    "\n",
    "retriever = hana_database.as_retriever(search_kwargs={'k': count_retrieved_documents})\n",
    "# hint: k smaller than 20 -> to much tokens\n",
    "\n",
    "question_answer_retriever = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    chain_type=\"stuff\",\n",
    "    chain_type_kwargs=chain_type_kwargs,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "answer = question_answer_retriever.run(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check function 4.2 : check retrieval from SAP HANA DB with prompt using chain_type=\"map_reduce\" (prompt-finetuning)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Prompt für die Map-Phase\n",
    "map_prompt_template = \"\"\"Analysiere den folgenden Kontext und extrahiere relevante Kontierungsinformationen für den Geschäftsfall.\n",
    "Wenn keine relevanten Informationen im Kontext gefunden werden können, gib die Antwort zurück: \"Ich habe keine Kontierungsinformationen zum Geschäftsfall gefunden\".\n",
    "\n",
    "{context}\n",
    "\n",
    "Frage: {question}\n",
    "\"\"\"\n",
    "\n",
    "# Prompt für die Combine/Reduce-Phase\n",
    "combine_prompt_template = \"\"\"\n",
    "- Fasse die folgenden Kontierungsinformationen zusammen und entferne Duplikate.\n",
    "- Gib nur die relevantesten und eindeutigsten Kontierungen zurück. \n",
    "- Wenn keine relevanten Informationen im Kontext gefunden werden können, gib die Antwort zurück: \"Ich habe keine Kontierungsinformationen zum Geschäftsfall gefunden\".\n",
    "- Wenn relevante Informationen gefunden wurden gib diese Informationen im folgenden Struktur aus:\n",
    "    ## Geschäftsfall: <Bezeichnung des Geschäftsfalls>\n",
    "    ## Kontierung\n",
    "    Konto-Soll: <Konto-Soll> - <Bezeichnung Konto-Soll> AN Konto-Haben: <Konto-Haben> - <Bezeichnung Konto-Haben>\n",
    "\n",
    "Beispiel:\n",
    "\n",
    "-----------------------------\n",
    "## Geschäftsfall: Bildung von Rückstellungen\n",
    "    ## Kontierung\n",
    "    Konto-Soll: L160501 - LC Instandhaltungskosten (Gebäude) AN Konto-Haben: L3909101 - LC Sonstige Rückstellungen\n",
    "-----------------------------\n",
    "\n",
    "{summaries}\n",
    "\n",
    "Frage: {question}\n",
    "\"\"\"\n",
    "\n",
    "# Korrekte Struktur für chain_type_kwargs\n",
    "chain_type_kwargs = {\n",
    "    \"question_prompt\": PromptTemplate(\n",
    "        template=map_prompt_template,\n",
    "        input_variables=[\"context\", \"question\"]\n",
    "    ),\n",
    "    \"combine_prompt\": PromptTemplate(\n",
    "        template=combine_prompt_template,\n",
    "        input_variables=[\"summaries\", \"question\"]\n",
    "    )\n",
    "}\n",
    "\n",
    "question = \"Finde Kontierung für die Buchung von Bildung von Rückstellungen\"\n",
    "\n",
    "retriever = hana_database.as_retriever(search_kwargs={'k': 20})\n",
    "\n",
    "question_answer_retriever = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"map_reduce\",\n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs=chain_type_kwargs,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "answer = question_answer_retriever.run(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example setup SAP HANA Vector Database\n",
    "\n",
    "Wichtige Aspekte des Codes:\n",
    "Tabellenstruktur:\n",
    "ID: Eindeutiger Identifier für jeden Eintrag\n",
    "DOCUMENT: Der eigentliche Dokumententext\n",
    "METADATA: JSON-formatierte Metadaten\n",
    "EMBEDDING: Der Embedding-Vektor als BLOB\n",
    "VECTOR_DIMENSION: Dimension des Embedding-Vektors (1536 für ada-002)\n",
    "Vector-Index:\n",
    "Verwendet HANA's native Vektorindexierung\n",
    "Cosine-Similarity mit Threshold 0.75\n",
    "Optimiert für 1536-dimensionale Vektoren\n",
    "Testdaten:\n",
    "Beispieldaten für Kontierungsregeln\n",
    "Dummy-Embeddings für Testzwecke\n",
    "Strukturierte Metadaten im JSON-Format\n",
    "Verifikation:\n",
    "Überprüft Tabellenstruktur\n",
    "Zeigt vorhandene Indizes\n",
    "Gibt Anzahl der Datensätze aus\n",
    "Um den Code zu verwenden:\n",
    "1. Stellen Sie sicher, dass die Umgebungsvariablen gesetzt sind\n",
    "2. Führen Sie das Setup aus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HDB_HOST'] = 'your_host'\n",
    "os.environ['HDB_PORT'] = 'your_port'\n",
    "os.environ['HDB_USER'] = 'your_user'\n",
    "os.environ['HDB_PASSWORD'] = 'your_password'\n",
    "os.environ['HDB_SCHEMA'] = 'your_schema'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabelle erstellen und Testdaten einfügen\n",
    "create_vector_table(connection_params)\n",
    "insert_test_data(connection_params)\n",
    "verify_table_setup(connection_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exmaple setup code SAP HANA Vector DB (cursor)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "from hdbcli import dbapi\n",
    "import numpy as np\n",
    "\n",
    "def create_vector_table(connection_params, table_name=\"VECTOR_TABLE\"):\n",
    "    \"\"\"\n",
    "    Erstellt eine Vektortabelle in SAP HANA für die Speicherung von Dokumenten und deren Embeddings.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Verbindung zur HANA-Datenbank herstellen\n",
    "        conn = dbapi.connect(\n",
    "            address=connection_params['address'],\n",
    "            port=connection_params['port'],\n",
    "            user=connection_params['user'],\n",
    "            password=connection_params['password']\n",
    "        )\n",
    "        \n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Zum angegebenen Schema wechseln\n",
    "        if 'schema' in connection_params:\n",
    "            cursor.execute(f\"SET SCHEMA {connection_params['schema']}\")\n",
    "\n",
    "        # Tabelle erstellen, falls sie nicht existiert\n",
    "        create_table_sql = f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "            ID NVARCHAR(100) PRIMARY KEY,\n",
    "            DOCUMENT NCLOB,\n",
    "            METADATA NCLOB,\n",
    "            EMBEDDING BLOB,\n",
    "            VECTOR_DIMENSION INTEGER\n",
    "        )\n",
    "        \"\"\"\n",
    "        cursor.execute(create_table_sql)\n",
    "        \n",
    "        # Vector-Index erstellen\n",
    "        create_index_sql = f\"\"\"\n",
    "        CREATE INDEX IF NOT EXISTS IDX_{table_name}_VECTOR \n",
    "        ON {table_name}(EMBEDDING) \n",
    "        VECTOR DIMENSION 1536 \n",
    "        DOUBLE COSINE THRESHOLD 0.75\n",
    "        \"\"\"\n",
    "        cursor.execute(create_index_sql)\n",
    "        \n",
    "        print(f\"Tabelle {table_name} und Vector-Index wurden erfolgreich erstellt.\")\n",
    "        \n",
    "        # Überprüfen, ob die Tabelle leer ist\n",
    "        cursor.execute(f\"SELECT COUNT(*) FROM {table_name}\")\n",
    "        count = cursor.fetchone()[0]\n",
    "        print(f\"Anzahl der Einträge in der Tabelle: {count}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Fehler beim Erstellen der Tabelle: {str(e)}\")\n",
    "        raise\n",
    "    finally:\n",
    "        if 'cursor' in locals():\n",
    "            cursor.close()\n",
    "        if 'conn' in locals():\n",
    "            conn.close()\n",
    "\n",
    "def insert_test_data(connection_params, table_name=\"VECTOR_TABLE\"):\n",
    "    \"\"\"\n",
    "    Fügt Testdaten in die Vektortabelle ein.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = dbapi.connect(\n",
    "            address=connection_params['address'],\n",
    "            port=connection_params['port'],\n",
    "            user=connection_params['user'],\n",
    "            password=connection_params['password']\n",
    "        )\n",
    "        \n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        if 'schema' in connection_params:\n",
    "            cursor.execute(f\"SET SCHEMA {connection_params['schema']}\")\n",
    "\n",
    "        # Beispiel-Kontierungsdaten\n",
    "        test_data = [\n",
    "            {\n",
    "                'id': 'doc1',\n",
    "                'document': 'Buchung von Rückstellungen für Garantieverpflichtungen',\n",
    "                'metadata': '{\"type\": \"accounting_rule\", \"category\": \"provisions\"}',\n",
    "                'embedding': np.random.rand(1536).astype(np.float64)  # Dummy-Embedding\n",
    "            },\n",
    "            {\n",
    "                'id': 'doc2',\n",
    "                'document': 'Anlagenzugang durch Kauf einer Maschine',\n",
    "                'metadata': '{\"type\": \"accounting_rule\", \"category\": \"fixed_assets\"}',\n",
    "                'embedding': np.random.rand(1536).astype(np.float64)\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        # Daten einfügen\n",
    "        for data in test_data:\n",
    "            insert_sql = f\"\"\"\n",
    "            INSERT INTO {table_name} \n",
    "            (ID, DOCUMENT, METADATA, EMBEDDING, VECTOR_DIMENSION) \n",
    "            VALUES(?, ?, ?, ?, 1536)\n",
    "            \"\"\"\n",
    "            \n",
    "            cursor.execute(insert_sql, \n",
    "                         (data['id'], \n",
    "                          data['document'], \n",
    "                          data['metadata'], \n",
    "                          data['embedding'].tobytes()))\n",
    "\n",
    "        conn.commit()\n",
    "        print(f\"Testdaten wurden erfolgreich in {table_name} eingefügt.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Fehler beim Einfügen der Testdaten: {str(e)}\")\n",
    "        raise\n",
    "    finally:\n",
    "        if 'cursor' in locals():\n",
    "            cursor.close()\n",
    "        if 'conn' in locals():\n",
    "            conn.close()\n",
    "\n",
    "def verify_table_setup(connection_params, table_name=\"VECTOR_TABLE\"):\n",
    "    \"\"\"\n",
    "    Überprüft die Einrichtung der Vektortabelle.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = dbapi.connect(\n",
    "            address=connection_params['address'],\n",
    "            port=connection_params['port'],\n",
    "            user=connection_params['user'],\n",
    "            password=connection_params['password']\n",
    "        )\n",
    "        \n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        if 'schema' in connection_params:\n",
    "            cursor.execute(f\"SET SCHEMA {connection_params['schema']}\")\n",
    "\n",
    "        # Tabellenstruktur überprüfen\n",
    "        cursor.execute(f\"\"\"\n",
    "        SELECT COLUMN_NAME, DATA_TYPE_NAME, LENGTH, IS_NULLABLE \n",
    "        FROM TABLE_COLUMNS \n",
    "        WHERE TABLE_NAME = '{table_name.upper()}'\n",
    "        \"\"\")\n",
    "        \n",
    "        print(\"\\nTabellenstruktur:\")\n",
    "        for col in cursor.fetchall():\n",
    "            print(f\"Spalte: {col[0]}, Typ: {col[1]}, Länge: {col[2]}, Nullable: {col[3]}\")\n",
    "\n",
    "        # Index überprüfen\n",
    "        cursor.execute(f\"\"\"\n",
    "        SELECT INDEX_NAME, CONSTRAINT \n",
    "        FROM INDEXES \n",
    "        WHERE TABLE_NAME = '{table_name.upper()}'\n",
    "        \"\"\")\n",
    "        \n",
    "        print(\"\\nIndizes:\")\n",
    "        for idx in cursor.fetchall():\n",
    "            print(f\"Index: {idx[0]}, Constraint: {idx[1]}\")\n",
    "\n",
    "        # Datenbestand überprüfen\n",
    "        cursor.execute(f\"SELECT COUNT(*) FROM {table_name}\")\n",
    "        count = cursor.fetchone()[0]\n",
    "        print(f\"\\nAnzahl der Datensätze: {count}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler bei der Überprüfung: {str(e)}\")\n",
    "        raise\n",
    "    finally:\n",
    "        if 'cursor' in locals():\n",
    "            cursor.close()\n",
    "        if 'conn' in locals():\n",
    "            conn.close()\n",
    "\n",
    "# Verwendung:\n",
    "if __name__ == \"__main__\":\n",
    "    # Verbindungsparameter (sollten aus Umgebungsvariablen kommen)\n",
    "    connection_params = {\n",
    "        'address': os.getenv('HDB_HOST'),\n",
    "        'port': os.getenv('HDB_PORT'),\n",
    "        'user': os.getenv('HDB_USER'),\n",
    "        'password': os.getenv('HDB_PASSWORD'),\n",
    "        'schema': os.getenv('HDB_SCHEMA')\n",
    "    }\n",
    "\n",
    "    table_name = \"VECTOR_TABLE\"\n",
    "\n",
    "    try:\n",
    "        # Tabelle erstellen\n",
    "        create_vector_table(connection_params, table_name)\n",
    "        \n",
    "        # Optional: Testdaten einfügen\n",
    "        insert_test_data(connection_params, table_name)\n",
    "        \n",
    "        # Setup überprüfen\n",
    "        verify_table_setup(connection_params, table_name)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Fehler beim Setup: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example insert documentws in SAP HANA\n",
    "\n",
    "Geschäftsfall          | Konto_Soll | Konto_Haben | Beschreibung\n",
    "---------------------- | ----------- | ----------- | ------------\n",
    "Rückstellung_Garantie  | 6815       | 3050        | Buchung von Rückstellungen für Garantieverpflichtungen...\n",
    "Anlagenzugang_Kauf    | 0410       | 2800        | Anschaffung einer neuen Produktionsmaschine...\n",
    "\n",
    "Die wichtigsten Features:\n",
    "Excel-Verarbeitung:\n",
    "Liest Kontierungsregeln aus Excel\n",
    "Unterstützt strukturierte Daten mit Geschäftsfall, Konten und Beschreibung\n",
    "Embedding-Erstellung:\n",
    "Verwendet OpenAI's ada-002 Modell\n",
    "Verarbeitet Dokumente in Batches\n",
    "Fortschrittsanzeige mit tqdm\n",
    "Datenbankintegration:\n",
    "Sichere Verbindungshandhabung\n",
    "Effiziente Batch-Verarbeitung\n",
    "Fehlerbehandlung und Logging\n",
    "Metadaten-Handling:\n",
    "Strukturierte Speicherung der Kontierungsinformationen\n",
    "JSON-Format für flexible Erweiterbarkeit\n",
    "Verwendung:\n",
    "Excel-Datei vorbereiten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beispiel für manuelle Datenerstellung\n",
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    'Geschäftsfall': ['Rückstellung_Garantie', 'Anlagenzugang_Kauf'],\n",
    "    'Konto_Soll': ['6815', '0410'],\n",
    "    'Konto_Haben': ['3050', '2800'],\n",
    "    'Beschreibung': [\n",
    "        'Buchung von Rückstellungen für Garantieverpflichtungen...',\n",
    "        'Anschaffung einer neuen Produktionsmaschine...'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_excel('kontierungsregeln.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processor initialisieren und ausführen\n",
    "processor = DocumentProcessor(connection_params)\n",
    "processor.process_and_store_documents('kontierungsregeln.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "from typing import List, Dict\n",
    "import json\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import uuid\n",
    "\n",
    "class DocumentProcessor:\n",
    "    def __init__(self, connection_params: Dict, table_name: str = \"VECTOR_TABLE\"):\n",
    "        \"\"\"\n",
    "        Initialisiert den Document Processor.\n",
    "        \n",
    "        Args:\n",
    "            connection_params: Dictionary mit HANA-Verbindungsparametern\n",
    "            table_name: Name der Vektortabelle\n",
    "        \"\"\"\n",
    "        self.connection_params = connection_params\n",
    "        self.table_name = table_name\n",
    "        self.embeddings = OpenAIEmbeddings(\n",
    "            model=\"text-embedding-ada-002\"\n",
    "        )\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=200,\n",
    "            length_function=len\n",
    "        )\n",
    "        \n",
    "    def process_excel_data(self, excel_file: str, sheet_name: str = \"Sheet1\") -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Verarbeitet Excel-Daten mit Kontierungsregeln.\n",
    "        \n",
    "        Args:\n",
    "            excel_file: Pfad zur Excel-Datei\n",
    "            sheet_name: Name des Excel-Sheets\n",
    "        \n",
    "        Returns:\n",
    "            Liste von Dokumenten mit Metadaten\n",
    "        \"\"\"\n",
    "        try:\n",
    "            df = pd.read_excel(excel_file, sheet_name=sheet_name)\n",
    "            documents = []\n",
    "            \n",
    "            for _, row in df.iterrows():\n",
    "                # Annahme: Excel-Spalten sind \"Geschäftsfall\", \"Konto_Soll\", \"Konto_Haben\", \"Beschreibung\"\n",
    "                doc = {\n",
    "                    'id': str(uuid.uuid4()),\n",
    "                    'document': row['Beschreibung'],\n",
    "                    'metadata': json.dumps({\n",
    "                        'Geschäftsfall': row['Geschäftsfall'],\n",
    "                        'Konto_Soll': str(row['Konto_Soll']),\n",
    "                        'Konto_Haben': str(row['Konto_Haben'])\n",
    "                    }, ensure_ascii=False)\n",
    "                }\n",
    "                documents.append(doc)\n",
    "            \n",
    "            return documents\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Fehler beim Verarbeiten der Excel-Datei: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def create_embeddings(self, documents: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Erstellt Embeddings für die Dokumente.\n",
    "        \n",
    "        Args:\n",
    "            documents: Liste von Dokumenten\n",
    "        \n",
    "        Returns:\n",
    "            Liste von Dokumenten mit Embeddings\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(\"Erstelle Embeddings...\")\n",
    "            for doc in tqdm(documents):\n",
    "                # Embedding für den Dokumententext erstellen\n",
    "                embedding = self.embeddings.embed_query(doc['document'])\n",
    "                doc['embedding'] = embedding\n",
    "            return documents\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Fehler beim Erstellen der Embeddings: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def insert_documents(self, documents: List[Dict]):\n",
    "        \"\"\"\n",
    "        Fügt Dokumente in die HANA-Datenbank ein.\n",
    "        \n",
    "        Args:\n",
    "            documents: Liste von Dokumenten mit Embeddings\n",
    "        \"\"\"\n",
    "        from hdbcli import dbapi\n",
    "        import numpy as np\n",
    "        \n",
    "        try:\n",
    "            conn = dbapi.connect(\n",
    "                address=self.connection_params['address'],\n",
    "                port=self.connection_params['port'],\n",
    "                user=self.connection_params['user'],\n",
    "                password=self.connection_params['password']\n",
    "            )\n",
    "            \n",
    "            cursor = conn.cursor()\n",
    "            \n",
    "            if 'schema' in self.connection_params:\n",
    "                cursor.execute(f\"SET SCHEMA {self.connection_params['schema']}\")\n",
    "\n",
    "            print(\"Füge Dokumente in die Datenbank ein...\")\n",
    "            for doc in tqdm(documents):\n",
    "                insert_sql = f\"\"\"\n",
    "                INSERT INTO {self.table_name} \n",
    "                (ID, DOCUMENT, METADATA, EMBEDDING, VECTOR_DIMENSION) \n",
    "                VALUES(?, ?, ?, ?, 1536)\n",
    "                \"\"\"\n",
    "                \n",
    "                # Embedding in bytes konvertieren\n",
    "                embedding_bytes = np.array(doc['embedding']).astype(np.float64).tobytes()\n",
    "                \n",
    "                cursor.execute(insert_sql, \n",
    "                             (doc['id'], \n",
    "                              doc['document'], \n",
    "                              doc['metadata'], \n",
    "                              embedding_bytes))\n",
    "\n",
    "            conn.commit()\n",
    "            print(f\"Alle Dokumente wurden erfolgreich eingefügt.\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Fehler beim Einfügen der Dokumente: {str(e)}\")\n",
    "            raise\n",
    "        finally:\n",
    "            if 'cursor' in locals():\n",
    "                cursor.close()\n",
    "            if 'conn' in locals():\n",
    "                conn.close()\n",
    "\n",
    "    def process_and_store_documents(self, excel_file: str, sheet_name: str = \"Sheet1\"):\n",
    "        \"\"\"\n",
    "        Hauptmethode zum Verarbeiten und Speichern von Dokumenten.\n",
    "        \n",
    "        Args:\n",
    "            excel_file: Pfad zur Excel-Datei\n",
    "            sheet_name: Name des Excel-Sheets\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Dokumente aus Excel laden\n",
    "            documents = self.process_excel_data(excel_file, sheet_name)\n",
    "            print(f\"Anzahl geladener Dokumente: {len(documents)}\")\n",
    "            \n",
    "            # Embeddings erstellen\n",
    "            documents_with_embeddings = self.create_embeddings(documents)\n",
    "            \n",
    "            # Dokumente in die Datenbank einfügen\n",
    "            self.insert_documents(documents_with_embeddings)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Fehler bei der Verarbeitung: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "# Beispiel für die Verwendung:\n",
    "if __name__ == \"__main__\":\n",
    "    # Verbindungsparameter\n",
    "    connection_params = {\n",
    "        'address': os.getenv('HDB_HOST'),\n",
    "        'port': os.getenv('HDB_PORT'),\n",
    "        'user': os.getenv('HDB_USER'),\n",
    "        'password': os.getenv('HDB_PASSWORD'),\n",
    "        'schema': os.getenv('HDB_SCHEMA')\n",
    "    }\n",
    "\n",
    "    # Excel-Datei mit Kontierungsregeln\n",
    "    excel_file = \"kontierungsregeln.xlsx\"\n",
    "\n",
    "    # Document Processor initialisieren und ausführen\n",
    "    processor = DocumentProcessor(connection_params)\n",
    "    \n",
    "    try:\n",
    "        processor.process_and_store_documents(excel_file)\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler beim Gesamtprozess: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
