{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook for use case digital posting assistant - stage1\n",
    "### Module A vectorize accounting assignment guide\n",
    "#### Ojectives\n",
    "- In this module we will develop the load and the vectorization of the text-file for the accounting assignment guide\n",
    "- The vectorized accounting assignemnt guide will finaly stored in a SAP HANA vector database\n",
    "\n",
    "#### Processing steps from concept\n",
    "A0 - preparation\n",
    "\n",
    "A2 - load and splitt: load the pdf-file containing the accounting assignment guide data from a folder and splitt the data into text_chunks\n",
    "\n",
    "A3 - vectorize and embedd: vectorize the splitted data with embedding function. Use an embedding function to convert the text chunks into vector representations\n",
    "\n",
    "A4 - store: create/clear a sap hana database - table and store the vector in this table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A0 - Setup and configuration Modul A\n",
    "\n",
    "The following setup-steps where processed:\n",
    "\n",
    "* A0.0 Start SAP instances\n",
    "* A0.1 install py-packages\n",
    "* A0.2 load env-variables from config.json-file\n",
    "* A0.3 Setup and test connection to HANA DB\n",
    "* A0.4 Setup LLM-Connection to SAP AI-HUB\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A0.0 Start SAP Instances\n",
    "\n",
    "* Start BTP Cockpit\n",
    "* Start SAP Build Dev Space\n",
    "* Start HANA DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A0.1 install py-packages\n",
    "# RESET KERNEL AFTER INSTALLATION\n",
    "\n",
    "%pip install --upgrade pip\n",
    "\n",
    "%pip install hdbcli --break-system-packages\n",
    "%pip install generative-ai-hub-sdk[all] --break-system-packages\n",
    "%pip install folium --break-system-packages\n",
    "%pip install ipywidgets --break-system-packages\n",
    "%pip install pypdf\n",
    "%pip install -U ipykernel\n",
    "%pip install hana-ml\n",
    "%pip install langchain\n",
    "%pip install hdbcli\n",
    "%pip install sqlalchemy-hana\n",
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A0.2 load env-variables from config.json-file\n",
    "# This script loads environment variables from a JSON configuration file\n",
    "# and sets them in the current environment. It raises an error if the file does not exist\n",
    "# or if the JSON file is malformed.\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "def load_env_variables(config_file):\n",
    "    \"\"\"\n",
    "    Load environment variables from a JSON configuration file.\n",
    "\n",
    "    Args:\n",
    "        config_file (str): Path to the JSON configuration file.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the environment variables.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(config_file):\n",
    "        raise FileNotFoundError(f\"The configuration file {config_file} does not exist.\")\n",
    "    \n",
    "    try:\n",
    "        with open(config_file, 'r') as file:\n",
    "            env_variables = json.load(file)\n",
    "    except json.JSONDecodeError as e:\n",
    "        raise ValueError(f\"Error decoding JSON from the configuration file {config_file}: {e}\")\n",
    "    \n",
    "    for key, value in env_variables.items():\n",
    "        # Convert non-string values to strings before setting them in os.environ\n",
    "        if isinstance(value, dict):\n",
    "            value = json.dumps(value)  # Convert dictionaries to JSON strings\n",
    "        os.environ[key] = str(value)\n",
    "    \n",
    "    return env_variables\n",
    "\n",
    "# Example usage\n",
    "config_file = \"/home/user/.aicore/config.json\"\n",
    "try:\n",
    "    env_variables = load_env_variables(config_file)\n",
    "    print(f\"Loaded environment variables: {env_variables}\")\n",
    "except (FileNotFoundError, ValueError) as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A0.2 Test connection with env-Variables to SAP AI core\n",
    "\n",
    "from gen_ai_hub.proxy.native.openai import embeddings\n",
    "\n",
    "response = embeddings.create(\n",
    "    input=\"SAP Generative AI Hub is awesome!\",\n",
    "    model_name=\"text-embedding-ada-002\"\n",
    "    \n",
    ")\n",
    "print(response.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A0.3 Setup and test connection to HANA DB\n",
    "\n",
    "import os\n",
    "# from hana_ml import ConnectionContext\n",
    "from hdbcli import dbapi\n",
    "\n",
    "# Fetch environment variables\n",
    "hdb_host_address = os.getenv(\"hdb_host_address\")\n",
    "hdb_user = os.getenv(\"hdb_user\")\n",
    "hdb_password = os.getenv(\"hdb_password\")\n",
    "hdb_port = os.getenv(\"hdb_port\")\n",
    "\n",
    "# Debugging: Print non-sensitive environment variables\n",
    "print(f\"hdb_host_address: {hdb_host_address}\")\n",
    "print(f\"hdb_user: {hdb_user}\")\n",
    "print(f\"hdb_port: {hdb_port}\")\n",
    "\n",
    "# Ensure variables are defined\n",
    "if not all([hdb_host_address, hdb_user, hdb_password, hdb_port]):\n",
    "    raise ValueError(\"One or more HANA DB connection parameters are missing.\")\n",
    "\n",
    "# Convert port to integer\n",
    "hdb_port = int(hdb_port)\n",
    "\n",
    "# Create a connection to the HANA database\n",
    "# hana_connection = ConnectionContext(\n",
    "#     address=hdb_host_address,\n",
    "#     port=hdb_port,\n",
    "#     user=hdb_user,\n",
    "#     password=hdb_password,\n",
    "#     encrypt=True\n",
    "# )\n",
    "\n",
    "# Test the connection\n",
    "# print(\"HANA DB Version:\", hana_connection.hana_version())\n",
    "# print(\"Current Schema:\", hana_connection.get_current_schema())\n",
    "\n",
    "hana_connection = dbapi.connect(\n",
    "    address=hdb_host_address,\n",
    "    port=hdb_port,\n",
    "    user=hdb_user,\n",
    "    password=hdb_password,\n",
    "    #encrypt=True\n",
    "    autocommit=True,\n",
    "    sslValidateCertificate=False,\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A0.4 Setup LLM-Connection to SAP AI-HUB\n",
    "\n",
    "import os\n",
    "import dotenv\n",
    "from gen_ai_hub.proxy.langchain.openai import ChatOpenAI\n",
    "from gen_ai_hub.proxy.langchain.openai import OpenAI\n",
    "\n",
    "# Lade aicore_model_name aus der Umgebungskonfiguration\n",
    "aicore_model_name = str(os.getenv(\"AICORE_DEPLOYMENT_MODEL\"))\n",
    "\n",
    "# √úberpr√ºfe, ob die Variable definiert ist\n",
    "if not aicore_model_name:\n",
    "    raise ValueError(f\"\"\"Parameter LLM-Model-Name {aicore_model_name} fehlt in der Umgebungskonfiguration.\"\"\")\n",
    "\n",
    "llm = ChatOpenAI(proxy_model_name=aicore_model_name)\n",
    "#llm = OpenAI(proxy_model_name=aicore_model_name)\n",
    "\n",
    "if not llm:\n",
    "    raise ValueError(f\"\"\"Parameter LLM-Model-Name {aicore_model_name} fehlt in der Umgebungskonfiguration.\"\"\")\n",
    "else:\n",
    "    print(f\"\"\"Parameter LLM-Model-Name: {aicore_model_name} wurde erfolgreich geladen.\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### processing functions Modul A\n",
    "\n",
    "- function A2: load the pdf-file with accounting assignment guide data and splitt the data into text_chunk\n",
    "\n",
    "- function A3: vectorize the splitted data with embedding function \n",
    "\n",
    "- function A4.1: create a LangChain VectorStore interface for the HANA database and specify the table\n",
    "\n",
    "- function A4.2: delete existing documents from the table and load embeddings to SAP HANA-Tabele\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function A2: load the pdf-file containing the accounting assignment guide data from a folder and splitt the data into text_chunks\n",
    "# the funktion uses the os modules to read the file and split the data with langChain-modules\n",
    "# function A2.1 load data\n",
    "\n",
    "import os\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "def load_pdf(file_path):\n",
    "    # Check if the file exists\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n",
    "    \n",
    "    # Load the PDF file\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    documents = loader.load()\n",
    "    return (documents)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Test the function with a sample PDF file path\n",
    "    file_path = \"data/sample_accounting_guide.pdf\"\n",
    "    try:\n",
    "        documents = load_pdf(file_path)\n",
    "        print(f\"Length of text created: {len(documents)}\")\n",
    "        print(f\"First page from Text: {documents[1]}\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function A2: load the pdf-file containing the accounting assignment guide data from a folder and splitt the data into text_chunks\n",
    "# the funktion uses the os modules to read the file and split the data with langChain-modules\n",
    "# function A2.2.1 split document in chunks - version 1: Character Text-Splitter)\n",
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "chunk_size_param = 100\n",
    "chunk_overlap_param = 20\n",
    "\n",
    "def split_document_charakter(document):\n",
    "    # Split the documents into text_chunks\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=chunk_size_param, chunk_overlap=chunk_overlap_param)\n",
    "    text_chunks = text_splitter.split_documents(documents)\n",
    "    \n",
    "    return text_chunks\n",
    "\n",
    "   \n",
    "# Test the function with a sample PDF file path\n",
    "text_chunks = split_document_charakter(documents)\n",
    "print(f\"Number of text_chunks created: {len(text_chunks)}\")\n",
    "print(f\"First text_chunks: {text_chunks[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Anzahl Chunks: 430\n",
      "\n",
      "--- Chunk 1 ---\n",
      "Kontierungshandbuch \n",
      " \n",
      " \n",
      "1 von 70 \n",
      "Inhaltsverzeichnis \n",
      "1 Einleitung 3 \n",
      "1.1 Inhalt und Aufbau des Handbuchs 3 \n",
      "1.2 Kontennomenklatur 4 \n",
      "1.3 Buchungslogik und Zusatzkontierungen 5 \n",
      "1.4 Bedienungshinweise zum Kontierungshandbuch 6 \n",
      "1.4.1 Hyperlinks 6 \n",
      "1.4.2 Farbschema 6 \n",
      "2 Bilanz: Aktiva 7\n",
      "\n",
      "--- Chunk 2 ---\n",
      "2 Bilanz: Aktiva 7 \n",
      "2.1 Anlageverm√∂gen 7 \n",
      "2.1.1 Immaterielle Verm√∂gensgegenst√§nde 7 \n",
      "2.1.1.1 Selbsterstellte Software 7 \n",
      "2.1.2 Sachanlagen 9 \n",
      "2.1.2.1 Au√üerplanm√§√üige Abschreibung von Sachanlagen 9 \n",
      "2.1.2.2 Aktivierung der Verbindlichkeiten aus Capital Lease 10\n",
      "\n",
      "--- Chunk 3 ---\n",
      "2.1.2.2.1 Aktivierung der Verbindlichkeiten aus Capital Lease 11 \n",
      "2.1.2.2.2 Aktivierung der Verbindlichkeiten aus Capital Lease 11 \n",
      "2.2 Umlaufverm√∂gen 12 \n",
      "2.2.1 Vorr√§te 12 \n",
      "2.2.1.1 Ermittlung des beizulegenden Werts (Marktwert) 13 \n",
      "2.2.1.2 Niederstwerttest (Lower of Cost or Market Prinzip) 13\n",
      "\n",
      "--- Chunk 4 ---\n",
      "2.2.2 Forderungen und sonstige Verm√∂gensgegenst√§nde 14 \n",
      "2.2.2.1 Forderungen gegen verbundene Unternehmen 15 \n",
      "2.2.2.2 Umbuchung von langfristigen Forderungen auf kurzfristigen Anteil 15 \n",
      "2.2.2.2.1 Umbuchung von langfristigen Darlehensforderungen auf kurzfristigen Anteil 15\n",
      "\n",
      "--- Chunk 5 ---\n",
      "2.2.2.2.2 Abbau des kurzfristigen Anteils der langfristigen Forderungen 16 \n",
      "2.2.2.2.3 Ausgleich der offenen Posten der langfristigen Forderungen und des \n",
      "kurzfristigen Anteils bei R√ºckzahlung des Gesamtdarlehensbetrages 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# function A2: load the pdf-file containing the accounting assignment guide data from a folder and splitt the data into text_chunks\n",
    "# the funktion uses the os modules to read the file and split the data with langChain-modules\n",
    "# function A2.2.2 split document in chunks - version 2: Semantic Chunking (German)))\n",
    "\n",
    "\n",
    "import os\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "chunk_size_param = 100\n",
    "chunk_overlap_param = 20\n",
    "\n",
    "def extract_texts(documents):\n",
    "    # Hole den reinen Text aus jedem Document-Objekt\n",
    "    return [doc.page_content for doc in documents]\n",
    "\n",
    "def semantic_chunks(texts, lang=\"german\", chunk_size=300):\n",
    "    all_sentences = []\n",
    "    for text in texts:\n",
    "        all_sentences.extend(sent_tokenize(text, language=lang))\n",
    "\n",
    "    # Baue gr√∂√üere semantische Chunks (2‚Äì3 S√§tze je nach L√§nge)\n",
    "    chunks = []\n",
    "    chunk = \"\"\n",
    "    for sentence in all_sentences:\n",
    "        if len(chunk) + len(sentence) < chunk_size:\n",
    "            chunk += \" \" + sentence\n",
    "        else:\n",
    "            chunks.append(chunk.strip())\n",
    "            chunk = sentence\n",
    "    if chunk:\n",
    "        chunks.append(chunk.strip())\n",
    "\n",
    "    # Splitting with LangChain\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap_param,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \"]\n",
    "    )\n",
    "\n",
    "    final_chunks = []\n",
    "    for ch in chunks:\n",
    "        final_chunks.extend(splitter.split_text(ch))\n",
    "\n",
    "    return final_chunks\n",
    "\n",
    "# === Main Workflow ===\n",
    "if __name__ == \"__main__\":\n",
    "        \n",
    "    try:\n",
    "        texts = extract_texts(documents)\n",
    "        text_chunks = semantic_chunks(texts)\n",
    "\n",
    "        print(f\"üìÑ Anzahl Chunks: {len(chunks)}\")\n",
    "        for i, ch in enumerate(text_chunks[:5]):  # Nur erste 5 anzeigen\n",
    "            print(f\"\\n--- Chunk {i+1} ---\\n{ch}\")\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "# function A3 - vectorize the splitted data with embedding function \n",
    "# Load the text file containing the accounting assignment guide data from a folder and splitt the data into chunks. \n",
    "# Use the embedding models from SAP AI-hub for embedding.\n",
    "\n",
    "# Initialize embeddings\n",
    "\n",
    "from gen_ai_hub.proxy.langchain.init_models import init_embedding_model\n",
    "\n",
    "ai_core_embedding_model_name = str(os.getenv('AICORE_DEPLOYMENT_MODEL_EMBEDDING'))\n",
    " \n",
    "try:\n",
    "    embeddings = init_embedding_model(ai_core_embedding_model_name)\n",
    "    print(\"Embedding model initialized successfully.\")\n",
    "except Exception as e:\n",
    "    print(\"Embedding model not initialized.\")\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Successfully created SAP HANA VectorStore interface: <dbapi.Connection Connection object : bfff8255-c34a-41cb-a822-bf9b5f56fb16.hna0.prod-eu20.hanacloud.ondemand.com,443,DBADMIN,DB@hmx04,True>\n",
      "    and SAP HANA table: ACCOUNTING_ASSIGN_SUPPORT_TABLE_DBADMIN.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# function A4.1 - Create a LangChain VectorStore interface for the HANA database and specify the table (collection) \n",
    "# to use for accessing the vector embeddings\n",
    "# check table creation with sap-hana-database explorer: select * from ACCOUNTING_ASSIGN_SUPPORT_TABLE_DBADMIN\n",
    "\n",
    "from langchain_community.vectorstores.hanavector import HanaDB\n",
    "\n",
    "vector_table_name = str(os.getenv('hdb_table_name'))\n",
    "\n",
    "hana_database = HanaDB(\n",
    "    embedding = embeddings, \n",
    "    connection = hana_connection, \n",
    "    table_name = vector_table_name\n",
    ")\n",
    "\n",
    "try:\n",
    "    print(f\"\"\"\n",
    "    Successfully created SAP HANA VectorStore interface: {hana_database.connection}\n",
    "    and SAP HANA table: {vector_table_name}.\n",
    "    \"\"\")\n",
    "except Exception as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'id'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m hana_database.delete(\u001b[38;5;28mfilter\u001b[39m={})\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# add the loaded document text_chunks\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43mhana_database\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_chunks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSuccessfully added \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text_chunks)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m document chunks to the database.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mtable-name: \u001b[39m\u001b[33m\"\u001b[39m,hana_database.table_name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.asdf-inst/installs/python/3.13.1/lib/python3.13/site-packages/langchain_core/vectorstores/base.py:278\u001b[39m, in \u001b[36mVectorStore.add_documents\u001b[39m\u001b[34m(self, documents, **kwargs)\u001b[39m\n\u001b[32m    276\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).add_texts != VectorStore.add_texts:\n\u001b[32m    277\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mids\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[32m--> \u001b[39m\u001b[32m278\u001b[39m         ids = [\u001b[43mdoc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mid\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[32m    280\u001b[39m         \u001b[38;5;66;03m# If there's at least one valid ID, we'll assume that IDs\u001b[39;00m\n\u001b[32m    281\u001b[39m         \u001b[38;5;66;03m# should be used.\u001b[39;00m\n\u001b[32m    282\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ids):\n",
      "\u001b[31mAttributeError\u001b[39m: 'str' object has no attribute 'id'"
     ]
    }
   ],
   "source": [
    "# function A4.2 - delete existing documents from the table and load embeddings to SAP HANA-Table\n",
    "\n",
    "# Delete already existing documents from the SAP HANA table\n",
    "hana_database.delete(filter={})\n",
    "\n",
    "# add the loaded document text_chunks\n",
    "hana_database.add_documents(text_chunks)\n",
    "\n",
    "print(f\"Successfully added {len(text_chunks)} document chunks to the database.\")\n",
    "print(\"table-name: \",hana_database.table_name)\n",
    "print(\"Successfully connected to the HANA Cloud database.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check function A4.2 - query to the table to verify embeddings\n",
    "\n",
    "cursor = hana_connection.cursor()\n",
    "sql = f'SELECT VEC_TEXT, TO_NVARCHAR(VEC_VECTOR) FROM \"{hana_database.table_name}\"'\n",
    "\n",
    "cursor.execute(sql)\n",
    "vectors = cursor.fetchall()\n",
    "\n",
    "print(vectors[:1])\n",
    "\n",
    "# for vector in vectors:\n",
    "#     print(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrap-Book Development Modul A\n",
    "\n",
    "* function A2: load the pdf-file and split\n",
    "* function A4.3: Check retrieval for the embeddings in the SAP-Hana-Database\n",
    "* check function A4.3: check retrieval from SAP HANA DB with prompt using chain_type=\"map_reduce\"\n",
    "* check function A4.3: check retrieval from SAP HANA DB with prompt using chain_type=\"stuff\"\n",
    "* check function A4.3: check retrieval from SAP HANA DB with prompt using chain_type=\"map_reduce\" and optimized prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function A2: load the pdf-file containing the accounting assignment guide data from a folder and splitt the data into text_chunks\n",
    "# the funktion uses the os modules to read the file and split the data with langChain-modules\n",
    "\n",
    "\n",
    "import os\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "def load_and_split_pdf(file_path):\n",
    "    # Check if the file exists\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n",
    "    \n",
    "    # Load the PDF file\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    documents = loader.load()\n",
    "    \n",
    "    # Split the documents into text_chunks\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=3000, chunk_overlap=200)\n",
    "    text_chunks = text_splitter.split_documents(documents)\n",
    "    \n",
    "    return text_chunks\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Test the function with a sample PDF file path\n",
    "    file_path = \"data/sample_accounting_guide.pdf\"\n",
    "    try:\n",
    "        text_chunks = load_and_split_pdf(file_path)\n",
    "        print(f\"Number of text_chunks created: {len(text_chunks)}\")\n",
    "        print(f\"First text_chunks: {text_chunks[0]}\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Wir splitten zuerst nach Abs√§tzen und dann rekursiv, falls ein Absatz zu gro√ü ist\n",
    "paragraphs = text.split(\"\\n\\n\")  # Annahme: Abs√§tze sind durch zwei Zeilen getrennt\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300,  # Gesamtgr√∂√üe eines Chunks\n",
    "    chunk_overlap=50,  # √úberlappung in Tokens/Zeichen\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \"],  # Reihenfolge der Split-Priorit√§t\n",
    ")\n",
    "\n",
    "chunks = []\n",
    "for para in paragraphs:\n",
    "    if para.strip():  # Leere Abs√§tze √ºberspringen\n",
    "        sub_chunks = text_splitter.split_text(para)\n",
    "        chunks.extend(sub_chunks)\n",
    "\n",
    "# Ausgabe\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"--- Chunk {i+1} ---\")\n",
    "    print(chunk)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check retrievaleval for the embeddings in the SAP-Hana-Database\n",
    "\n",
    "This code snippet integrates various components from the langchain library to create a retrieval-based question-answering (QA) system. Here's a breakdown of the key parts and their functionality:\n",
    "\n",
    "Retriever Initialization: The db.as_retriever function is used to initialize a retriever object with specific search arguments ('k':20), which likely defines the number of search results to consider.\n",
    "\n",
    "Prompt Template : The PromptTemplate was defined in the previous step that instructs how to use the context to answer a question. It emphasizes not to fabricate answers if the information is unavailable. The template also outlines the structure for the expected JSON output with various product and supplier details.The prompt template is crucial for guiding the model's responses, ensuring that the answers are relevant and accurate based on the retrieved information.\n",
    "\n",
    "Once the retriever and prompt template are set up, the next step involves using the LLM (Language Model) to generate answers based on the retrieved documents. This process typically includes passing the retrieved context to the LLM along with the user's query, allowing it to formulate a coherent and contextually appropriate response.After that, the LLM processes the prompt and generates a response, which can then be formatted and returned to the user, ensuring a seamless interaction with the QA system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function A4.3: Check retrievaleval for the embeddings in the SAP-Hana-Database\n",
    "\n",
    "import os\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "llm_chain = prompt | llm\n",
    "\n",
    "question = \"What NFL team won the Super Bowl in the year Justin Bieber was born?\"\n",
    "\n",
    "print(llm_chain.invoke({'question': question}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check function 4.2 : check retrieval from SAP HANA DB with prompt using chain_type=\"map_reduce\"\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "map_template = \"\"\"\n",
    "Analysiere den folgenden Kontext und extrahiere relevante Informationen zur Kontierung:\n",
    "\n",
    "{context}\n",
    "\n",
    "Frage: {question}\n",
    "\n",
    "Gib die relevanten Informationen in einem kurzen Zwischenergebnis zur√ºck.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "reduce_template = \"\"\"\n",
    "Basierend auf den folgenden Zwischenergebnissen, erstelle eine finale Antwort:\n",
    "\n",
    "{summerization}\n",
    "\n",
    "Frage: {question}\n",
    "\n",
    "Formatiere die Ergebnisse in einer Liste von JSON-Elementen mit den folgenden Schl√ºsseln:\n",
    "\"Gesch√§ftsfall\"\n",
    "\"Konto Soll\"\n",
    "\"Konto Haben\"\n",
    "\n",
    "Die Ergebnisse d√ºrfen keine json markdown codeblock syntax enthalten.\n",
    "Wenn keine relevanten Informationen gefunden wurden, gib an dass Du keine Antwort kennst.\n",
    "\"\"\"\n",
    "\n",
    "MAP_PROMPT = PromptTemplate(template=map_template, input_variables=[\"context\", \"question\"])\n",
    "REDUCE_PROMPT = PromptTemplate(template=reduce_template, input_variables=[\"summerization\", \"question\"])\n",
    "\n",
    "chain_type_kwargs = {\n",
    "    \"question_prompt\": MAP_PROMPT,\n",
    "    \"reduce_prompt\": REDUCE_PROMPT\n",
    "}\n",
    "\n",
    "question = \"Finde Kontierung f√ºr die Umbuchung von langfristigen Forderungen\"\n",
    "\n",
    "retriever = hana_database.as_retriever(search_kwargs={'k':20})\n",
    "\n",
    "question_answer_retriever = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    chain_type='map_reduce',\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "answer = question_answer_retriever.run(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check function 4.2 : check retrieval from SAP HANA DB with prompt using chain_type=\"stuff\"\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "prompt_template = \"\"\"Verwende den folgenden Kontext, um die Frage am Ende zu beantworten. Wenn du die Antwort nicht kennst,\n",
    "    sage einfach, dass du es nicht wei√üt. Versuche nicht, eine Antwort zu erfinden. Formatiere die Ergebnisse als Liste von JSON-Elementen mit den folgenden Schl√ºsseln:\n",
    "\n",
    "    \"Gesch√§ftsfall\",\n",
    "    \"Konto Soll\",\n",
    "    \"Konto Haben\"\n",
    "\n",
    "    F√ºge keine JSON-Markdown-Codeblock-Syntax in die Ergebnisse ein.\n",
    "\n",
    "    {context}\n",
    "\n",
    "    Frage: {question}\n",
    "\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(template=prompt_template, \n",
    "                       input_variables=[\"context\", \"question\"]\n",
    "                      )\n",
    "    \n",
    "chain_type_kwargs = {\"prompt\": PROMPT}\n",
    "\n",
    "question = \"Finde Kontierung f√ºr die Buchung von R√ºckstellungen\"\n",
    "\n",
    "count_retrieved_documents = 10\n",
    "\n",
    "retriever = hana_database.as_retriever(search_kwargs={'k': count_retrieved_documents})\n",
    "# hint: k smaller than 20 -> to much tokens\n",
    "\n",
    "question_answer_retriever = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    chain_type=\"stuff\",\n",
    "    chain_type_kwargs=chain_type_kwargs,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "answer = question_answer_retriever.run(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check function 4.2 : check retrieval from SAP HANA DB with prompt using chain_type=\"map_reduce\" (prompt-finetuning)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Prompt f√ºr die Map-Phase\n",
    "map_prompt_template = \"\"\"Analysiere den folgenden Kontext und extrahiere relevante Kontierungsinformationen f√ºr den Gesch√§ftsfall.\n",
    "Wenn keine relevanten Informationen im Kontext gefunden werden k√∂nnen, gib die Antwort zur√ºck: \"Ich habe keine Kontierungsinformationen zum Gesch√§ftsfall gefunden\".\n",
    "\n",
    "{context}\n",
    "\n",
    "Frage: {question}\n",
    "\"\"\"\n",
    "\n",
    "# Prompt f√ºr die Combine/Reduce-Phase\n",
    "combine_prompt_template = \"\"\"\n",
    "- Fasse die folgenden Kontierungsinformationen zusammen und entferne Duplikate.\n",
    "- Gib nur die relevantesten und eindeutigsten Kontierungen zur√ºck. \n",
    "- Wenn keine relevanten Informationen im Kontext gefunden werden k√∂nnen, gib die Antwort zur√ºck: \"Ich habe keine Kontierungsinformationen zum Gesch√§ftsfall gefunden\".\n",
    "- Wenn relevante Informationen gefunden wurden gib diese Informationen im folgenden Struktur aus:\n",
    "    ## Gesch√§ftsfall: <Bezeichnung des Gesch√§ftsfalls>\n",
    "    ## Kontierung\n",
    "    Konto-Soll: <Konto-Soll> - <Bezeichnung Konto-Soll> AN Konto-Haben: <Konto-Haben> - <Bezeichnung Konto-Haben>\n",
    "\n",
    "Beispiel:\n",
    "\n",
    "-----------------------------\n",
    "## Gesch√§ftsfall: Bildung von R√ºckstellungen\n",
    "    ## Kontierung\n",
    "    Konto-Soll: L160501 - LC Instandhaltungskosten (Geb√§ude) AN Konto-Haben: L3909101 - LC Sonstige R√ºckstellungen\n",
    "-----------------------------\n",
    "\n",
    "{summaries}\n",
    "\n",
    "Frage: {question}\n",
    "\"\"\"\n",
    "\n",
    "# Korrekte Struktur f√ºr chain_type_kwargs\n",
    "chain_type_kwargs = {\n",
    "    \"question_prompt\": PromptTemplate(\n",
    "        template=map_prompt_template,\n",
    "        input_variables=[\"context\", \"question\"]\n",
    "    ),\n",
    "    \"combine_prompt\": PromptTemplate(\n",
    "        template=combine_prompt_template,\n",
    "        input_variables=[\"summaries\", \"question\"]\n",
    "    )\n",
    "}\n",
    "\n",
    "question = \"Finde Kontierung f√ºr die Buchung von Bildung von R√ºckstellungen\"\n",
    "\n",
    "retriever = hana_database.as_retriever(search_kwargs={'k': 20})\n",
    "\n",
    "question_answer_retriever = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"map_reduce\",\n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs=chain_type_kwargs,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "answer = question_answer_retriever.run(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example setup SAP HANA Vector Database\n",
    "\n",
    "Wichtige Aspekte des Codes:\n",
    "Tabellenstruktur:\n",
    "ID: Eindeutiger Identifier f√ºr jeden Eintrag\n",
    "DOCUMENT: Der eigentliche Dokumententext\n",
    "METADATA: JSON-formatierte Metadaten\n",
    "EMBEDDING: Der Embedding-Vektor als BLOB\n",
    "VECTOR_DIMENSION: Dimension des Embedding-Vektors (1536 f√ºr ada-002)\n",
    "Vector-Index:\n",
    "Verwendet HANA's native Vektorindexierung\n",
    "Cosine-Similarity mit Threshold 0.75\n",
    "Optimiert f√ºr 1536-dimensionale Vektoren\n",
    "Testdaten:\n",
    "Beispieldaten f√ºr Kontierungsregeln\n",
    "Dummy-Embeddings f√ºr Testzwecke\n",
    "Strukturierte Metadaten im JSON-Format\n",
    "Verifikation:\n",
    "√úberpr√ºft Tabellenstruktur\n",
    "Zeigt vorhandene Indizes\n",
    "Gibt Anzahl der Datens√§tze aus\n",
    "Um den Code zu verwenden:\n",
    "1. Stellen Sie sicher, dass die Umgebungsvariablen gesetzt sind\n",
    "2. F√ºhren Sie das Setup aus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HDB_HOST'] = 'your_host'\n",
    "os.environ['HDB_PORT'] = 'your_port'\n",
    "os.environ['HDB_USER'] = 'your_user'\n",
    "os.environ['HDB_PASSWORD'] = 'your_password'\n",
    "os.environ['HDB_SCHEMA'] = 'your_schema'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabelle erstellen und Testdaten einf√ºgen\n",
    "create_vector_table(connection_params)\n",
    "insert_test_data(connection_params)\n",
    "verify_table_setup(connection_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exmaple setup code SAP HANA Vector DB (cursor)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "from hdbcli import dbapi\n",
    "import numpy as np\n",
    "\n",
    "def create_vector_table(connection_params, table_name=\"VECTOR_TABLE\"):\n",
    "    \"\"\"\n",
    "    Erstellt eine Vektortabelle in SAP HANA f√ºr die Speicherung von Dokumenten und deren Embeddings.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Verbindung zur HANA-Datenbank herstellen\n",
    "        conn = dbapi.connect(\n",
    "            address=connection_params['address'],\n",
    "            port=connection_params['port'],\n",
    "            user=connection_params['user'],\n",
    "            password=connection_params['password']\n",
    "        )\n",
    "        \n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Zum angegebenen Schema wechseln\n",
    "        if 'schema' in connection_params:\n",
    "            cursor.execute(f\"SET SCHEMA {connection_params['schema']}\")\n",
    "\n",
    "        # Tabelle erstellen, falls sie nicht existiert\n",
    "        create_table_sql = f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "            ID NVARCHAR(100) PRIMARY KEY,\n",
    "            DOCUMENT NCLOB,\n",
    "            METADATA NCLOB,\n",
    "            EMBEDDING BLOB,\n",
    "            VECTOR_DIMENSION INTEGER\n",
    "        )\n",
    "        \"\"\"\n",
    "        cursor.execute(create_table_sql)\n",
    "        \n",
    "        # Vector-Index erstellen\n",
    "        create_index_sql = f\"\"\"\n",
    "        CREATE INDEX IF NOT EXISTS IDX_{table_name}_VECTOR \n",
    "        ON {table_name}(EMBEDDING) \n",
    "        VECTOR DIMENSION 1536 \n",
    "        DOUBLE COSINE THRESHOLD 0.75\n",
    "        \"\"\"\n",
    "        cursor.execute(create_index_sql)\n",
    "        \n",
    "        print(f\"Tabelle {table_name} und Vector-Index wurden erfolgreich erstellt.\")\n",
    "        \n",
    "        # √úberpr√ºfen, ob die Tabelle leer ist\n",
    "        cursor.execute(f\"SELECT COUNT(*) FROM {table_name}\")\n",
    "        count = cursor.fetchone()[0]\n",
    "        print(f\"Anzahl der Eintr√§ge in der Tabelle: {count}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Fehler beim Erstellen der Tabelle: {str(e)}\")\n",
    "        raise\n",
    "    finally:\n",
    "        if 'cursor' in locals():\n",
    "            cursor.close()\n",
    "        if 'conn' in locals():\n",
    "            conn.close()\n",
    "\n",
    "def insert_test_data(connection_params, table_name=\"VECTOR_TABLE\"):\n",
    "    \"\"\"\n",
    "    F√ºgt Testdaten in die Vektortabelle ein.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = dbapi.connect(\n",
    "            address=connection_params['address'],\n",
    "            port=connection_params['port'],\n",
    "            user=connection_params['user'],\n",
    "            password=connection_params['password']\n",
    "        )\n",
    "        \n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        if 'schema' in connection_params:\n",
    "            cursor.execute(f\"SET SCHEMA {connection_params['schema']}\")\n",
    "\n",
    "        # Beispiel-Kontierungsdaten\n",
    "        test_data = [\n",
    "            {\n",
    "                'id': 'doc1',\n",
    "                'document': 'Buchung von R√ºckstellungen f√ºr Garantieverpflichtungen',\n",
    "                'metadata': '{\"type\": \"accounting_rule\", \"category\": \"provisions\"}',\n",
    "                'embedding': np.random.rand(1536).astype(np.float64)  # Dummy-Embedding\n",
    "            },\n",
    "            {\n",
    "                'id': 'doc2',\n",
    "                'document': 'Anlagenzugang durch Kauf einer Maschine',\n",
    "                'metadata': '{\"type\": \"accounting_rule\", \"category\": \"fixed_assets\"}',\n",
    "                'embedding': np.random.rand(1536).astype(np.float64)\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        # Daten einf√ºgen\n",
    "        for data in test_data:\n",
    "            insert_sql = f\"\"\"\n",
    "            INSERT INTO {table_name} \n",
    "            (ID, DOCUMENT, METADATA, EMBEDDING, VECTOR_DIMENSION) \n",
    "            VALUES(?, ?, ?, ?, 1536)\n",
    "            \"\"\"\n",
    "            \n",
    "            cursor.execute(insert_sql, \n",
    "                         (data['id'], \n",
    "                          data['document'], \n",
    "                          data['metadata'], \n",
    "                          data['embedding'].tobytes()))\n",
    "\n",
    "        conn.commit()\n",
    "        print(f\"Testdaten wurden erfolgreich in {table_name} eingef√ºgt.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Fehler beim Einf√ºgen der Testdaten: {str(e)}\")\n",
    "        raise\n",
    "    finally:\n",
    "        if 'cursor' in locals():\n",
    "            cursor.close()\n",
    "        if 'conn' in locals():\n",
    "            conn.close()\n",
    "\n",
    "def verify_table_setup(connection_params, table_name=\"VECTOR_TABLE\"):\n",
    "    \"\"\"\n",
    "    √úberpr√ºft die Einrichtung der Vektortabelle.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = dbapi.connect(\n",
    "            address=connection_params['address'],\n",
    "            port=connection_params['port'],\n",
    "            user=connection_params['user'],\n",
    "            password=connection_params['password']\n",
    "        )\n",
    "        \n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        if 'schema' in connection_params:\n",
    "            cursor.execute(f\"SET SCHEMA {connection_params['schema']}\")\n",
    "\n",
    "        # Tabellenstruktur √ºberpr√ºfen\n",
    "        cursor.execute(f\"\"\"\n",
    "        SELECT COLUMN_NAME, DATA_TYPE_NAME, LENGTH, IS_NULLABLE \n",
    "        FROM TABLE_COLUMNS \n",
    "        WHERE TABLE_NAME = '{table_name.upper()}'\n",
    "        \"\"\")\n",
    "        \n",
    "        print(\"\\nTabellenstruktur:\")\n",
    "        for col in cursor.fetchall():\n",
    "            print(f\"Spalte: {col[0]}, Typ: {col[1]}, L√§nge: {col[2]}, Nullable: {col[3]}\")\n",
    "\n",
    "        # Index √ºberpr√ºfen\n",
    "        cursor.execute(f\"\"\"\n",
    "        SELECT INDEX_NAME, CONSTRAINT \n",
    "        FROM INDEXES \n",
    "        WHERE TABLE_NAME = '{table_name.upper()}'\n",
    "        \"\"\")\n",
    "        \n",
    "        print(\"\\nIndizes:\")\n",
    "        for idx in cursor.fetchall():\n",
    "            print(f\"Index: {idx[0]}, Constraint: {idx[1]}\")\n",
    "\n",
    "        # Datenbestand √ºberpr√ºfen\n",
    "        cursor.execute(f\"SELECT COUNT(*) FROM {table_name}\")\n",
    "        count = cursor.fetchone()[0]\n",
    "        print(f\"\\nAnzahl der Datens√§tze: {count}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler bei der √úberpr√ºfung: {str(e)}\")\n",
    "        raise\n",
    "    finally:\n",
    "        if 'cursor' in locals():\n",
    "            cursor.close()\n",
    "        if 'conn' in locals():\n",
    "            conn.close()\n",
    "\n",
    "# Verwendung:\n",
    "if __name__ == \"__main__\":\n",
    "    # Verbindungsparameter (sollten aus Umgebungsvariablen kommen)\n",
    "    connection_params = {\n",
    "        'address': os.getenv('HDB_HOST'),\n",
    "        'port': os.getenv('HDB_PORT'),\n",
    "        'user': os.getenv('HDB_USER'),\n",
    "        'password': os.getenv('HDB_PASSWORD'),\n",
    "        'schema': os.getenv('HDB_SCHEMA')\n",
    "    }\n",
    "\n",
    "    table_name = \"VECTOR_TABLE\"\n",
    "\n",
    "    try:\n",
    "        # Tabelle erstellen\n",
    "        create_vector_table(connection_params, table_name)\n",
    "        \n",
    "        # Optional: Testdaten einf√ºgen\n",
    "        insert_test_data(connection_params, table_name)\n",
    "        \n",
    "        # Setup √ºberpr√ºfen\n",
    "        verify_table_setup(connection_params, table_name)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Fehler beim Setup: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example insert documentws in SAP HANA\n",
    "\n",
    "Gesch√§ftsfall          | Konto_Soll | Konto_Haben | Beschreibung\n",
    "---------------------- | ----------- | ----------- | ------------\n",
    "R√ºckstellung_Garantie  | 6815       | 3050        | Buchung von R√ºckstellungen f√ºr Garantieverpflichtungen...\n",
    "Anlagenzugang_Kauf    | 0410       | 2800        | Anschaffung einer neuen Produktionsmaschine...\n",
    "\n",
    "Die wichtigsten Features:\n",
    "Excel-Verarbeitung:\n",
    "Liest Kontierungsregeln aus Excel\n",
    "Unterst√ºtzt strukturierte Daten mit Gesch√§ftsfall, Konten und Beschreibung\n",
    "Embedding-Erstellung:\n",
    "Verwendet OpenAI's ada-002 Modell\n",
    "Verarbeitet Dokumente in Batches\n",
    "Fortschrittsanzeige mit tqdm\n",
    "Datenbankintegration:\n",
    "Sichere Verbindungshandhabung\n",
    "Effiziente Batch-Verarbeitung\n",
    "Fehlerbehandlung und Logging\n",
    "Metadaten-Handling:\n",
    "Strukturierte Speicherung der Kontierungsinformationen\n",
    "JSON-Format f√ºr flexible Erweiterbarkeit\n",
    "Verwendung:\n",
    "Excel-Datei vorbereiten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beispiel f√ºr manuelle Datenerstellung\n",
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    'Gesch√§ftsfall': ['R√ºckstellung_Garantie', 'Anlagenzugang_Kauf'],\n",
    "    'Konto_Soll': ['6815', '0410'],\n",
    "    'Konto_Haben': ['3050', '2800'],\n",
    "    'Beschreibung': [\n",
    "        'Buchung von R√ºckstellungen f√ºr Garantieverpflichtungen...',\n",
    "        'Anschaffung einer neuen Produktionsmaschine...'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_excel('kontierungsregeln.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processor initialisieren und ausf√ºhren\n",
    "processor = DocumentProcessor(connection_params)\n",
    "processor.process_and_store_documents('kontierungsregeln.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "from typing import List, Dict\n",
    "import json\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import uuid\n",
    "\n",
    "class DocumentProcessor:\n",
    "    def __init__(self, connection_params: Dict, table_name: str = \"VECTOR_TABLE\"):\n",
    "        \"\"\"\n",
    "        Initialisiert den Document Processor.\n",
    "        \n",
    "        Args:\n",
    "            connection_params: Dictionary mit HANA-Verbindungsparametern\n",
    "            table_name: Name der Vektortabelle\n",
    "        \"\"\"\n",
    "        self.connection_params = connection_params\n",
    "        self.table_name = table_name\n",
    "        self.embeddings = OpenAIEmbeddings(\n",
    "            model=\"text-embedding-ada-002\"\n",
    "        )\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=200,\n",
    "            length_function=len\n",
    "        )\n",
    "        \n",
    "    def process_excel_data(self, excel_file: str, sheet_name: str = \"Sheet1\") -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Verarbeitet Excel-Daten mit Kontierungsregeln.\n",
    "        \n",
    "        Args:\n",
    "            excel_file: Pfad zur Excel-Datei\n",
    "            sheet_name: Name des Excel-Sheets\n",
    "        \n",
    "        Returns:\n",
    "            Liste von Dokumenten mit Metadaten\n",
    "        \"\"\"\n",
    "        try:\n",
    "            df = pd.read_excel(excel_file, sheet_name=sheet_name)\n",
    "            documents = []\n",
    "            \n",
    "            for _, row in df.iterrows():\n",
    "                # Annahme: Excel-Spalten sind \"Gesch√§ftsfall\", \"Konto_Soll\", \"Konto_Haben\", \"Beschreibung\"\n",
    "                doc = {\n",
    "                    'id': str(uuid.uuid4()),\n",
    "                    'document': row['Beschreibung'],\n",
    "                    'metadata': json.dumps({\n",
    "                        'Gesch√§ftsfall': row['Gesch√§ftsfall'],\n",
    "                        'Konto_Soll': str(row['Konto_Soll']),\n",
    "                        'Konto_Haben': str(row['Konto_Haben'])\n",
    "                    }, ensure_ascii=False)\n",
    "                }\n",
    "                documents.append(doc)\n",
    "            \n",
    "            return documents\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Fehler beim Verarbeiten der Excel-Datei: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def create_embeddings(self, documents: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Erstellt Embeddings f√ºr die Dokumente.\n",
    "        \n",
    "        Args:\n",
    "            documents: Liste von Dokumenten\n",
    "        \n",
    "        Returns:\n",
    "            Liste von Dokumenten mit Embeddings\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(\"Erstelle Embeddings...\")\n",
    "            for doc in tqdm(documents):\n",
    "                # Embedding f√ºr den Dokumententext erstellen\n",
    "                embedding = self.embeddings.embed_query(doc['document'])\n",
    "                doc['embedding'] = embedding\n",
    "            return documents\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Fehler beim Erstellen der Embeddings: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def insert_documents(self, documents: List[Dict]):\n",
    "        \"\"\"\n",
    "        F√ºgt Dokumente in die HANA-Datenbank ein.\n",
    "        \n",
    "        Args:\n",
    "            documents: Liste von Dokumenten mit Embeddings\n",
    "        \"\"\"\n",
    "        from hdbcli import dbapi\n",
    "        import numpy as np\n",
    "        \n",
    "        try:\n",
    "            conn = dbapi.connect(\n",
    "                address=self.connection_params['address'],\n",
    "                port=self.connection_params['port'],\n",
    "                user=self.connection_params['user'],\n",
    "                password=self.connection_params['password']\n",
    "            )\n",
    "            \n",
    "            cursor = conn.cursor()\n",
    "            \n",
    "            if 'schema' in self.connection_params:\n",
    "                cursor.execute(f\"SET SCHEMA {self.connection_params['schema']}\")\n",
    "\n",
    "            print(\"F√ºge Dokumente in die Datenbank ein...\")\n",
    "            for doc in tqdm(documents):\n",
    "                insert_sql = f\"\"\"\n",
    "                INSERT INTO {self.table_name} \n",
    "                (ID, DOCUMENT, METADATA, EMBEDDING, VECTOR_DIMENSION) \n",
    "                VALUES(?, ?, ?, ?, 1536)\n",
    "                \"\"\"\n",
    "                \n",
    "                # Embedding in bytes konvertieren\n",
    "                embedding_bytes = np.array(doc['embedding']).astype(np.float64).tobytes()\n",
    "                \n",
    "                cursor.execute(insert_sql, \n",
    "                             (doc['id'], \n",
    "                              doc['document'], \n",
    "                              doc['metadata'], \n",
    "                              embedding_bytes))\n",
    "\n",
    "            conn.commit()\n",
    "            print(f\"Alle Dokumente wurden erfolgreich eingef√ºgt.\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Fehler beim Einf√ºgen der Dokumente: {str(e)}\")\n",
    "            raise\n",
    "        finally:\n",
    "            if 'cursor' in locals():\n",
    "                cursor.close()\n",
    "            if 'conn' in locals():\n",
    "                conn.close()\n",
    "\n",
    "    def process_and_store_documents(self, excel_file: str, sheet_name: str = \"Sheet1\"):\n",
    "        \"\"\"\n",
    "        Hauptmethode zum Verarbeiten und Speichern von Dokumenten.\n",
    "        \n",
    "        Args:\n",
    "            excel_file: Pfad zur Excel-Datei\n",
    "            sheet_name: Name des Excel-Sheets\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Dokumente aus Excel laden\n",
    "            documents = self.process_excel_data(excel_file, sheet_name)\n",
    "            print(f\"Anzahl geladener Dokumente: {len(documents)}\")\n",
    "            \n",
    "            # Embeddings erstellen\n",
    "            documents_with_embeddings = self.create_embeddings(documents)\n",
    "            \n",
    "            # Dokumente in die Datenbank einf√ºgen\n",
    "            self.insert_documents(documents_with_embeddings)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Fehler bei der Verarbeitung: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "# Beispiel f√ºr die Verwendung:\n",
    "if __name__ == \"__main__\":\n",
    "    # Verbindungsparameter\n",
    "    connection_params = {\n",
    "        'address': os.getenv('HDB_HOST'),\n",
    "        'port': os.getenv('HDB_PORT'),\n",
    "        'user': os.getenv('HDB_USER'),\n",
    "        'password': os.getenv('HDB_PASSWORD'),\n",
    "        'schema': os.getenv('HDB_SCHEMA')\n",
    "    }\n",
    "\n",
    "    # Excel-Datei mit Kontierungsregeln\n",
    "    excel_file = \"kontierungsregeln.xlsx\"\n",
    "\n",
    "    # Document Processor initialisieren und ausf√ºhren\n",
    "    processor = DocumentProcessor(connection_params)\n",
    "    \n",
    "    try:\n",
    "        processor.process_and_store_documents(excel_file)\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler beim Gesamtprozess: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
