{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook for use case digital posting assistant - stage1\n",
    "### Module A vectorize accounting assignment guide\n",
    "#### Ojectives\n",
    "- In this module we will develop the load and the vectorization of the text-file for the accounting assignment guide\n",
    "- The vectorized accounting assignemnt guide will finaly stored in a SAP HANA vector database\n",
    "\n",
    "#### Processing steps from concept\n",
    "A0 - preparation\n",
    "\n",
    "A2 - load and splitt: load the pdf-file containing the accounting assignment guide data from a folder and splitt the data into text_chunks\n",
    "\n",
    "A3 - vectorize and embedd: vectorize the splitted data with embedding function. Use an embedding function to convert the text chunks into vector representations\n",
    "\n",
    "A4 - store: create/clear a sap hana database - table and store the vector in this table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A0 - Setup and configuration Modul A\n",
    "\n",
    "The following setup-steps where processed:\n",
    "\n",
    "* A0.0 Start SAP instances\n",
    "* A0.1 install py-packages\n",
    "* A0.2 load env-variables from config.json-file\n",
    "* A0.3 Setup and test connection to HANA DB\n",
    "* A0.4 Setup LLM-Connection to SAP AI-HUB\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A0.0 Start SAP Instances\n",
    "\n",
    "* Start BTP Cockpit\n",
    "* Start SAP Build Dev Space\n",
    "* Start HANA DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /home/user/.asdf-inst/installs/python/3.13.1/lib/python3.13/site-packages (25.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "py-packages installed!\n"
     ]
    }
   ],
   "source": [
    "# A0.1 install py-packages\n",
    "# RESET KERNEL AFTER INSTALLATION\n",
    "\n",
    "%pip install --upgrade pip\n",
    "\n",
    "%pip install --quiet hdbcli --break-system-packages\n",
    "%pip install --quiet generative-ai-hub-sdk[all] --break-system-packages\n",
    "%pip install --quiet folium --break-system-packages\n",
    "%pip install --quiet ipywidgets --break-system-packages\n",
    "%pip install --quiet pypdf\n",
    "%pip install --quiet -U ipykernel\n",
    "%pip install --quiet hana-ml\n",
    "%pip install --quiet hdbcli\n",
    "%pip install --quiet sqlalchemy-hana\n",
    "%pip install --quiet nltk\n",
    "%pip install --quiet langchain langchain_experimental langchain_openai\n",
    "print(\"py-packages installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded environment variables: {'AICORE_AUTH_URL': 'https://sap-ai.authentication.eu10.hana.ondemand.com', 'AICORE_CLIENT_ID': 'sb-6ebfacf0-d811-447e-9262-b18974ccd67e!b364403|aicore!b540', 'AICORE_CLIENT_SECRET': '17e8de41-d715-4cfe-b086-9c59302640df$_9OJ4fWV28VeLV5uSMyxA_DfSXHna3dEE9-0KocfNeE=', 'AICORE_RESOURCE_GROUP': 'default', 'AICORE_BASE_URL': 'https://api.ai.prod.eu-central-1.aws.ml.hana.ondemand.com/v2', 'AICORE_DEPLOYMENT_ID': 'dc4204c43f309ca1', 'AICORE_DEPLOYMENT_URL': 'https://api.ai.prod.eu-central-1.aws.ml.hana.ondemand.com/v2/inference/deployments/dc4204c43f309ca1', 'AICORE_DEPLOYMENT_MODEL': 'gpt-4', 'AICORE_DEPLOYMENT_MODEL_VERSION': 'latest', 'AICORE_DEPLOYMENT_MODEL_EMBEDDING': 'text-embedding-ada-002', 'hdb_host_address': 'bfff8255-c34a-41cb-a822-bf9b5f56fb16.hna0.prod-eu20.hanacloud.ondemand.com', 'hdb_user': 'DBADMIN', 'hdb_password': 'DB@hmx04', 'hdb_port': '443', 'hdb_table_name': 'ACCOUNTING_ASSIGN_SUPPORT_TABLE_DBADMIN'}\n"
     ]
    }
   ],
   "source": [
    "# A0.2 load env-variables from config.json-file\n",
    "# This script loads environment variables from a JSON configuration file\n",
    "# and sets them in the current environment. It raises an error if the file does not exist\n",
    "# or if the JSON file is malformed.\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "def load_env_variables(config_file):\n",
    "    \"\"\"\n",
    "    Load environment variables from a JSON configuration file.\n",
    "\n",
    "    Args:\n",
    "        config_file (str): Path to the JSON configuration file.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the environment variables.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(config_file):\n",
    "        raise FileNotFoundError(f\"The configuration file {config_file} does not exist.\")\n",
    "    \n",
    "    try:\n",
    "        with open(config_file, 'r') as file:\n",
    "            env_variables = json.load(file)\n",
    "    except json.JSONDecodeError as e:\n",
    "        raise ValueError(f\"Error decoding JSON from the configuration file {config_file}: {e}\")\n",
    "    \n",
    "    for key, value in env_variables.items():\n",
    "        # Convert non-string values to strings before setting them in os.environ\n",
    "        if isinstance(value, dict):\n",
    "            value = json.dumps(value)  # Convert dictionaries to JSON strings\n",
    "        os.environ[key] = str(value)\n",
    "    \n",
    "    return env_variables\n",
    "\n",
    "# Example usage\n",
    "config_file = \"/home/user/.aicore/config.json\"\n",
    "try:\n",
    "    env_variables = load_env_variables(config_file)\n",
    "    print(f\"Loaded environment variables: {env_variables}\")\n",
    "except (FileNotFoundError, ValueError) as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A0.2 Test connection with env-Variables to SAP AI core\n",
    "\n",
    "from gen_ai_hub.proxy.native.openai import embeddings\n",
    "\n",
    "response = embeddings.create(\n",
    "    input=\"SAP Generative AI Hub is awesome!\",\n",
    "    model_name=\"text-embedding-ada-002\"\n",
    "    \n",
    ")\n",
    "print(response.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A0.3 Setup and test connection to HANA DB\n",
    "\n",
    "import os\n",
    "# from hana_ml import ConnectionContext\n",
    "from hdbcli import dbapi\n",
    "\n",
    "# Fetch environment variables\n",
    "hdb_host_address = os.getenv(\"hdb_host_address\")\n",
    "hdb_user = os.getenv(\"hdb_user\")\n",
    "hdb_password = os.getenv(\"hdb_password\")\n",
    "hdb_port = os.getenv(\"hdb_port\")\n",
    "\n",
    "# Debugging: Print non-sensitive environment variables\n",
    "print(f\"hdb_host_address: {hdb_host_address}\")\n",
    "print(f\"hdb_user: {hdb_user}\")\n",
    "print(f\"hdb_port: {hdb_port}\")\n",
    "\n",
    "# Ensure variables are defined\n",
    "if not all([hdb_host_address, hdb_user, hdb_password, hdb_port]):\n",
    "    raise ValueError(\"One or more HANA DB connection parameters are missing.\")\n",
    "\n",
    "# Convert port to integer\n",
    "hdb_port = int(hdb_port)\n",
    "\n",
    "# Create a connection to the HANA database\n",
    "# hana_connection = ConnectionContext(\n",
    "#     address=hdb_host_address,\n",
    "#     port=hdb_port,\n",
    "#     user=hdb_user,\n",
    "#     password=hdb_password,\n",
    "#     encrypt=True\n",
    "# )\n",
    "\n",
    "# Test the connection\n",
    "# print(\"HANA DB Version:\", hana_connection.hana_version())\n",
    "# print(\"Current Schema:\", hana_connection.get_current_schema())\n",
    "\n",
    "hana_connection = dbapi.connect(\n",
    "    address=hdb_host_address,\n",
    "    port=hdb_port,\n",
    "    user=hdb_user,\n",
    "    password=hdb_password,\n",
    "    #encrypt=True\n",
    "    autocommit=True,\n",
    "    sslValidateCertificate=False,\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A0.4 Setup LLM-Connection to SAP AI-HUB\n",
    "\n",
    "import os\n",
    "import dotenv\n",
    "from gen_ai_hub.proxy.langchain.openai import ChatOpenAI\n",
    "from gen_ai_hub.proxy.langchain.openai import OpenAI\n",
    "\n",
    "# Lade aicore_model_name aus der Umgebungskonfiguration\n",
    "aicore_model_name = str(os.getenv(\"AICORE_DEPLOYMENT_MODEL\"))\n",
    "\n",
    "# √úberpr√ºfe, ob die Variable definiert ist\n",
    "if not aicore_model_name:\n",
    "    raise ValueError(f\"\"\"Parameter LLM-Model-Name {aicore_model_name} fehlt in der Umgebungskonfiguration.\"\"\")\n",
    "\n",
    "llm = ChatOpenAI(proxy_model_name=aicore_model_name)\n",
    "#llm = OpenAI(proxy_model_name=aicore_model_name)\n",
    "\n",
    "if not llm:\n",
    "    raise ValueError(f\"\"\"Parameter LLM-Model-Name {aicore_model_name} fehlt in der Umgebungskonfiguration.\"\"\")\n",
    "else:\n",
    "    print(f\"\"\"Parameter LLM-Model-Name: {aicore_model_name} wurde erfolgreich geladen.\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### processing functions Modul A\n",
    "\n",
    "- function A2: load the pdf-file with accounting assignment guide data and splitt the data into text_chunk\n",
    "\n",
    "- function A3: vectorize the splitted data with embedding function \n",
    "\n",
    "- function A4.1: create a LangChain VectorStore interface for the HANA database and specify the table\n",
    "\n",
    "- function A4.2: delete existing documents from the table and load embeddings to SAP HANA-Tabele\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text created: 70\n",
      "First page from Text: page_content='Kontierungshandbuch \n",
      " \n",
      " \n",
      "2 von 70 \n",
      "2.2.2.6.2 Aufzinsung langfristiger Finanzforderungen 26 \n",
      "2.2.2.7 Transaktionen in Fremdw√§hrungen - Forderungen 27 \n",
      "2.2.2.7.1 Forderungsabwertung durch W√§hrungsverluste 28 \n",
      "2.2.2.7.2 Ausweis unrealisierter W√§hrungsgewinne bei Forderungen aus L u. L 29 \n",
      "2.2.2.7.3 R√ºcknahme der unrealisierten W√§hrungsgewinne aus L u. L. 30 \n",
      "2.2.3 Wertpapiere des Umlaufverm√∂gens 30 \n",
      "2.2.3.1 Zuordnung 30 \n",
      "2.2.3.2 Bewertung von Wertpapieren des Umlaufverm√∂gens 31 \n",
      "2.2.3.3 Umgliederung von Wertpapieren 33 \n",
      "2.2.3.3.1 Umgliederung von ‚Äûtrading‚Äú (T) nach ‚Äûavailable for sale‚Äú (A) oder ‚Äûheld to \n",
      "maturity‚Äú (H) nach Ausweis eines unrealisierten Gewinns 33 \n",
      "2.2.3.3.2 Umgliederung von ‚Äûavailable for sale‚Äú (A) oder ‚Äûheld to maturity‚Äú (H) nach \n",
      "‚Äûtrading‚Äú (T) bei Ausweis eines unrealisierten Gewinns 34 \n",
      "2.2.3.3.3 Umgliederung von ‚Äûheld to maturity‚Äú (H) nach ‚Äûavailable for sale‚Äú (A) bei \n",
      "erfolgsneutralem Ausweis eines unrealisierten Gewinns 35 \n",
      "2.2.3.3.4 Umgliederung von ‚Äûavailable for sale‚Äú (A) nach ‚Äûheld to maturity‚Äú (H nach \n",
      "erfolgsneutraler Aufwertung 36 \n",
      "2.2.3.3.5 Erfolgswirksame Vereinnahmung der Neubewertungsr√ºcklage √ºber die \n",
      "Restlaufzeit des Wertpapiers 37 \n",
      "2.2.3.4 Wertminderungen am Beispiel marktf√§higer Wertpapiere 37 \n",
      "2.3 Aktive latente Steuern 39 \n",
      "2.3.1 Ansatz und Methodik latenter Steuern 39 \n",
      "2.3.2 Buchung der aktivischen Steuerdifferenz 40 \n",
      "2.3.3 Aufl√∂sung der aktivischen Steuerdifferenz 41 \n",
      "3 Bilanz: Passiva 42 \n",
      "3.1 R√ºckstellungen 42 \n",
      "3.1.1 Zuordnung 42 \n",
      "3.1.2 Bewertung 43 \n",
      "3.1.3 Aufwandsr√ºckstellungen nach HGB 44 \n",
      "3.1.3.1 Aufl√∂sung der HGB-R√ºckstellung bei Eintritt des R√ºckstellungsgrundes 45 \n",
      "3.1.3.2 Aufl√∂sung der HGB-R√ºckst. bei Nicht-Eintritt des R√ºckstellungsgrundes 46 \n",
      "3.1.4 Langfristige R√ºckstellungen: Aufzinsung / Abzinsung 46 \n",
      "3.1.4.1 Aufzinsung langfristiger R√ºckstellungen 48 \n",
      "3.1.4.2 Abzinsung langfristiger R√ºckstellungen 49 \n",
      "3.1.5 Pensionsr√ºckstellungen 50 \n",
      "3.1.5.1 Bildung von Pensionsr√ºckstellungen 51 \n",
      "3.1.5.2 Aufl√∂sung von Pensionsr√ºckstellungen 52 \n",
      "3.2 Verbindlichkeiten 53 \n",
      "3.2.1 Zuordnung von Verbindlichkeiten nach HGB und US GAAP 53 \n",
      "3.2.2 Bewertung von Verbindlichkeiten 55 \n",
      "3.2.3 Abzinsung von Verbindlichkeiten 55 \n",
      "3.2.4 Verbindlichkeiten in Fremdw√§hrungen 57' metadata={'producer': 'Microsoft¬Æ Word f√ºr Microsoft 365', 'creator': 'Microsoft¬Æ Word f√ºr Microsoft 365', 'creationdate': '2025-04-08T10:47:12+02:00', 'title': 'Kontierungshandbuch', 'author': 'Ein zufriedengestellter Microsoft Office-Anwender', 'moddate': '2025-04-08T10:47:12+02:00', 'source': 'data/sample_accounting_guide.pdf', 'total_pages': 70, 'page': 1, 'page_label': '2'}\n"
     ]
    }
   ],
   "source": [
    "# function A2: load the pdf-file containing the accounting assignment guide data from a folder and splitt the data into text_chunks\n",
    "# function A2.1 load data\n",
    "\n",
    "import os\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "def load_pdf(file_path):\n",
    "    # Check if the file exists\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n",
    "    \n",
    "    # Load the PDF file\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    documents = loader.load()\n",
    "    return (documents)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Test the function with a sample PDF file path\n",
    "    file_path = \"data/sample_accounting_guide.pdf\"\n",
    "    try:\n",
    "        documents = load_pdf(file_path)\n",
    "        print(f\"Length of text created: {len(documents)}\")\n",
    "        print(f\"First page from Text: {documents[1]}\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function A2: load the pdf-file containing the accounting assignment guide data from a folder and splitt the data into text_chunks\n",
    "# function A2.2.1 split document in chunks - version 1: Character Text-Splitter)\n",
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "chunk_size_param = 100\n",
    "chunk_overlap_param = 20\n",
    "\n",
    "def split_document_charakter(document):\n",
    "    # Split the documents into text_chunks\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=chunk_size_param, chunk_overlap=chunk_overlap_param)\n",
    "    text_chunks = text_splitter.split_documents(documents)\n",
    "    \n",
    "    return text_chunks\n",
    "\n",
    "   \n",
    "# Test the function with a sample PDF file path\n",
    "text_chunks = split_document_charakter(documents)\n",
    "print(f\"Number of text_chunks created: {len(text_chunks)}\")\n",
    "print(f\"First text_chunks: {text_chunks[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Lade Dokument...\n",
      "üß© Erstelle Chunks...\n",
      "üß† Verwende Sprache f√ºr Tokenizer: german\n",
      "\n",
      "üìÑ Insgesamt 1397 Chunks generiert.\n",
      "\n",
      "--- Chunk 1 ---\n",
      "page_content='Kontierungshandbuch \n",
      " \n",
      " \n",
      "1 von 70 \n",
      "Inhaltsverzeichnis \n",
      "1 Einleitung 3'\n",
      "\n",
      "--- Chunk 2 ---\n",
      "page_content='1 Einleitung 3 \n",
      "1.1 Inhalt und Aufbau des Handbuchs 3 \n",
      "1.2 Kontennomenklatur 4'\n",
      "\n",
      "--- Chunk 3 ---\n",
      "page_content='1.3 Buchungslogik und Zusatzkontierungen 5 \n",
      "1.4 Bedienungshinweise zum Kontierungshandbuch 6'\n",
      "\n",
      "--- Chunk 4 ---\n",
      "page_content='1.4.1 Hyperlinks 6 \n",
      "1.4.2 Farbschema 6 \n",
      "2 Bilanz: Aktiva 7 \n",
      "2.1 Anlageverm√∂gen 7'\n",
      "\n",
      "--- Chunk 5 ---\n",
      "page_content='2.1.1 Immaterielle Verm√∂gensgegenst√§nde 7 \n",
      "2.1.1.1 Selbsterstellte Software 7 \n",
      "2.1.2 Sachanlagen 9'\n"
     ]
    }
   ],
   "source": [
    "# function A2: load the pdf-file containing the accounting assignment guide data from a folder and splitt the data into text_chunks\n",
    "# function A2.2.2 Class for load pdf-data and split documents in chunks (Sematic Chunker)\n",
    "\n",
    "import os\n",
    "import nltk\n",
    "from typing import List\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "\n",
    "class PDFSemanticChunker:\n",
    "    def __init__(self, pdf_path: str, language: str = \"german\", chunk_size: int = 100, chunk_overlap: int = 20):\n",
    "        self.pdf_path = pdf_path\n",
    "        self.language = language\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "\n",
    "        # Stelle sicher, dass Punkt verf√ºgbar ist\n",
    "        self._ensure_nltk_resources()\n",
    "\n",
    "    def _ensure_nltk_resources(self):\n",
    "        try:\n",
    "            nltk.data.find(\"tokenizers/punkt\")\n",
    "        except LookupError:\n",
    "            print(\"üì• Lade NLTK Punkt-Tokenizer herunter...\")\n",
    "            nltk.download(\"punkt\")\n",
    "\n",
    "    def load_documents(self):\n",
    "        if not os.path.exists(self.pdf_path):\n",
    "            raise FileNotFoundError(f\"‚ùå Die Datei '{self.pdf_path}' wurde nicht gefunden.\")\n",
    "        loader = PyPDFLoader(self.pdf_path)\n",
    "        return loader.load()\n",
    "\n",
    "    def extract_texts(self, documents) -> List[str]:\n",
    "        return [doc.page_content for doc in documents]\n",
    "\n",
    "    def chunk_semantically(self, texts: List[str]) -> List[str]:\n",
    "        print(f\"üß† Verwende Sprache f√ºr Tokenizer: {self.language}\")\n",
    "        valid_langs = [\"english\", \"german\"]\n",
    "        if self.language not in valid_langs:\n",
    "            print(f\"‚ö†Ô∏è Sprache '{self.language}' nicht unterst√ºtzt. Standard = 'german'\")\n",
    "            self.language = \"german\"\n",
    "\n",
    "        all_sentences = []\n",
    "        for text in texts:\n",
    "            all_sentences.extend(sent_tokenize(text, language=self.language))\n",
    "\n",
    "        chunks = []\n",
    "        current_chunk = \"\"\n",
    "        for sentence in all_sentences:\n",
    "            if len(current_chunk) + len(sentence) < self.chunk_size:\n",
    "                current_chunk += \" \" + sentence\n",
    "            else:\n",
    "                chunks.append(current_chunk.strip())\n",
    "                current_chunk = sentence\n",
    "        if current_chunk:\n",
    "            chunks.append(current_chunk.strip())\n",
    "\n",
    "        # Mit LangChain weiter aufteilen\n",
    "        splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=self.chunk_size,\n",
    "            chunk_overlap=self.chunk_overlap,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \".\", \" \"]\n",
    "        )\n",
    "\n",
    "        final_chunks = []\n",
    "        for chunk in chunks:\n",
    "            final_chunks.extend(splitter.split_text(chunk))\n",
    "\n",
    "        # chunks in LangChain-Dokument-Interface √ºbergeben\n",
    "        chunk_documents = [Document(page_content=chunk) for chunk in final_chunks]\n",
    "\n",
    "        return chunk_documents\n",
    "\n",
    "    def process(self) -> List[str]:\n",
    "        print(\"üìÇ Lade Dokument...\")\n",
    "        docs = self.load_documents()\n",
    "        texts = self.extract_texts(docs)\n",
    "        print(\"üß© Erstelle Chunks...\")\n",
    "        return self.chunk_semantically(texts)\n",
    "    \n",
    "#function-call Class\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = \"data/sample_accounting_guide.pdf\"\n",
    "\n",
    "    chunker = PDFSemanticChunker(\n",
    "        pdf_path=file_path,\n",
    "        language=\"german\",\n",
    "        chunk_size=100,\n",
    "        chunk_overlap=20\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        text_chunks = chunker.process()\n",
    "        print(f\"\\nüìÑ Insgesamt {len(text_chunks)} Chunks generiert.\")\n",
    "        for i, chunk in enumerate(text_chunks[:5]):\n",
    "            print(f\"\\n--- Chunk {i+1} ---\\n{chunk}\")\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model initialized successfully.\n",
      "text-splitter-instance set!\n",
      "\n",
      "üìÑ Insgesamt 139 Chunks generiert.\n",
      "\n",
      "--- Chunk 1 ---\n",
      "page_content='Kontierungshandbuch \n",
      " \n",
      " \n",
      "1 von 70 \n",
      "Inhaltsverzeichnis \n",
      "1 Einleitung 3 \n",
      "1.1 Inhalt und Aufbau des Handbuchs 3 \n",
      "1.2 Kontennomenklatur 4 \n",
      "1.3 Buchungslogik und Zusatzkontierungen 5 \n",
      "1.4 Bedienungshinweise zum Kontierungshandbuch 6 \n",
      "1.4.1 Hyperlinks 6 \n",
      "1.4.2 Farbschema 6 \n",
      "2 Bilanz: Aktiva 7 \n",
      "2.1 Anlageverm√∂gen 7 \n",
      "2.1.1 Immaterielle Verm√∂gensgegenst√§nde 7 \n",
      "2.1.1.1 Selbsterstellte Software 7 \n",
      "2.1.2 Sachanlagen 9 \n",
      "2.1.2.1 Au√üerplanm√§√üige Abschreibung von Sachanlagen 9 \n",
      "2.1.2.2 Aktivierung der Verbindlichkeiten aus Capital Lease 10 \n",
      "2.1.2.2.1 Aktivierung der Verbindlichkeiten aus Capital Lease 11 \n",
      "2.1.2.2.2 Aktivierung der Verbindlichkeiten aus Capital Lease 11 \n",
      "2.2 Umlaufverm√∂gen 12 \n",
      "2.2.1 Vorr√§te 12 \n",
      "2.2.1.1 Ermittlung des beizulegenden Werts (Marktwert) 13 \n",
      "2.2.1.2 Niederstwerttest (Lower of Cost or Market Prinzip) 13 \n",
      "2.2.2 Forderungen und sonstige Verm√∂gensgegenst√§nde 14 \n",
      "2.2.2.1 Forderungen gegen verbundene Unternehmen 15 \n",
      "2.2.2.2 Umbuchung von langfristigen Forderungen auf kurzfristigen Anteil 15 \n",
      "2.2.2.2.1 Umbuchung von langfristigen Darlehensforderungen auf kurzfristigen Anteil 15 \n",
      "2.2.2.2.2 Abbau des kurzfristigen Anteils der langfristigen Forderungen 16 \n",
      "2.2.2.2.3 Ausgleich der offenen Posten der langfristigen Forderungen und des \n",
      "kurzfristigen Anteils bei R√ºckzahlung des Gesamtdarlehensbetrages 16 \n",
      "2.2.2.2.4 Umbuchung von langfristigen Forderungen aus L u.' metadata={'producer': 'Microsoft¬Æ Word f√ºr Microsoft 365', 'creator': 'Microsoft¬Æ Word f√ºr Microsoft 365', 'creationdate': '2025-04-08T10:47:12+02:00', 'title': 'Kontierungshandbuch', 'author': 'Ein zufriedengestellter Microsoft Office-Anwender', 'moddate': '2025-04-08T10:47:12+02:00', 'source': 'data/sample_accounting_guide.pdf', 'total_pages': 70, 'page': 0, 'page_label': '1'}\n",
      "\n",
      "--- Chunk 2 ---\n",
      "page_content='L auf kurzfristigen Anteil \n",
      "   16 \n",
      "2.2.2.2.5 Abbau des kurzfristigen Anteils langfristiger Forderungen 17 \n",
      "2.2.2.3 Forderungsumbuchung von kurzfristigen auf langfristige Forderungen 17 \n",
      "2.2.2.4 Abschreibung von Forderungen 18 \n",
      "2.2.2.4.1 Abschreibung von Forderungen wegen Uneinbringlichkeit ‚Äì identische \n",
      "Bewertung nach HGB und US GAAP 19 \n",
      "2.2.2.4.2 Abschreibung von Forderungen wegen Uneinbringlichkeit - unterschiedliche \n",
      "Bewertung nach HGB und US GAAP 20 \n",
      "2.2.2.5 Auf-/ Abzinsung von langfristigen Forderungen aus L u. L 21 \n",
      "2.2.2.5.1 Abzinsung langfristiger Forderungen aus L u. L 21 \n",
      "2.2.2.5.2 Aufzinsung langfristiger Forderungen aus L u. L 23 \n",
      "2.2.2.6 Auf-/ Abzinsung von langfristigen Finanzforderungen 25 \n",
      "2.2.2.6.1 Abzinsung langfristiger Finanzforderungen 26' metadata={'producer': 'Microsoft¬Æ Word f√ºr Microsoft 365', 'creator': 'Microsoft¬Æ Word f√ºr Microsoft 365', 'creationdate': '2025-04-08T10:47:12+02:00', 'title': 'Kontierungshandbuch', 'author': 'Ein zufriedengestellter Microsoft Office-Anwender', 'moddate': '2025-04-08T10:47:12+02:00', 'source': 'data/sample_accounting_guide.pdf', 'total_pages': 70, 'page': 0, 'page_label': '1'}\n",
      "\n",
      "--- Chunk 3 ---\n",
      "page_content='Kontierungshandbuch \n",
      " \n",
      " \n",
      "2 von 70 \n",
      "2.2.2.6.2 Aufzinsung langfristiger Finanzforderungen 26 \n",
      "2.2.2.7 Transaktionen in Fremdw√§hrungen - Forderungen 27 \n",
      "2.2.2.7.1 Forderungsabwertung durch W√§hrungsverluste 28 \n",
      "2.2.2.7.2 Ausweis unrealisierter W√§hrungsgewinne bei Forderungen aus L u.' metadata={'producer': 'Microsoft¬Æ Word f√ºr Microsoft 365', 'creator': 'Microsoft¬Æ Word f√ºr Microsoft 365', 'creationdate': '2025-04-08T10:47:12+02:00', 'title': 'Kontierungshandbuch', 'author': 'Ein zufriedengestellter Microsoft Office-Anwender', 'moddate': '2025-04-08T10:47:12+02:00', 'source': 'data/sample_accounting_guide.pdf', 'total_pages': 70, 'page': 1, 'page_label': '2'}\n",
      "\n",
      "--- Chunk 4 ---\n",
      "page_content='L 29 \n",
      "2.2.2.7.3 R√ºcknahme der unrealisierten W√§hrungsgewinne aus L u. L. 30 \n",
      "2.2.3 Wertpapiere des Umlaufverm√∂gens 30 \n",
      "2.2.3.1 Zuordnung 30 \n",
      "2.2.3.2 Bewertung von Wertpapieren des Umlaufverm√∂gens 31 \n",
      "2.2.3.3 Umgliederung von Wertpapieren 33 \n",
      "2.2.3.3.1 Umgliederung von ‚Äûtrading‚Äú (T) nach ‚Äûavailable for sale‚Äú (A) oder ‚Äûheld to \n",
      "maturity‚Äú (H) nach Ausweis eines unrealisierten Gewinns 33 \n",
      "2.2.3.3.2 Umgliederung von ‚Äûavailable for sale‚Äú (A) oder ‚Äûheld to maturity‚Äú (H) nach \n",
      "‚Äûtrading‚Äú (T) bei Ausweis eines unrealisierten Gewinns 34 \n",
      "2.2.3.3.3 Umgliederung von ‚Äûheld to maturity‚Äú (H) nach ‚Äûavailable for sale‚Äú (A) bei \n",
      "erfolgsneutralem Ausweis eines unrealisierten Gewinns 35 \n",
      "2.2.3.3.4 Umgliederung von ‚Äûavailable for sale‚Äú (A) nach ‚Äûheld to maturity‚Äú (H nach \n",
      "erfolgsneutraler Aufwertung 36 \n",
      "2.2.3.3.5 Erfolgswirksame Vereinnahmung der Neubewertungsr√ºcklage √ºber die \n",
      "Restlaufzeit des Wertpapiers 37 \n",
      "2.2.3.4 Wertminderungen am Beispiel marktf√§higer Wertpapiere 37 \n",
      "2.3 Aktive latente Steuern 39 \n",
      "2.3.1 Ansatz und Methodik latenter Steuern 39 \n",
      "2.3.2 Buchung der aktivischen Steuerdifferenz 40 \n",
      "2.3.3 Aufl√∂sung der aktivischen Steuerdifferenz 41 \n",
      "3 Bilanz: Passiva 42 \n",
      "3.1 R√ºckstellungen 42 \n",
      "3.1.1 Zuordnung 42 \n",
      "3.1.2 Bewertung 43 \n",
      "3.1.3 Aufwandsr√ºckstellungen nach HGB 44 \n",
      "3.1.3.1 Aufl√∂sung der HGB-R√ºckstellung bei Eintritt des R√ºckstellungsgrundes 45 \n",
      "3.1.3.2 Aufl√∂sung der HGB-R√ºckst. bei Nicht-Eintritt des R√ºckstellungsgrundes 46 \n",
      "3.1.4 Langfristige R√ºckstellungen: Aufzinsung / Abzinsung 46 \n",
      "3.1.4.1 Aufzinsung langfristiger R√ºckstellungen 48 \n",
      "3.1.4.2 Abzinsung langfristiger R√ºckstellungen 49 \n",
      "3.1.5 Pensionsr√ºckstellungen 50 \n",
      "3.1.5.1 Bildung von Pensionsr√ºckstellungen 51 \n",
      "3.1.5.2 Aufl√∂sung von Pensionsr√ºckstellungen 52 \n",
      "3.2 Verbindlichkeiten 53 \n",
      "3.2.1 Zuordnung von Verbindlichkeiten nach HGB und US GAAP 53 \n",
      "3.2.2 Bewertung von Verbindlichkeiten 55 \n",
      "3.2.3 Abzinsung von Verbindlichkeiten 55 \n",
      "3.2.4 Verbindlichkeiten in Fremdw√§hrungen 57' metadata={'producer': 'Microsoft¬Æ Word f√ºr Microsoft 365', 'creator': 'Microsoft¬Æ Word f√ºr Microsoft 365', 'creationdate': '2025-04-08T10:47:12+02:00', 'title': 'Kontierungshandbuch', 'author': 'Ein zufriedengestellter Microsoft Office-Anwender', 'moddate': '2025-04-08T10:47:12+02:00', 'source': 'data/sample_accounting_guide.pdf', 'total_pages': 70, 'page': 1, 'page_label': '2'}\n",
      "\n",
      "--- Chunk 5 ---\n",
      "page_content='Kontierungshandbuch \n",
      " \n",
      " \n",
      "3 von 70 \n",
      "3.2.4.1 Identische Verbindlichkeitsaufwertung aus L u. L (W√§hrung) 57 \n",
      "3.2.4.2 Verbindlichkeitsabwertung aus L u. L US GAAP (W√§hrung) 59 \n",
      "3.2.4.3 Ausweis unrealisierter Gewinne bei Verbindlichkeiten 59 \n",
      "3.2.5 Verbindlichkeitsumbuchung von kurzfristig auf langfristig 60 \n",
      "3.2.6 Verbindlichkeitsumbuchung von langfristig auf kurzfristig 61 \n",
      "3.2.6.1 Verbindlichkeitsumbuchung des kurzfristigen Anteils der langfristigen \n",
      "Verbindlichkeiten 61 \n",
      "3.2.6.2 Abbau des kurzfristigen Anteils der langfristigen Verbindlichkeiten 61 \n",
      "3.2.6.3 Verbindlichkeitsumbuchung des kurzfristigen Anteils der langfristigen \n",
      "Verbindlichkeiten 62 \n",
      "3.2.6.4 Abbau des kurzfristigen Anteils der langfristigen Verbindlichkeiten 62 \n",
      "3.2.6.5 Umbuchung des kurzfristigen Anteils langfristiger Verbindlichkeiten aus Capital \n",
      "Lease 63 \n",
      "3.2.6.6 Abbau des kurzfristigen Anteils langfristiger Verbindlichkeiten aus Capital Lease \n",
      "  63 \n",
      "3.3 Passive latente Steuern 64 \n",
      "3.3.1 Buchung der passivischen Steuerdifferenz 65 \n",
      "3.3.2 Aufl√∂sung der passivischen Steuerdifferenz 66 \n",
      "4 Die Gliederung der GuV nach dem Umsatzkostenverfahren 66 \n",
      "4.1 Umsatzkostenverfahren nach US GAAP bei #### 68 \n",
      " \n",
      " \n",
      " \n",
      "1 Einleitung \n",
      " \n",
      "1.1 Inhalt und Aufbau des Handbuchs \n",
      " \n",
      "Dieses Kontierungshandbuch soll die Handhabung der parallelen Buchf√ºhrung  gem√§√ü \n",
      "deutschem Handelsrecht (HGB) und der US GAAP anhand von Sachverhalten verdeutlichen, die \n",
      "f√ºr den Einzelabschlu√ü von Gesellschaften des #### Konzerns von Belang sind. Die \n",
      "Buchungsbeispiele wurden prim√§r f√ºr Gesellschaften erstellt, die f√ºr ihre kaufm√§nnischen \n",
      "Anwendungen SAP R/3 einsetzen, k√∂nnen jedoch hinsichtlich der grundlegenden Buchungslogik \n",
      "auf alle Gesellschaften √ºbertragen werden, die eine kaufm√§nnische Anwendungssoftware \n",
      "verwenden, in der die Integration von externem und internem Rechnungswesen zu gew√§hrleisten \n",
      "ist. Im Besonderen werden nur die Unterschiede in der Gliederung oder Bewertung der \n",
      "verschiedenen Rechnungslegungen thematisiert. Der Aufbau des Handbuchs  lehnt sich an die Gliederung der Bilanz und GuV nach Handelsrecht \n",
      "an, Unterschiede zu US -amerikanischen Vorschriften werden an den jeweiligen Positionen \n",
      "angesprochen. Zur Ermittlung von Bemessungsgrundlagen sind teilweise Bewertungsschemata \n",
      "eingef√ºgt.' metadata={'producer': 'Microsoft¬Æ Word f√ºr Microsoft 365', 'creator': 'Microsoft¬Æ Word f√ºr Microsoft 365', 'creationdate': '2025-04-08T10:47:12+02:00', 'title': 'Kontierungshandbuch', 'author': 'Ein zufriedengestellter Microsoft Office-Anwender', 'moddate': '2025-04-08T10:47:12+02:00', 'source': 'data/sample_accounting_guide.pdf', 'total_pages': 70, 'page': 2, 'page_label': '3'}\n"
     ]
    }
   ],
   "source": [
    "# function A2: load the pdf-file containing the accounting assignment guide data from a folder and splitt the data into text_chunks\n",
    "# function A2.2.3 split document in chunks - version 3: Semantic Chunking with LChain \n",
    "# (see LChain docs - https://python.langchain.com/docs/how_to/semantic-chunker/)\n",
    "# problem: SemanticChunker needs String-Structure, load document has LChain document-object-structure\n",
    "\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from gen_ai_hub.proxy.langchain.init_models import init_embedding_model\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Input: documents (from function A2 - load data)\n",
    "# Output: text_chunks\n",
    "\n",
    "# load env-key for embedidding model SAP AI Core\n",
    "ai_core_embedding_model_name = str(os.getenv('AICORE_DEPLOYMENT_MODEL_EMBEDDING'))\n",
    "\n",
    "# init embedding-instance\n",
    "try:\n",
    "    embeddings = init_embedding_model(ai_core_embedding_model_name)\n",
    "    print(\"Embedding model initialized successfully.\")\n",
    "except Exception as e:\n",
    "    print(\"Embedding model not initialized.\")\n",
    "    print(e)\n",
    "\n",
    "\n",
    "# Init text_splitter-Instance with type \"gradient\" \n",
    "\n",
    "text_splitter = SemanticChunker(\n",
    "    embeddings=embeddings,\n",
    "    breakpoint_threshold_type=\"gradient\"\n",
    ")\n",
    "\n",
    "print(\"text-splitter-instance set!\")\n",
    "\n",
    "# Init text_splitter-Instance - other types\n",
    "# \n",
    "# text_splitter = SemanticChunker(embeddings)\n",
    "# text_splitter = SemanticChunker(embeddings=embeddings, breakpoint_threshold_type=\"interquartile\")\n",
    "# text_splitter = SemanticChunker(embeddings=embeddings, breakpoint_threshold_type=\"standard_deviation\")\n",
    "# text_splitter = SemanticChunker(embeddings=embeddings, breakpoint_threshold_type=\"percentile\")\n",
    "\n",
    "# We split text in the usual way, e.g., by invoking .create_documents to create LangChain Document objects\n",
    "# docs = text_splitter.create_documents([state_of_the_union] <- List not document-object)\n",
    "\n",
    "\n",
    "# split docs for every page in documents with SematicChunker-Splitter and rebuild document-object Lchain\n",
    "text_chunks = []\n",
    "for doc in documents:\n",
    "    text_split = text_splitter.split_text(doc.page_content)\n",
    "    # rebuild documents-objekt in LChain-Document-Structure\n",
    "    for text in text_split:\n",
    "        text_chunks.append(Document(page_content=text, metadata=doc.metadata))\n",
    "\n",
    "\n",
    "print(f\"\\nüìÑ Insgesamt {len(text_chunks)} Chunks generiert.\")\n",
    "for i, chunk in enumerate(text_chunks[:5]):\n",
    "    print(f\"\\n--- Chunk {i+1} ---\\n{chunk}\")\n",
    "\n",
    "### - other method for splitting an rebuild\n",
    "\n",
    "# Extract text from Document-Objekten ()\n",
    "# texts = [doc.page_content for doc in documents]\n",
    "# metadatas = [doc.metadata for doc in documents]\n",
    "\n",
    "# all_split_texts = []\n",
    "# all_split_metadatas = []\n",
    "\n",
    "# # Verarbeiten Sie jeden Text einzeln mit split_text (SemanticChunker works with a list not with LChain-Document-object)\n",
    "# for i, text in enumerate(texts):\n",
    "#     split_texts = text_splitter.split_text(text)\n",
    "#     split_metadatas = [metadatas[i]] * len(split_texts) # Metadaten f√ºr jeden Chunk duplizieren\n",
    "#     all_split_texts.extend(split_texts)\n",
    "#     all_split_metadatas.extend(split_metadatas)\n",
    "\n",
    "# # create new document from splitted Texte and Metadaten\n",
    "# split_docs = [Document(page_content=text, metadata=metadata)\n",
    "#               for text, metadata in zip(all_split_texts, all_split_metadatas)]\n",
    "\n",
    "# print(split_docs[0].page_content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function A3 - vectorize the splitted data with embedding function \n",
    "# function A3.1 init embeddings-instance\n",
    "# Use the embedding models from SAP AI-hub for embedding.\n",
    "\n",
    "from gen_ai_hub.proxy.langchain.init_models import init_embedding_model\n",
    "\n",
    "ai_core_embedding_model_name = str(os.getenv('AICORE_DEPLOYMENT_MODEL_EMBEDDING'))\n",
    " \n",
    "try:\n",
    "    embeddings = init_embedding_model(ai_core_embedding_model_name)\n",
    "    print(\"Embedding model initialized successfully.\")\n",
    "except Exception as e:\n",
    "    print(\"Embedding model not initialized.\")\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Successfully created SAP HANA VectorStore interface: <dbapi.Connection Connection object : bfff8255-c34a-41cb-a822-bf9b5f56fb16.hna0.prod-eu20.hanacloud.ondemand.com,443,DBADMIN,DB@hmx04,True>\n",
      "    and SAP HANA table: ACCOUNTING_ASSIGN_SUPPORT_TABLE_DBADMIN.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# function A4 - store: create/clear a sap hana database - table and store the vector in this table\n",
    "# function A4.1 - Create a LangChain VectorStore interface for the HANA database and specify the table \n",
    "# Hint: check table creation with sap-hana-database explorer: select * from ACCOUNTING_ASSIGN_SUPPORT_TABLE_DBADMIN\n",
    "\n",
    "from langchain_community.vectorstores.hanavector import HanaDB\n",
    "\n",
    "vector_table_name = str(os.getenv('hdb_table_name'))\n",
    "\n",
    "hana_database = HanaDB(\n",
    "    embedding = embeddings, \n",
    "    connection = hana_connection, \n",
    "    table_name = vector_table_name\n",
    ")\n",
    "\n",
    "try:\n",
    "    print(f\"\"\"\n",
    "    Successfully created SAP HANA VectorStore interface: {hana_database.connection}\n",
    "    and SAP HANA table: {vector_table_name}.\n",
    "    \"\"\")\n",
    "except Exception as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully added 139 document chunks to the database.\n",
      "table-name:  ACCOUNTING_ASSIGN_SUPPORT_TABLE_DBADMIN\n",
      "Successfully connected to the HANA Cloud database.\n"
     ]
    }
   ],
   "source": [
    "# function A4 - store: create/clear a sap hana database - table and store the vector in this table\n",
    "# function A4.2.1 - delete existing documents from the table and load embeddings to SAP HANA-Table\n",
    "\n",
    "# Delete already existing documents from the SAP HANA table\n",
    "hana_database.delete(filter={})\n",
    "\n",
    "# add the loaded document text_chunks\n",
    "hana_database.add_documents(text_chunks)\n",
    "\n",
    "print(f\"Successfully added {len(text_chunks)} document chunks to the database.\")\n",
    "print(\"table-name: \",hana_database.table_name)\n",
    "print(\"Successfully connected to the HANA Cloud database.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Lade Dokument...\n",
      "üß© Erstelle Chunks...\n",
      "üß† Verwende Sprache f√ºr Tokenizer: german\n",
      "‚úÖ 1397 Chunks wurden erfolgreich zur HANA-Datenbank hinzugef√ºgt.\n",
      "üìÅ Tabellenname: ACCOUNTING_ASSIGN_SUPPORT_TABLE_DBADMIN\n"
     ]
    }
   ],
   "source": [
    "# function A2 - load and splitt: load the pdf-file containing the accounting assignment guide data from a folder and splitt the data into text_chunks\n",
    "# function A3 - vectorize and embedd: vectorize the splitted data with embedding function. Use an embedding function to convert the text chunks into vector representations\n",
    "# function A4 - store: create/clear a sap hana database - table and store the vector in this table\n",
    "\n",
    "# function A2/A3/A4 as one\n",
    "\n",
    "#from my_chunking_module import PDFSemanticChunker  # falls es in Datei gespeichert ist\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = \"data/sample_accounting_guide.pdf\"\n",
    "\n",
    "    # init chunker-instance\n",
    "\n",
    "    chunker = PDFSemanticChunker(\n",
    "        pdf_path=file_path,\n",
    "        language=\"german\",\n",
    "        chunk_size=100,\n",
    "        chunk_overlap=20\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # 1. Chunking ausf√ºhren\n",
    "        text_chunks = chunker.process()\n",
    "\n",
    "        # 2. Optional - Umwandeln in LangChain Document-Objekte\n",
    "        # from langchain.schema import Document\n",
    "        # documents = [Document(page_content=chunk) for chunk in text_chunks]\n",
    "\n",
    "        # 3. Vorherige Daten in HANA l√∂schen\n",
    "        hana_database.delete(filter={})\n",
    "\n",
    "        # 4. Neue Dokumente hinzuf√ºgen\n",
    "        hana_database.add_documents(documents)\n",
    "\n",
    "        print(f\"‚úÖ {len(documents)} Chunks wurden erfolgreich zur HANA-Datenbank hinzugef√ºgt.\")\n",
    "        print(\"üìÅ Tabellenname:\", hana_database.table_name)\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"‚ùå Datei nicht gefunden: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check function A4.2 - query to the table to verify embeddings\n",
    "\n",
    "cursor = hana_connection.cursor()\n",
    "sql = f'SELECT VEC_TEXT, TO_NVARCHAR(VEC_VECTOR) FROM \"{hana_database.table_name}\"'\n",
    "\n",
    "cursor.execute(sql)\n",
    "vectors = cursor.fetchall()\n",
    "\n",
    "print(vectors[:1])\n",
    "\n",
    "# for vector in vectors:\n",
    "#     print(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrap-Book Development Modul A\n",
    "\n",
    "* function A2: load the pdf-file and split\n",
    "* function A4.3: Check retrieval for the embeddings in the SAP-Hana-Database\n",
    "* check function A4.3: check retrieval from SAP HANA DB with prompt using chain_type=\"map_reduce\"\n",
    "* check function A4.3: check retrieval from SAP HANA DB with prompt using chain_type=\"stuff\"\n",
    "* check function A4.3: check retrieval from SAP HANA DB with prompt using chain_type=\"map_reduce\" and optimized prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function A2: load the pdf-file containing the accounting assignment guide data from a folder and splitt the data into text_chunks\n",
    "# the funktion uses the os modules to read the file and split the data with langChain-modules\n",
    "\n",
    "\n",
    "import os\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "def load_and_split_pdf(file_path):\n",
    "    # Check if the file exists\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n",
    "    \n",
    "    # Load the PDF file\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    documents = loader.load()\n",
    "    \n",
    "    # Split the documents into text_chunks\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=3000, chunk_overlap=200)\n",
    "    text_chunks = text_splitter.split_documents(documents)\n",
    "    \n",
    "    return text_chunks\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Test the function with a sample PDF file path\n",
    "    file_path = \"data/sample_accounting_guide.pdf\"\n",
    "    try:\n",
    "        text_chunks = load_and_split_pdf(file_path)\n",
    "        print(f\"Number of text_chunks created: {len(text_chunks)}\")\n",
    "        print(f\"First text_chunks: {text_chunks[0]}\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Wir splitten zuerst nach Abs√§tzen und dann rekursiv, falls ein Absatz zu gro√ü ist\n",
    "paragraphs = text.split(\"\\n\\n\")  # Annahme: Abs√§tze sind durch zwei Zeilen getrennt\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300,  # Gesamtgr√∂√üe eines Chunks\n",
    "    chunk_overlap=50,  # √úberlappung in Tokens/Zeichen\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \"],  # Reihenfolge der Split-Priorit√§t\n",
    ")\n",
    "\n",
    "chunks = []\n",
    "for para in paragraphs:\n",
    "    if para.strip():  # Leere Abs√§tze √ºberspringen\n",
    "        sub_chunks = text_splitter.split_text(para)\n",
    "        chunks.extend(sub_chunks)\n",
    "\n",
    "# Ausgabe\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"--- Chunk {i+1} ---\")\n",
    "    print(chunk)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check retrievaleval for the embeddings in the SAP-Hana-Database\n",
    "\n",
    "This code snippet integrates various components from the langchain library to create a retrieval-based question-answering (QA) system. Here's a breakdown of the key parts and their functionality:\n",
    "\n",
    "Retriever Initialization: The db.as_retriever function is used to initialize a retriever object with specific search arguments ('k':20), which likely defines the number of search results to consider.\n",
    "\n",
    "Prompt Template : The PromptTemplate was defined in the previous step that instructs how to use the context to answer a question. It emphasizes not to fabricate answers if the information is unavailable. The template also outlines the structure for the expected JSON output with various product and supplier details.The prompt template is crucial for guiding the model's responses, ensuring that the answers are relevant and accurate based on the retrieved information.\n",
    "\n",
    "Once the retriever and prompt template are set up, the next step involves using the LLM (Language Model) to generate answers based on the retrieved documents. This process typically includes passing the retrieved context to the LLM along with the user's query, allowing it to formulate a coherent and contextually appropriate response.After that, the LLM processes the prompt and generates a response, which can then be formatted and returned to the user, ensuring a seamless interaction with the QA system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function A4.3: Check retrievaleval for the embeddings in the SAP-Hana-Database\n",
    "\n",
    "import os\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "llm_chain = prompt | llm\n",
    "\n",
    "question = \"What NFL team won the Super Bowl in the year Justin Bieber was born?\"\n",
    "\n",
    "print(llm_chain.invoke({'question': question}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check function 4.2 : check retrieval from SAP HANA DB with prompt using chain_type=\"map_reduce\"\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "map_template = \"\"\"\n",
    "Analysiere den folgenden Kontext und extrahiere relevante Informationen zur Kontierung:\n",
    "\n",
    "{context}\n",
    "\n",
    "Frage: {question}\n",
    "\n",
    "Gib die relevanten Informationen in einem kurzen Zwischenergebnis zur√ºck.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "reduce_template = \"\"\"\n",
    "Basierend auf den folgenden Zwischenergebnissen, erstelle eine finale Antwort:\n",
    "\n",
    "{summerization}\n",
    "\n",
    "Frage: {question}\n",
    "\n",
    "Formatiere die Ergebnisse in einer Liste von JSON-Elementen mit den folgenden Schl√ºsseln:\n",
    "\"Gesch√§ftsfall\"\n",
    "\"Konto Soll\"\n",
    "\"Konto Haben\"\n",
    "\n",
    "Die Ergebnisse d√ºrfen keine json markdown codeblock syntax enthalten.\n",
    "Wenn keine relevanten Informationen gefunden wurden, gib an dass Du keine Antwort kennst.\n",
    "\"\"\"\n",
    "\n",
    "MAP_PROMPT = PromptTemplate(template=map_template, input_variables=[\"context\", \"question\"])\n",
    "REDUCE_PROMPT = PromptTemplate(template=reduce_template, input_variables=[\"summerization\", \"question\"])\n",
    "\n",
    "chain_type_kwargs = {\n",
    "    \"question_prompt\": MAP_PROMPT,\n",
    "    \"reduce_prompt\": REDUCE_PROMPT\n",
    "}\n",
    "\n",
    "question = \"Finde Kontierung f√ºr die Umbuchung von langfristigen Forderungen\"\n",
    "\n",
    "retriever = hana_database.as_retriever(search_kwargs={'k':20})\n",
    "\n",
    "question_answer_retriever = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    chain_type='map_reduce',\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "answer = question_answer_retriever.run(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check function 4.2 : check retrieval from SAP HANA DB with prompt using chain_type=\"stuff\"\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "prompt_template = \"\"\"Verwende den folgenden Kontext, um die Frage am Ende zu beantworten. Wenn du die Antwort nicht kennst,\n",
    "    sage einfach, dass du es nicht wei√üt. Versuche nicht, eine Antwort zu erfinden. Formatiere die Ergebnisse als Liste von JSON-Elementen mit den folgenden Schl√ºsseln:\n",
    "\n",
    "    \"Gesch√§ftsfall\",\n",
    "    \"Konto Soll\",\n",
    "    \"Konto Haben\"\n",
    "\n",
    "    F√ºge keine JSON-Markdown-Codeblock-Syntax in die Ergebnisse ein.\n",
    "\n",
    "    {context}\n",
    "\n",
    "    Frage: {question}\n",
    "\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(template=prompt_template, \n",
    "                       input_variables=[\"context\", \"question\"]\n",
    "                      )\n",
    "    \n",
    "chain_type_kwargs = {\"prompt\": PROMPT}\n",
    "\n",
    "question = \"Finde Kontierung f√ºr die Buchung von R√ºckstellungen\"\n",
    "\n",
    "count_retrieved_documents = 10\n",
    "\n",
    "retriever = hana_database.as_retriever(search_kwargs={'k': count_retrieved_documents})\n",
    "# hint: k smaller than 20 -> to much tokens\n",
    "\n",
    "question_answer_retriever = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    chain_type=\"stuff\",\n",
    "    chain_type_kwargs=chain_type_kwargs,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "answer = question_answer_retriever.run(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check function 4.2 : check retrieval from SAP HANA DB with prompt using chain_type=\"map_reduce\" (prompt-finetuning)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Prompt f√ºr die Map-Phase\n",
    "map_prompt_template = \"\"\"Analysiere den folgenden Kontext und extrahiere relevante Kontierungsinformationen f√ºr den Gesch√§ftsfall.\n",
    "Wenn keine relevanten Informationen im Kontext gefunden werden k√∂nnen, gib die Antwort zur√ºck: \"Ich habe keine Kontierungsinformationen zum Gesch√§ftsfall gefunden\".\n",
    "\n",
    "{context}\n",
    "\n",
    "Frage: {question}\n",
    "\"\"\"\n",
    "\n",
    "# Prompt f√ºr die Combine/Reduce-Phase\n",
    "combine_prompt_template = \"\"\"\n",
    "- Fasse die folgenden Kontierungsinformationen zusammen und entferne Duplikate.\n",
    "- Gib nur die relevantesten und eindeutigsten Kontierungen zur√ºck. \n",
    "- Wenn keine relevanten Informationen im Kontext gefunden werden k√∂nnen, gib die Antwort zur√ºck: \"Ich habe keine Kontierungsinformationen zum Gesch√§ftsfall gefunden\".\n",
    "- Wenn relevante Informationen gefunden wurden gib diese Informationen im folgenden Struktur aus:\n",
    "    ## Gesch√§ftsfall: <Bezeichnung des Gesch√§ftsfalls>\n",
    "    ## Kontierung\n",
    "    Konto-Soll: <Konto-Soll> - <Bezeichnung Konto-Soll> AN Konto-Haben: <Konto-Haben> - <Bezeichnung Konto-Haben>\n",
    "\n",
    "Beispiel:\n",
    "\n",
    "-----------------------------\n",
    "## Gesch√§ftsfall: Bildung von R√ºckstellungen\n",
    "    ## Kontierung\n",
    "    Konto-Soll: L160501 - LC Instandhaltungskosten (Geb√§ude) AN Konto-Haben: L3909101 - LC Sonstige R√ºckstellungen\n",
    "-----------------------------\n",
    "\n",
    "{summaries}\n",
    "\n",
    "Frage: {question}\n",
    "\"\"\"\n",
    "\n",
    "# Korrekte Struktur f√ºr chain_type_kwargs\n",
    "chain_type_kwargs = {\n",
    "    \"question_prompt\": PromptTemplate(\n",
    "        template=map_prompt_template,\n",
    "        input_variables=[\"context\", \"question\"]\n",
    "    ),\n",
    "    \"combine_prompt\": PromptTemplate(\n",
    "        template=combine_prompt_template,\n",
    "        input_variables=[\"summaries\", \"question\"]\n",
    "    )\n",
    "}\n",
    "\n",
    "question = \"Finde Kontierung f√ºr die Buchung von Bildung von R√ºckstellungen\"\n",
    "\n",
    "retriever = hana_database.as_retriever(search_kwargs={'k': 20})\n",
    "\n",
    "question_answer_retriever = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"map_reduce\",\n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs=chain_type_kwargs,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "answer = question_answer_retriever.run(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example setup SAP HANA Vector Database\n",
    "\n",
    "Wichtige Aspekte des Codes:\n",
    "Tabellenstruktur:\n",
    "ID: Eindeutiger Identifier f√ºr jeden Eintrag\n",
    "DOCUMENT: Der eigentliche Dokumententext\n",
    "METADATA: JSON-formatierte Metadaten\n",
    "EMBEDDING: Der Embedding-Vektor als BLOB\n",
    "VECTOR_DIMENSION: Dimension des Embedding-Vektors (1536 f√ºr ada-002)\n",
    "Vector-Index:\n",
    "Verwendet HANA's native Vektorindexierung\n",
    "Cosine-Similarity mit Threshold 0.75\n",
    "Optimiert f√ºr 1536-dimensionale Vektoren\n",
    "Testdaten:\n",
    "Beispieldaten f√ºr Kontierungsregeln\n",
    "Dummy-Embeddings f√ºr Testzwecke\n",
    "Strukturierte Metadaten im JSON-Format\n",
    "Verifikation:\n",
    "√úberpr√ºft Tabellenstruktur\n",
    "Zeigt vorhandene Indizes\n",
    "Gibt Anzahl der Datens√§tze aus\n",
    "Um den Code zu verwenden:\n",
    "1. Stellen Sie sicher, dass die Umgebungsvariablen gesetzt sind\n",
    "2. F√ºhren Sie das Setup aus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HDB_HOST'] = 'your_host'\n",
    "os.environ['HDB_PORT'] = 'your_port'\n",
    "os.environ['HDB_USER'] = 'your_user'\n",
    "os.environ['HDB_PASSWORD'] = 'your_password'\n",
    "os.environ['HDB_SCHEMA'] = 'your_schema'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabelle erstellen und Testdaten einf√ºgen\n",
    "create_vector_table(connection_params)\n",
    "insert_test_data(connection_params)\n",
    "verify_table_setup(connection_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exmaple setup code SAP HANA Vector DB (cursor)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "from hdbcli import dbapi\n",
    "import numpy as np\n",
    "\n",
    "def create_vector_table(connection_params, table_name=\"VECTOR_TABLE\"):\n",
    "    \"\"\"\n",
    "    Erstellt eine Vektortabelle in SAP HANA f√ºr die Speicherung von Dokumenten und deren Embeddings.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Verbindung zur HANA-Datenbank herstellen\n",
    "        conn = dbapi.connect(\n",
    "            address=connection_params['address'],\n",
    "            port=connection_params['port'],\n",
    "            user=connection_params['user'],\n",
    "            password=connection_params['password']\n",
    "        )\n",
    "        \n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Zum angegebenen Schema wechseln\n",
    "        if 'schema' in connection_params:\n",
    "            cursor.execute(f\"SET SCHEMA {connection_params['schema']}\")\n",
    "\n",
    "        # Tabelle erstellen, falls sie nicht existiert\n",
    "        create_table_sql = f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "            ID NVARCHAR(100) PRIMARY KEY,\n",
    "            DOCUMENT NCLOB,\n",
    "            METADATA NCLOB,\n",
    "            EMBEDDING BLOB,\n",
    "            VECTOR_DIMENSION INTEGER\n",
    "        )\n",
    "        \"\"\"\n",
    "        cursor.execute(create_table_sql)\n",
    "        \n",
    "        # Vector-Index erstellen\n",
    "        create_index_sql = f\"\"\"\n",
    "        CREATE INDEX IF NOT EXISTS IDX_{table_name}_VECTOR \n",
    "        ON {table_name}(EMBEDDING) \n",
    "        VECTOR DIMENSION 1536 \n",
    "        DOUBLE COSINE THRESHOLD 0.75\n",
    "        \"\"\"\n",
    "        cursor.execute(create_index_sql)\n",
    "        \n",
    "        print(f\"Tabelle {table_name} und Vector-Index wurden erfolgreich erstellt.\")\n",
    "        \n",
    "        # √úberpr√ºfen, ob die Tabelle leer ist\n",
    "        cursor.execute(f\"SELECT COUNT(*) FROM {table_name}\")\n",
    "        count = cursor.fetchone()[0]\n",
    "        print(f\"Anzahl der Eintr√§ge in der Tabelle: {count}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Fehler beim Erstellen der Tabelle: {str(e)}\")\n",
    "        raise\n",
    "    finally:\n",
    "        if 'cursor' in locals():\n",
    "            cursor.close()\n",
    "        if 'conn' in locals():\n",
    "            conn.close()\n",
    "\n",
    "def insert_test_data(connection_params, table_name=\"VECTOR_TABLE\"):\n",
    "    \"\"\"\n",
    "    F√ºgt Testdaten in die Vektortabelle ein.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = dbapi.connect(\n",
    "            address=connection_params['address'],\n",
    "            port=connection_params['port'],\n",
    "            user=connection_params['user'],\n",
    "            password=connection_params['password']\n",
    "        )\n",
    "        \n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        if 'schema' in connection_params:\n",
    "            cursor.execute(f\"SET SCHEMA {connection_params['schema']}\")\n",
    "\n",
    "        # Beispiel-Kontierungsdaten\n",
    "        test_data = [\n",
    "            {\n",
    "                'id': 'doc1',\n",
    "                'document': 'Buchung von R√ºckstellungen f√ºr Garantieverpflichtungen',\n",
    "                'metadata': '{\"type\": \"accounting_rule\", \"category\": \"provisions\"}',\n",
    "                'embedding': np.random.rand(1536).astype(np.float64)  # Dummy-Embedding\n",
    "            },\n",
    "            {\n",
    "                'id': 'doc2',\n",
    "                'document': 'Anlagenzugang durch Kauf einer Maschine',\n",
    "                'metadata': '{\"type\": \"accounting_rule\", \"category\": \"fixed_assets\"}',\n",
    "                'embedding': np.random.rand(1536).astype(np.float64)\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        # Daten einf√ºgen\n",
    "        for data in test_data:\n",
    "            insert_sql = f\"\"\"\n",
    "            INSERT INTO {table_name} \n",
    "            (ID, DOCUMENT, METADATA, EMBEDDING, VECTOR_DIMENSION) \n",
    "            VALUES(?, ?, ?, ?, 1536)\n",
    "            \"\"\"\n",
    "            \n",
    "            cursor.execute(insert_sql, \n",
    "                         (data['id'], \n",
    "                          data['document'], \n",
    "                          data['metadata'], \n",
    "                          data['embedding'].tobytes()))\n",
    "\n",
    "        conn.commit()\n",
    "        print(f\"Testdaten wurden erfolgreich in {table_name} eingef√ºgt.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Fehler beim Einf√ºgen der Testdaten: {str(e)}\")\n",
    "        raise\n",
    "    finally:\n",
    "        if 'cursor' in locals():\n",
    "            cursor.close()\n",
    "        if 'conn' in locals():\n",
    "            conn.close()\n",
    "\n",
    "def verify_table_setup(connection_params, table_name=\"VECTOR_TABLE\"):\n",
    "    \"\"\"\n",
    "    √úberpr√ºft die Einrichtung der Vektortabelle.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = dbapi.connect(\n",
    "            address=connection_params['address'],\n",
    "            port=connection_params['port'],\n",
    "            user=connection_params['user'],\n",
    "            password=connection_params['password']\n",
    "        )\n",
    "        \n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        if 'schema' in connection_params:\n",
    "            cursor.execute(f\"SET SCHEMA {connection_params['schema']}\")\n",
    "\n",
    "        # Tabellenstruktur √ºberpr√ºfen\n",
    "        cursor.execute(f\"\"\"\n",
    "        SELECT COLUMN_NAME, DATA_TYPE_NAME, LENGTH, IS_NULLABLE \n",
    "        FROM TABLE_COLUMNS \n",
    "        WHERE TABLE_NAME = '{table_name.upper()}'\n",
    "        \"\"\")\n",
    "        \n",
    "        print(\"\\nTabellenstruktur:\")\n",
    "        for col in cursor.fetchall():\n",
    "            print(f\"Spalte: {col[0]}, Typ: {col[1]}, L√§nge: {col[2]}, Nullable: {col[3]}\")\n",
    "\n",
    "        # Index √ºberpr√ºfen\n",
    "        cursor.execute(f\"\"\"\n",
    "        SELECT INDEX_NAME, CONSTRAINT \n",
    "        FROM INDEXES \n",
    "        WHERE TABLE_NAME = '{table_name.upper()}'\n",
    "        \"\"\")\n",
    "        \n",
    "        print(\"\\nIndizes:\")\n",
    "        for idx in cursor.fetchall():\n",
    "            print(f\"Index: {idx[0]}, Constraint: {idx[1]}\")\n",
    "\n",
    "        # Datenbestand √ºberpr√ºfen\n",
    "        cursor.execute(f\"SELECT COUNT(*) FROM {table_name}\")\n",
    "        count = cursor.fetchone()[0]\n",
    "        print(f\"\\nAnzahl der Datens√§tze: {count}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler bei der √úberpr√ºfung: {str(e)}\")\n",
    "        raise\n",
    "    finally:\n",
    "        if 'cursor' in locals():\n",
    "            cursor.close()\n",
    "        if 'conn' in locals():\n",
    "            conn.close()\n",
    "\n",
    "# Verwendung:\n",
    "if __name__ == \"__main__\":\n",
    "    # Verbindungsparameter (sollten aus Umgebungsvariablen kommen)\n",
    "    connection_params = {\n",
    "        'address': os.getenv('HDB_HOST'),\n",
    "        'port': os.getenv('HDB_PORT'),\n",
    "        'user': os.getenv('HDB_USER'),\n",
    "        'password': os.getenv('HDB_PASSWORD'),\n",
    "        'schema': os.getenv('HDB_SCHEMA')\n",
    "    }\n",
    "\n",
    "    table_name = \"VECTOR_TABLE\"\n",
    "\n",
    "    try:\n",
    "        # Tabelle erstellen\n",
    "        create_vector_table(connection_params, table_name)\n",
    "        \n",
    "        # Optional: Testdaten einf√ºgen\n",
    "        insert_test_data(connection_params, table_name)\n",
    "        \n",
    "        # Setup √ºberpr√ºfen\n",
    "        verify_table_setup(connection_params, table_name)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Fehler beim Setup: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example insert documentws in SAP HANA\n",
    "\n",
    "Gesch√§ftsfall          | Konto_Soll | Konto_Haben | Beschreibung\n",
    "---------------------- | ----------- | ----------- | ------------\n",
    "R√ºckstellung_Garantie  | 6815       | 3050        | Buchung von R√ºckstellungen f√ºr Garantieverpflichtungen...\n",
    "Anlagenzugang_Kauf    | 0410       | 2800        | Anschaffung einer neuen Produktionsmaschine...\n",
    "\n",
    "Die wichtigsten Features:\n",
    "Excel-Verarbeitung:\n",
    "Liest Kontierungsregeln aus Excel\n",
    "Unterst√ºtzt strukturierte Daten mit Gesch√§ftsfall, Konten und Beschreibung\n",
    "Embedding-Erstellung:\n",
    "Verwendet OpenAI's ada-002 Modell\n",
    "Verarbeitet Dokumente in Batches\n",
    "Fortschrittsanzeige mit tqdm\n",
    "Datenbankintegration:\n",
    "Sichere Verbindungshandhabung\n",
    "Effiziente Batch-Verarbeitung\n",
    "Fehlerbehandlung und Logging\n",
    "Metadaten-Handling:\n",
    "Strukturierte Speicherung der Kontierungsinformationen\n",
    "JSON-Format f√ºr flexible Erweiterbarkeit\n",
    "Verwendung:\n",
    "Excel-Datei vorbereiten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beispiel f√ºr manuelle Datenerstellung\n",
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    'Gesch√§ftsfall': ['R√ºckstellung_Garantie', 'Anlagenzugang_Kauf'],\n",
    "    'Konto_Soll': ['6815', '0410'],\n",
    "    'Konto_Haben': ['3050', '2800'],\n",
    "    'Beschreibung': [\n",
    "        'Buchung von R√ºckstellungen f√ºr Garantieverpflichtungen...',\n",
    "        'Anschaffung einer neuen Produktionsmaschine...'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_excel('kontierungsregeln.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processor initialisieren und ausf√ºhren\n",
    "processor = DocumentProcessor(connection_params)\n",
    "processor.process_and_store_documents('kontierungsregeln.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "from typing import List, Dict\n",
    "import json\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import uuid\n",
    "\n",
    "class DocumentProcessor:\n",
    "    def __init__(self, connection_params: Dict, table_name: str = \"VECTOR_TABLE\"):\n",
    "        \"\"\"\n",
    "        Initialisiert den Document Processor.\n",
    "        \n",
    "        Args:\n",
    "            connection_params: Dictionary mit HANA-Verbindungsparametern\n",
    "            table_name: Name der Vektortabelle\n",
    "        \"\"\"\n",
    "        self.connection_params = connection_params\n",
    "        self.table_name = table_name\n",
    "        self.embeddings = OpenAIEmbeddings(\n",
    "            model=\"text-embedding-ada-002\"\n",
    "        )\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=200,\n",
    "            length_function=len\n",
    "        )\n",
    "        \n",
    "    def process_excel_data(self, excel_file: str, sheet_name: str = \"Sheet1\") -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Verarbeitet Excel-Daten mit Kontierungsregeln.\n",
    "        \n",
    "        Args:\n",
    "            excel_file: Pfad zur Excel-Datei\n",
    "            sheet_name: Name des Excel-Sheets\n",
    "        \n",
    "        Returns:\n",
    "            Liste von Dokumenten mit Metadaten\n",
    "        \"\"\"\n",
    "        try:\n",
    "            df = pd.read_excel(excel_file, sheet_name=sheet_name)\n",
    "            documents = []\n",
    "            \n",
    "            for _, row in df.iterrows():\n",
    "                # Annahme: Excel-Spalten sind \"Gesch√§ftsfall\", \"Konto_Soll\", \"Konto_Haben\", \"Beschreibung\"\n",
    "                doc = {\n",
    "                    'id': str(uuid.uuid4()),\n",
    "                    'document': row['Beschreibung'],\n",
    "                    'metadata': json.dumps({\n",
    "                        'Gesch√§ftsfall': row['Gesch√§ftsfall'],\n",
    "                        'Konto_Soll': str(row['Konto_Soll']),\n",
    "                        'Konto_Haben': str(row['Konto_Haben'])\n",
    "                    }, ensure_ascii=False)\n",
    "                }\n",
    "                documents.append(doc)\n",
    "            \n",
    "            return documents\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Fehler beim Verarbeiten der Excel-Datei: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def create_embeddings(self, documents: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Erstellt Embeddings f√ºr die Dokumente.\n",
    "        \n",
    "        Args:\n",
    "            documents: Liste von Dokumenten\n",
    "        \n",
    "        Returns:\n",
    "            Liste von Dokumenten mit Embeddings\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(\"Erstelle Embeddings...\")\n",
    "            for doc in tqdm(documents):\n",
    "                # Embedding f√ºr den Dokumententext erstellen\n",
    "                embedding = self.embeddings.embed_query(doc['document'])\n",
    "                doc['embedding'] = embedding\n",
    "            return documents\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Fehler beim Erstellen der Embeddings: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def insert_documents(self, documents: List[Dict]):\n",
    "        \"\"\"\n",
    "        F√ºgt Dokumente in die HANA-Datenbank ein.\n",
    "        \n",
    "        Args:\n",
    "            documents: Liste von Dokumenten mit Embeddings\n",
    "        \"\"\"\n",
    "        from hdbcli import dbapi\n",
    "        import numpy as np\n",
    "        \n",
    "        try:\n",
    "            conn = dbapi.connect(\n",
    "                address=self.connection_params['address'],\n",
    "                port=self.connection_params['port'],\n",
    "                user=self.connection_params['user'],\n",
    "                password=self.connection_params['password']\n",
    "            )\n",
    "            \n",
    "            cursor = conn.cursor()\n",
    "            \n",
    "            if 'schema' in self.connection_params:\n",
    "                cursor.execute(f\"SET SCHEMA {self.connection_params['schema']}\")\n",
    "\n",
    "            print(\"F√ºge Dokumente in die Datenbank ein...\")\n",
    "            for doc in tqdm(documents):\n",
    "                insert_sql = f\"\"\"\n",
    "                INSERT INTO {self.table_name} \n",
    "                (ID, DOCUMENT, METADATA, EMBEDDING, VECTOR_DIMENSION) \n",
    "                VALUES(?, ?, ?, ?, 1536)\n",
    "                \"\"\"\n",
    "                \n",
    "                # Embedding in bytes konvertieren\n",
    "                embedding_bytes = np.array(doc['embedding']).astype(np.float64).tobytes()\n",
    "                \n",
    "                cursor.execute(insert_sql, \n",
    "                             (doc['id'], \n",
    "                              doc['document'], \n",
    "                              doc['metadata'], \n",
    "                              embedding_bytes))\n",
    "\n",
    "            conn.commit()\n",
    "            print(f\"Alle Dokumente wurden erfolgreich eingef√ºgt.\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Fehler beim Einf√ºgen der Dokumente: {str(e)}\")\n",
    "            raise\n",
    "        finally:\n",
    "            if 'cursor' in locals():\n",
    "                cursor.close()\n",
    "            if 'conn' in locals():\n",
    "                conn.close()\n",
    "\n",
    "    def process_and_store_documents(self, excel_file: str, sheet_name: str = \"Sheet1\"):\n",
    "        \"\"\"\n",
    "        Hauptmethode zum Verarbeiten und Speichern von Dokumenten.\n",
    "        \n",
    "        Args:\n",
    "            excel_file: Pfad zur Excel-Datei\n",
    "            sheet_name: Name des Excel-Sheets\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Dokumente aus Excel laden\n",
    "            documents = self.process_excel_data(excel_file, sheet_name)\n",
    "            print(f\"Anzahl geladener Dokumente: {len(documents)}\")\n",
    "            \n",
    "            # Embeddings erstellen\n",
    "            documents_with_embeddings = self.create_embeddings(documents)\n",
    "            \n",
    "            # Dokumente in die Datenbank einf√ºgen\n",
    "            self.insert_documents(documents_with_embeddings)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Fehler bei der Verarbeitung: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "# Beispiel f√ºr die Verwendung:\n",
    "if __name__ == \"__main__\":\n",
    "    # Verbindungsparameter\n",
    "    connection_params = {\n",
    "        'address': os.getenv('HDB_HOST'),\n",
    "        'port': os.getenv('HDB_PORT'),\n",
    "        'user': os.getenv('HDB_USER'),\n",
    "        'password': os.getenv('HDB_PASSWORD'),\n",
    "        'schema': os.getenv('HDB_SCHEMA')\n",
    "    }\n",
    "\n",
    "    # Excel-Datei mit Kontierungsregeln\n",
    "    excel_file = \"kontierungsregeln.xlsx\"\n",
    "\n",
    "    # Document Processor initialisieren und ausf√ºhren\n",
    "    processor = DocumentProcessor(connection_params)\n",
    "    \n",
    "    try:\n",
    "        processor.process_and_store_documents(excel_file)\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler beim Gesamtprozess: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
